{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atha/diploma/environ/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import argparse\n",
    "parent_path = pathlib.Path(os.getcwd()).parent.absolute()\n",
    "sys.path.append(str(parent_path))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "import torch_geometric.transforms as T\n",
    "# from torch_geometric.datasets import MovieLens\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "from utils.MyMovieLens import MyMovieLens\n",
    "from utils.gnn_simple import Model\n",
    "from utils.train_test import train_test\n",
    "from utils.visualize import plot_loss, plot_train, plot_val, plot_test\n",
    "from utils.suggestions import make_recommendations, get_reviews_suggestions_intersection, get_user_suggestions, get_user_reviews, analyze_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join(osp.dirname(osp.abspath('')), '../../data/MovieLens')\n",
    "dataset = MyMovieLens(path, model_name='all-MiniLM-L6-v2')\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user node features for message passing:\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes\n",
    "\n",
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label.\n",
    "\n",
    "# Perform a link-level split into training, validation, and test edges:\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and train-test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atha/diploma/environ/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 18.1725, Train: 3.1287, Val: 3.1235, Test: 3.1495\n",
      "Epoch: 002, Loss: 15.2480, Train: 1.9136, Val: 1.9339, Test: 1.9489\n",
      "Epoch: 003, Loss: 7.3015, Train: 1.9782, Val: 1.9702, Test: 1.9351\n",
      "Epoch: 004, Loss: 67.4875, Train: 1.1585, Val: 1.1910, Test: 1.1684\n",
      "Epoch: 005, Loss: 6.7654, Train: 2.3362, Val: 2.3425, Test: 2.3595\n",
      "Epoch: 006, Loss: 9.2047, Train: 2.8597, Val: 2.8519, Test: 2.8835\n",
      "Epoch: 007, Loss: 12.8358, Train: 3.0764, Val: 3.0689, Test: 3.0972\n",
      "Epoch: 008, Loss: 14.7563, Train: 3.1695, Val: 3.1639, Test: 3.1909\n",
      "Epoch: 009, Loss: 15.6435, Train: 3.2063, Val: 3.1990, Test: 3.2268\n",
      "Epoch: 010, Loss: 15.9789, Train: 3.2166, Val: 3.2096, Test: 3.2377\n",
      "Epoch: 011, Loss: 16.0820, Train: 3.2147, Val: 3.2071, Test: 3.2351\n",
      "Epoch: 012, Loss: 16.0626, Train: 3.2041, Val: 3.1972, Test: 3.2246\n",
      "Epoch: 013, Loss: 15.9559, Train: 3.1871, Val: 3.1798, Test: 3.2068\n",
      "Epoch: 014, Loss: 15.7916, Train: 3.1641, Val: 3.1564, Test: 3.1841\n",
      "Epoch: 015, Loss: 15.5756, Train: 3.1336, Val: 3.1271, Test: 3.1540\n",
      "Epoch: 016, Loss: 15.2907, Train: 3.0972, Val: 3.0890, Test: 3.1166\n",
      "Epoch: 017, Loss: 14.9466, Train: 3.0465, Val: 3.0380, Test: 3.0662\n",
      "Epoch: 018, Loss: 14.4852, Train: 2.9779, Val: 2.9734, Test: 3.0003\n",
      "Epoch: 019, Loss: 13.8963, Train: 2.8837, Val: 2.8806, Test: 2.9037\n",
      "Epoch: 020, Loss: 13.0679, Train: 2.7456, Val: 2.7349, Test: 2.7654\n",
      "Epoch: 021, Loss: 11.9876, Train: 2.5288, Val: 2.5343, Test: 2.5486\n",
      "Epoch: 022, Loss: 10.4128, Train: 2.1749, Val: 2.1483, Test: 2.1787\n",
      "Epoch: 023, Loss: 8.3130, Train: 1.5664, Val: 1.5752, Test: 1.6099\n",
      "Epoch: 024, Loss: 6.3472, Train: 1.1214, Val: 1.1295, Test: 1.1207\n",
      "Epoch: 025, Loss: 7.9412, Train: 1.1328, Val: 1.1504, Test: 1.1216\n",
      "Epoch: 026, Loss: 8.4043, Train: 1.1543, Val: 1.1729, Test: 1.1699\n",
      "Epoch: 027, Loss: 6.6777, Train: 1.4333, Val: 1.4334, Test: 1.4141\n",
      "Epoch: 028, Loss: 6.1955, Train: 1.6677, Val: 1.6682, Test: 1.6952\n",
      "Epoch: 029, Loss: 6.4077, Train: 1.7924, Val: 1.8037, Test: 1.8149\n",
      "Epoch: 030, Loss: 6.7461, Train: 1.8229, Val: 1.8443, Test: 1.8620\n",
      "Epoch: 031, Loss: 6.9227, Train: 1.8009, Val: 1.8111, Test: 1.8244\n",
      "Epoch: 032, Loss: 6.9088, Train: 1.7334, Val: 1.7464, Test: 1.7517\n",
      "Epoch: 033, Loss: 6.6747, Train: 1.6312, Val: 1.6389, Test: 1.6509\n",
      "Epoch: 034, Loss: 6.3378, Train: 1.4865, Val: 1.4834, Test: 1.5017\n",
      "Epoch: 035, Loss: 6.2206, Train: 1.3515, Val: 1.3454, Test: 1.3481\n",
      "Epoch: 036, Loss: 6.0305, Train: 1.2297, Val: 1.2312, Test: 1.2524\n",
      "Epoch: 037, Loss: 6.2486, Train: 1.1866, Val: 1.1927, Test: 1.1998\n",
      "Epoch: 038, Loss: 6.4342, Train: 1.1875, Val: 1.2162, Test: 1.2094\n",
      "Epoch: 039, Loss: 6.3441, Train: 1.2461, Val: 1.2565, Test: 1.2389\n",
      "Epoch: 040, Loss: 6.1289, Train: 1.3382, Val: 1.3288, Test: 1.3609\n",
      "Epoch: 041, Loss: 6.1079, Train: 1.4145, Val: 1.4498, Test: 1.4528\n",
      "Epoch: 042, Loss: 6.0617, Train: 1.4823, Val: 1.5018, Test: 1.5134\n",
      "Epoch: 043, Loss: 6.1715, Train: 1.5536, Val: 1.5444, Test: 1.5616\n",
      "Epoch: 044, Loss: 6.1184, Train: 1.5518, Val: 1.5869, Test: 1.5672\n",
      "Epoch: 045, Loss: 6.1714, Train: 1.5429, Val: 1.5595, Test: 1.5602\n",
      "Epoch: 046, Loss: 6.2156, Train: 1.5013, Val: 1.5155, Test: 1.5139\n",
      "Epoch: 047, Loss: 6.1074, Train: 1.4406, Val: 1.4394, Test: 1.4725\n",
      "Epoch: 048, Loss: 6.0249, Train: 1.3692, Val: 1.3791, Test: 1.3865\n",
      "Epoch: 049, Loss: 6.0081, Train: 1.3110, Val: 1.3043, Test: 1.3241\n",
      "Epoch: 050, Loss: 6.0591, Train: 1.2696, Val: 1.2756, Test: 1.2757\n",
      "Epoch: 051, Loss: 6.0374, Train: 1.2306, Val: 1.2590, Test: 1.2529\n",
      "Epoch: 052, Loss: 6.1107, Train: 1.2594, Val: 1.2704, Test: 1.2784\n",
      "Epoch: 053, Loss: 6.1423, Train: 1.3053, Val: 1.3168, Test: 1.3157\n",
      "Epoch: 054, Loss: 6.0217, Train: 1.3580, Val: 1.3712, Test: 1.3967\n",
      "Epoch: 055, Loss: 5.9493, Train: 1.3837, Val: 1.3893, Test: 1.4178\n",
      "Epoch: 056, Loss: 6.0069, Train: 1.4417, Val: 1.4437, Test: 1.4607\n",
      "Epoch: 057, Loss: 5.9940, Train: 1.4379, Val: 1.4715, Test: 1.4669\n",
      "Epoch: 058, Loss: 6.0262, Train: 1.4505, Val: 1.4523, Test: 1.4806\n",
      "Epoch: 059, Loss: 6.0203, Train: 1.4390, Val: 1.4543, Test: 1.4535\n",
      "Epoch: 060, Loss: 5.9324, Train: 1.3991, Val: 1.4145, Test: 1.4332\n",
      "Epoch: 061, Loss: 5.9720, Train: 1.3784, Val: 1.3834, Test: 1.3791\n",
      "Epoch: 062, Loss: 5.8751, Train: 1.3228, Val: 1.3263, Test: 1.3381\n",
      "Epoch: 063, Loss: 5.9144, Train: 1.3065, Val: 1.3020, Test: 1.3049\n",
      "Epoch: 064, Loss: 5.9637, Train: 1.2711, Val: 1.2909, Test: 1.2929\n",
      "Epoch: 065, Loss: 5.9091, Train: 1.2807, Val: 1.2898, Test: 1.2828\n",
      "Epoch: 066, Loss: 5.9341, Train: 1.3007, Val: 1.3026, Test: 1.3169\n",
      "Epoch: 067, Loss: 5.8935, Train: 1.3306, Val: 1.3349, Test: 1.3392\n",
      "Epoch: 068, Loss: 5.9204, Train: 1.3554, Val: 1.3867, Test: 1.3810\n",
      "Epoch: 069, Loss: 5.8793, Train: 1.3832, Val: 1.3928, Test: 1.4142\n",
      "Epoch: 070, Loss: 5.8775, Train: 1.4084, Val: 1.4228, Test: 1.4074\n",
      "Epoch: 071, Loss: 5.9153, Train: 1.4178, Val: 1.4127, Test: 1.4246\n",
      "Epoch: 072, Loss: 5.9215, Train: 1.3612, Val: 1.3861, Test: 1.3969\n",
      "Epoch: 073, Loss: 5.9051, Train: 1.3606, Val: 1.3626, Test: 1.3667\n",
      "Epoch: 074, Loss: 5.8420, Train: 1.3292, Val: 1.3342, Test: 1.3340\n",
      "Epoch: 075, Loss: 5.8195, Train: 1.3107, Val: 1.3161, Test: 1.3176\n",
      "Epoch: 076, Loss: 5.8112, Train: 1.3106, Val: 1.3056, Test: 1.3255\n",
      "Epoch: 077, Loss: 5.8468, Train: 1.3181, Val: 1.3170, Test: 1.3294\n",
      "Epoch: 078, Loss: 5.8381, Train: 1.3023, Val: 1.3076, Test: 1.3157\n",
      "Epoch: 079, Loss: 5.7908, Train: 1.3222, Val: 1.3153, Test: 1.3354\n",
      "Epoch: 080, Loss: 5.7639, Train: 1.3322, Val: 1.3565, Test: 1.3357\n",
      "Epoch: 081, Loss: 5.7756, Train: 1.3392, Val: 1.3640, Test: 1.3322\n",
      "Epoch: 082, Loss: 5.7885, Train: 1.3264, Val: 1.3428, Test: 1.3633\n",
      "Epoch: 083, Loss: 5.7118, Train: 1.3195, Val: 1.3365, Test: 1.3369\n",
      "Epoch: 084, Loss: 5.7137, Train: 1.2969, Val: 1.3182, Test: 1.3197\n",
      "Epoch: 085, Loss: 5.6792, Train: 1.3213, Val: 1.3427, Test: 1.3218\n",
      "Epoch: 086, Loss: 5.7111, Train: 1.3440, Val: 1.3568, Test: 1.3590\n",
      "Epoch: 087, Loss: 5.6646, Train: 1.3114, Val: 1.3143, Test: 1.3538\n",
      "Epoch: 088, Loss: 5.6535, Train: 1.2928, Val: 1.3195, Test: 1.3123\n",
      "Epoch: 089, Loss: 5.6472, Train: 1.2773, Val: 1.2987, Test: 1.3001\n",
      "Epoch: 090, Loss: 5.6192, Train: 1.2861, Val: 1.2853, Test: 1.3136\n",
      "Epoch: 091, Loss: 5.5374, Train: 1.2776, Val: 1.3002, Test: 1.3016\n",
      "Epoch: 092, Loss: 5.5448, Train: 1.3472, Val: 1.3550, Test: 1.3696\n",
      "Epoch: 093, Loss: 5.5505, Train: 1.3432, Val: 1.3932, Test: 1.3755\n",
      "Epoch: 094, Loss: 5.4965, Train: 1.3185, Val: 1.3040, Test: 1.3436\n",
      "Epoch: 095, Loss: 5.4553, Train: 1.2412, Val: 1.2606, Test: 1.2560\n",
      "Epoch: 096, Loss: 5.4914, Train: 1.2655, Val: 1.2892, Test: 1.2859\n",
      "Epoch: 097, Loss: 5.4041, Train: 1.3451, Val: 1.3474, Test: 1.3608\n",
      "Epoch: 098, Loss: 5.3959, Train: 1.3758, Val: 1.4067, Test: 1.3950\n",
      "Epoch: 099, Loss: 5.4275, Train: 1.3553, Val: 1.3319, Test: 1.3442\n",
      "Epoch: 100, Loss: 5.3068, Train: 1.2922, Val: 1.2776, Test: 1.3126\n",
      "Epoch: 101, Loss: 5.3626, Train: 1.2661, Val: 1.2942, Test: 1.3098\n",
      "Epoch: 102, Loss: 5.3504, Train: 1.3290, Val: 1.3526, Test: 1.3377\n",
      "Epoch: 103, Loss: 5.3363, Train: 1.3481, Val: 1.3679, Test: 1.3861\n",
      "Epoch: 104, Loss: 5.3160, Train: 1.3512, Val: 1.3717, Test: 1.3854\n",
      "Epoch: 105, Loss: 5.1985, Train: 1.2816, Val: 1.3196, Test: 1.3137\n",
      "Epoch: 106, Loss: 5.2641, Train: 1.2715, Val: 1.2886, Test: 1.2889\n",
      "Epoch: 107, Loss: 5.2299, Train: 1.3062, Val: 1.2893, Test: 1.3048\n",
      "Epoch: 108, Loss: 5.1839, Train: 1.3494, Val: 1.3678, Test: 1.3527\n",
      "Epoch: 109, Loss: 5.1598, Train: 1.3571, Val: 1.3560, Test: 1.3881\n",
      "Epoch: 110, Loss: 5.2481, Train: 1.3134, Val: 1.2926, Test: 1.3160\n",
      "Epoch: 111, Loss: 5.1628, Train: 1.2015, Val: 1.2525, Test: 1.2204\n",
      "Epoch: 112, Loss: 5.2210, Train: 1.3013, Val: 1.2861, Test: 1.3117\n",
      "Epoch: 113, Loss: 5.1370, Train: 1.4137, Val: 1.3939, Test: 1.4351\n",
      "Epoch: 114, Loss: 5.1612, Train: 1.4165, Val: 1.4259, Test: 1.3894\n",
      "Epoch: 115, Loss: 5.1227, Train: 1.2645, Val: 1.3034, Test: 1.3516\n",
      "Epoch: 116, Loss: 5.0596, Train: 1.2390, Val: 1.2539, Test: 1.2388\n",
      "Epoch: 117, Loss: 5.1306, Train: 1.2958, Val: 1.3187, Test: 1.2771\n",
      "Epoch: 118, Loss: 5.0593, Train: 1.3565, Val: 1.3654, Test: 1.3623\n",
      "Epoch: 119, Loss: 5.1184, Train: 1.3545, Val: 1.3753, Test: 1.3699\n",
      "Epoch: 120, Loss: 5.0299, Train: 1.2518, Val: 1.2857, Test: 1.2731\n",
      "Epoch: 121, Loss: 4.9729, Train: 1.2103, Val: 1.2455, Test: 1.2380\n",
      "Epoch: 122, Loss: 5.0175, Train: 1.2708, Val: 1.3258, Test: 1.2700\n",
      "Epoch: 123, Loss: 4.9788, Train: 1.3083, Val: 1.3520, Test: 1.3538\n",
      "Epoch: 124, Loss: 4.9804, Train: 1.2656, Val: 1.3214, Test: 1.3061\n",
      "Epoch: 125, Loss: 4.9507, Train: 1.2367, Val: 1.2754, Test: 1.2503\n",
      "Epoch: 126, Loss: 4.9193, Train: 1.2888, Val: 1.2717, Test: 1.2852\n",
      "Epoch: 127, Loss: 4.8851, Train: 1.3344, Val: 1.3535, Test: 1.3532\n",
      "Epoch: 128, Loss: 4.8715, Train: 1.2980, Val: 1.3509, Test: 1.3305\n",
      "Epoch: 129, Loss: 4.8803, Train: 1.1988, Val: 1.2626, Test: 1.2038\n",
      "Epoch: 130, Loss: 4.7921, Train: 1.1919, Val: 1.2227, Test: 1.2229\n",
      "Epoch: 131, Loss: 4.8495, Train: 1.3693, Val: 1.4164, Test: 1.3948\n",
      "Epoch: 132, Loss: 4.8273, Train: 1.3550, Val: 1.4427, Test: 1.3627\n",
      "Epoch: 133, Loss: 4.8329, Train: 1.1436, Val: 1.1827, Test: 1.2026\n",
      "Epoch: 134, Loss: 4.8135, Train: 1.2597, Val: 1.2490, Test: 1.2161\n",
      "Epoch: 135, Loss: 4.6359, Train: 1.2271, Val: 1.2454, Test: 1.2564\n",
      "Epoch: 136, Loss: 4.6584, Train: 1.2953, Val: 1.3237, Test: 1.2906\n",
      "Epoch: 137, Loss: 4.6056, Train: 1.2841, Val: 1.3271, Test: 1.3658\n",
      "Epoch: 138, Loss: 4.6236, Train: 1.1753, Val: 1.1947, Test: 1.1606\n",
      "Epoch: 139, Loss: 4.5587, Train: 1.2106, Val: 1.2450, Test: 1.2061\n",
      "Epoch: 140, Loss: 4.4665, Train: 1.3401, Val: 1.3269, Test: 1.3495\n",
      "Epoch: 141, Loss: 4.4893, Train: 1.3112, Val: 1.3032, Test: 1.3179\n",
      "Epoch: 142, Loss: 4.4324, Train: 1.1929, Val: 1.2172, Test: 1.1898\n",
      "Epoch: 143, Loss: 4.4311, Train: 1.2383, Val: 1.2624, Test: 1.2726\n",
      "Epoch: 144, Loss: 4.2849, Train: 1.3934, Val: 1.4256, Test: 1.3708\n",
      "Epoch: 145, Loss: 4.4918, Train: 1.2534, Val: 1.2783, Test: 1.2515\n",
      "Epoch: 146, Loss: 4.2531, Train: 1.1542, Val: 1.1649, Test: 1.1676\n",
      "Epoch: 147, Loss: 4.3248, Train: 1.1724, Val: 1.2858, Test: 1.2012\n",
      "Epoch: 148, Loss: 4.2059, Train: 1.2963, Val: 1.3068, Test: 1.3411\n",
      "Epoch: 149, Loss: 4.3031, Train: 1.2378, Val: 1.2753, Test: 1.3067\n",
      "Epoch: 150, Loss: 4.1626, Train: 1.1869, Val: 1.2053, Test: 1.2271\n",
      "Epoch: 151, Loss: 4.2094, Train: 1.2223, Val: 1.2283, Test: 1.2032\n",
      "Epoch: 152, Loss: 4.2005, Train: 1.2661, Val: 1.2865, Test: 1.2802\n",
      "Epoch: 153, Loss: 4.2057, Train: 1.2019, Val: 1.2704, Test: 1.1919\n",
      "Epoch: 154, Loss: 4.1171, Train: 1.2460, Val: 1.2424, Test: 1.2518\n",
      "Epoch: 155, Loss: 4.1363, Train: 1.2882, Val: 1.2926, Test: 1.3416\n",
      "Epoch: 156, Loss: 4.2773, Train: 1.2066, Val: 1.2625, Test: 1.2520\n",
      "Epoch: 157, Loss: 4.1465, Train: 1.1422, Val: 1.1751, Test: 1.1567\n",
      "Epoch: 158, Loss: 4.1472, Train: 1.2401, Val: 1.2610, Test: 1.2551\n",
      "Epoch: 159, Loss: 4.1002, Train: 1.4272, Val: 1.4088, Test: 1.3417\n",
      "Epoch: 160, Loss: 4.1679, Train: 1.2418, Val: 1.2876, Test: 1.2525\n",
      "Epoch: 161, Loss: 4.0188, Train: 1.1268, Val: 1.1852, Test: 1.1580\n",
      "Epoch: 162, Loss: 4.3114, Train: 1.2303, Val: 1.2589, Test: 1.2640\n",
      "Epoch: 163, Loss: 4.0678, Train: 1.3071, Val: 1.3746, Test: 1.3462\n",
      "Epoch: 164, Loss: 4.1032, Train: 1.2342, Val: 1.2519, Test: 1.2579\n",
      "Epoch: 165, Loss: 4.0017, Train: 1.1982, Val: 1.1937, Test: 1.1828\n",
      "Epoch: 166, Loss: 4.0834, Train: 1.1926, Val: 1.2119, Test: 1.2588\n",
      "Epoch: 167, Loss: 3.9816, Train: 1.2880, Val: 1.3284, Test: 1.3324\n",
      "Epoch: 168, Loss: 4.1825, Train: 1.2468, Val: 1.2252, Test: 1.2242\n",
      "Epoch: 169, Loss: 4.0001, Train: 1.1664, Val: 1.2337, Test: 1.2051\n",
      "Epoch: 170, Loss: 4.0501, Train: 1.2162, Val: 1.2960, Test: 1.2784\n",
      "Epoch: 171, Loss: 3.9741, Train: 1.2525, Val: 1.2931, Test: 1.3054\n",
      "Epoch: 172, Loss: 3.9758, Train: 1.2045, Val: 1.2374, Test: 1.3014\n",
      "Epoch: 173, Loss: 3.9510, Train: 1.1914, Val: 1.2142, Test: 1.2124\n",
      "Epoch: 174, Loss: 4.0466, Train: 1.1798, Val: 1.2208, Test: 1.1995\n",
      "Epoch: 175, Loss: 4.0490, Train: 1.1994, Val: 1.2497, Test: 1.2339\n",
      "Epoch: 176, Loss: 3.9514, Train: 1.2596, Val: 1.2862, Test: 1.2389\n",
      "Epoch: 177, Loss: 3.9283, Train: 1.1996, Val: 1.2519, Test: 1.2397\n",
      "Epoch: 178, Loss: 3.9029, Train: 1.1597, Val: 1.2392, Test: 1.1688\n",
      "Epoch: 179, Loss: 4.0368, Train: 1.3049, Val: 1.3295, Test: 1.3130\n",
      "Epoch: 180, Loss: 3.9901, Train: 1.2673, Val: 1.3356, Test: 1.2966\n",
      "Epoch: 181, Loss: 3.8769, Train: 1.1499, Val: 1.2060, Test: 1.2067\n",
      "Epoch: 182, Loss: 3.9386, Train: 1.2255, Val: 1.2560, Test: 1.2994\n",
      "Epoch: 183, Loss: 3.9623, Train: 1.2291, Val: 1.2371, Test: 1.2495\n",
      "Epoch: 184, Loss: 3.8745, Train: 1.2409, Val: 1.2389, Test: 1.2808\n",
      "Epoch: 185, Loss: 3.9616, Train: 1.1988, Val: 1.1959, Test: 1.1790\n",
      "Epoch: 186, Loss: 3.9320, Train: 1.2424, Val: 1.2249, Test: 1.2023\n",
      "Epoch: 187, Loss: 3.8904, Train: 1.2374, Val: 1.2583, Test: 1.2274\n",
      "Epoch: 188, Loss: 3.8994, Train: 1.2043, Val: 1.2373, Test: 1.2314\n",
      "Epoch: 189, Loss: 3.8556, Train: 1.2532, Val: 1.2664, Test: 1.2735\n",
      "Epoch: 190, Loss: 3.8368, Train: 1.2803, Val: 1.2743, Test: 1.2912\n",
      "Epoch: 191, Loss: 3.7727, Train: 1.2572, Val: 1.2252, Test: 1.2469\n",
      "Epoch: 192, Loss: 3.8017, Train: 1.2080, Val: 1.2702, Test: 1.2167\n",
      "Epoch: 193, Loss: 3.8288, Train: 1.2449, Val: 1.2800, Test: 1.2800\n",
      "Epoch: 194, Loss: 3.8660, Train: 1.1575, Val: 1.2055, Test: 1.1938\n",
      "Epoch: 195, Loss: 3.7934, Train: 1.1800, Val: 1.2243, Test: 1.2064\n",
      "Epoch: 196, Loss: 3.7768, Train: 1.1858, Val: 1.2156, Test: 1.2495\n",
      "Epoch: 197, Loss: 3.8066, Train: 1.3088, Val: 1.3487, Test: 1.3161\n",
      "Epoch: 198, Loss: 3.8752, Train: 1.2239, Val: 1.2858, Test: 1.2876\n",
      "Epoch: 199, Loss: 3.7563, Train: 1.1795, Val: 1.1963, Test: 1.2022\n",
      "Epoch: 200, Loss: 3.8054, Train: 1.1991, Val: 1.2114, Test: 1.2111\n",
      "Epoch: 201, Loss: 3.7458, Train: 1.2929, Val: 1.3059, Test: 1.2749\n",
      "Epoch: 202, Loss: 3.8223, Train: 1.2001, Val: 1.2237, Test: 1.2209\n",
      "Epoch: 203, Loss: 3.8326, Train: 1.1554, Val: 1.1890, Test: 1.1982\n",
      "Epoch: 204, Loss: 3.8619, Train: 1.3508, Val: 1.3595, Test: 1.3534\n",
      "Epoch: 205, Loss: 3.8508, Train: 1.2821, Val: 1.3510, Test: 1.3265\n",
      "Epoch: 206, Loss: 3.8949, Train: 1.1356, Val: 1.1709, Test: 1.1788\n",
      "Epoch: 207, Loss: 3.9071, Train: 1.1361, Val: 1.1835, Test: 1.1543\n",
      "Epoch: 208, Loss: 3.8325, Train: 1.2988, Val: 1.3505, Test: 1.2881\n",
      "Epoch: 209, Loss: 3.8584, Train: 1.2638, Val: 1.3262, Test: 1.2936\n",
      "Epoch: 210, Loss: 3.8578, Train: 1.1875, Val: 1.2078, Test: 1.1892\n",
      "Epoch: 211, Loss: 3.7250, Train: 1.1387, Val: 1.1549, Test: 1.1757\n",
      "Epoch: 212, Loss: 3.7726, Train: 1.2669, Val: 1.2646, Test: 1.3089\n",
      "Epoch: 213, Loss: 3.7237, Train: 1.3399, Val: 1.3685, Test: 1.3288\n",
      "Epoch: 214, Loss: 3.7747, Train: 1.1889, Val: 1.2274, Test: 1.1863\n",
      "Epoch: 215, Loss: 3.7454, Train: 1.1205, Val: 1.1515, Test: 1.1526\n",
      "Epoch: 216, Loss: 3.9668, Train: 1.2387, Val: 1.2689, Test: 1.2808\n",
      "Epoch: 217, Loss: 3.7229, Train: 1.3096, Val: 1.3658, Test: 1.3481\n",
      "Epoch: 218, Loss: 3.8877, Train: 1.2285, Val: 1.2543, Test: 1.2422\n",
      "Epoch: 219, Loss: 3.6958, Train: 1.1484, Val: 1.1615, Test: 1.1776\n",
      "Epoch: 220, Loss: 3.8141, Train: 1.1788, Val: 1.2260, Test: 1.1887\n",
      "Epoch: 221, Loss: 3.7059, Train: 1.3016, Val: 1.3102, Test: 1.3069\n",
      "Epoch: 222, Loss: 3.7630, Train: 1.2223, Val: 1.2321, Test: 1.2480\n",
      "Epoch: 223, Loss: 3.7008, Train: 1.1472, Val: 1.1802, Test: 1.1724\n",
      "Epoch: 224, Loss: 3.7912, Train: 1.1676, Val: 1.1878, Test: 1.1921\n",
      "Epoch: 225, Loss: 3.6872, Train: 1.2793, Val: 1.3103, Test: 1.2841\n",
      "Epoch: 226, Loss: 3.7727, Train: 1.2466, Val: 1.2764, Test: 1.2767\n",
      "Epoch: 227, Loss: 3.7018, Train: 1.1446, Val: 1.1770, Test: 1.1878\n",
      "Epoch: 228, Loss: 3.6637, Train: 1.1226, Val: 1.1604, Test: 1.1404\n",
      "Epoch: 229, Loss: 3.7045, Train: 1.2205, Val: 1.2768, Test: 1.2329\n",
      "Epoch: 230, Loss: 3.7158, Train: 1.2648, Val: 1.2949, Test: 1.2996\n",
      "Epoch: 231, Loss: 3.7652, Train: 1.2009, Val: 1.2326, Test: 1.2185\n",
      "Epoch: 232, Loss: 3.7027, Train: 1.1061, Val: 1.1387, Test: 1.1301\n",
      "Epoch: 233, Loss: 3.7545, Train: 1.1330, Val: 1.1842, Test: 1.1687\n",
      "Epoch: 234, Loss: 3.6814, Train: 1.2362, Val: 1.2434, Test: 1.2417\n",
      "Epoch: 235, Loss: 3.6404, Train: 1.2797, Val: 1.3113, Test: 1.2856\n",
      "Epoch: 236, Loss: 3.6759, Train: 1.2077, Val: 1.2394, Test: 1.2576\n",
      "Epoch: 237, Loss: 3.6081, Train: 1.1571, Val: 1.1766, Test: 1.1638\n",
      "Epoch: 238, Loss: 3.6417, Train: 1.1657, Val: 1.1960, Test: 1.1731\n",
      "Epoch: 239, Loss: 3.6362, Train: 1.2286, Val: 1.2670, Test: 1.2552\n",
      "Epoch: 240, Loss: 3.5918, Train: 1.2347, Val: 1.2626, Test: 1.2559\n",
      "Epoch: 241, Loss: 3.6642, Train: 1.1768, Val: 1.2136, Test: 1.1978\n",
      "Epoch: 242, Loss: 3.6105, Train: 1.1409, Val: 1.1855, Test: 1.1679\n",
      "Epoch: 243, Loss: 3.6210, Train: 1.2074, Val: 1.2011, Test: 1.2246\n",
      "Epoch: 244, Loss: 3.6241, Train: 1.2212, Val: 1.2421, Test: 1.2505\n",
      "Epoch: 245, Loss: 3.6159, Train: 1.2151, Val: 1.2338, Test: 1.2219\n",
      "Epoch: 246, Loss: 3.5785, Train: 1.1866, Val: 1.2013, Test: 1.2109\n",
      "Epoch: 247, Loss: 3.6534, Train: 1.1490, Val: 1.1996, Test: 1.1667\n",
      "Epoch: 248, Loss: 3.5979, Train: 1.1761, Val: 1.1997, Test: 1.2121\n",
      "Epoch: 249, Loss: 3.6537, Train: 1.2422, Val: 1.2398, Test: 1.2461\n",
      "Epoch: 250, Loss: 3.6441, Train: 1.1869, Val: 1.1865, Test: 1.2153\n",
      "Epoch: 251, Loss: 3.5849, Train: 1.1300, Val: 1.1788, Test: 1.1320\n",
      "Epoch: 252, Loss: 3.6890, Train: 1.2124, Val: 1.2263, Test: 1.2436\n",
      "Epoch: 253, Loss: 3.6065, Train: 1.2595, Val: 1.2958, Test: 1.2552\n",
      "Epoch: 254, Loss: 3.6261, Train: 1.1642, Val: 1.1992, Test: 1.1820\n",
      "Epoch: 255, Loss: 3.6015, Train: 1.1036, Val: 1.1349, Test: 1.1258\n",
      "Epoch: 256, Loss: 3.6063, Train: 1.1748, Val: 1.2134, Test: 1.1995\n",
      "Epoch: 257, Loss: 3.5930, Train: 1.2292, Val: 1.2577, Test: 1.2593\n",
      "Epoch: 258, Loss: 3.6378, Train: 1.1988, Val: 1.2165, Test: 1.2195\n",
      "Epoch: 259, Loss: 3.5730, Train: 1.1816, Val: 1.1860, Test: 1.1807\n",
      "Epoch: 260, Loss: 3.6000, Train: 1.1926, Val: 1.1857, Test: 1.2013\n",
      "Epoch: 261, Loss: 3.5465, Train: 1.1909, Val: 1.2466, Test: 1.2166\n",
      "Epoch: 262, Loss: 3.6356, Train: 1.1873, Val: 1.2020, Test: 1.2054\n",
      "Epoch: 263, Loss: 3.5340, Train: 1.1520, Val: 1.1551, Test: 1.1819\n",
      "Epoch: 264, Loss: 3.5603, Train: 1.1890, Val: 1.1762, Test: 1.2004\n",
      "Epoch: 265, Loss: 3.4989, Train: 1.2587, Val: 1.2264, Test: 1.2269\n",
      "Epoch: 266, Loss: 3.5983, Train: 1.2354, Val: 1.2168, Test: 1.2170\n",
      "Epoch: 267, Loss: 3.4939, Train: 1.1510, Val: 1.1554, Test: 1.1719\n",
      "Epoch: 268, Loss: 3.5605, Train: 1.1505, Val: 1.1870, Test: 1.1667\n",
      "Epoch: 269, Loss: 3.5573, Train: 1.2046, Val: 1.2261, Test: 1.2174\n",
      "Epoch: 270, Loss: 3.5430, Train: 1.2620, Val: 1.2965, Test: 1.2552\n",
      "Epoch: 271, Loss: 3.5278, Train: 1.1567, Val: 1.1833, Test: 1.1991\n",
      "Epoch: 272, Loss: 3.4756, Train: 1.1235, Val: 1.1700, Test: 1.1280\n",
      "Epoch: 273, Loss: 3.5503, Train: 1.2107, Val: 1.2278, Test: 1.2073\n",
      "Epoch: 274, Loss: 3.5423, Train: 1.1883, Val: 1.2165, Test: 1.2310\n",
      "Epoch: 275, Loss: 3.5176, Train: 1.1439, Val: 1.1922, Test: 1.1551\n",
      "Epoch: 276, Loss: 3.4954, Train: 1.1783, Val: 1.1978, Test: 1.1768\n",
      "Epoch: 277, Loss: 3.5377, Train: 1.1841, Val: 1.2241, Test: 1.2205\n",
      "Epoch: 278, Loss: 3.5273, Train: 1.1398, Val: 1.1527, Test: 1.1610\n",
      "Epoch: 279, Loss: 3.4845, Train: 1.1049, Val: 1.1497, Test: 1.1359\n",
      "Epoch: 280, Loss: 3.5753, Train: 1.1926, Val: 1.2095, Test: 1.2243\n",
      "Epoch: 281, Loss: 3.5695, Train: 1.2284, Val: 1.2528, Test: 1.2402\n",
      "Epoch: 282, Loss: 3.4416, Train: 1.1709, Val: 1.2060, Test: 1.1959\n",
      "Epoch: 283, Loss: 3.4439, Train: 1.1418, Val: 1.1917, Test: 1.1524\n",
      "Epoch: 284, Loss: 3.4533, Train: 1.1831, Val: 1.1860, Test: 1.1697\n",
      "Epoch: 285, Loss: 3.4804, Train: 1.1866, Val: 1.2100, Test: 1.2061\n",
      "Epoch: 286, Loss: 3.4933, Train: 1.2153, Val: 1.1824, Test: 1.1771\n",
      "Epoch: 287, Loss: 3.5012, Train: 1.1667, Val: 1.1800, Test: 1.2191\n",
      "Epoch: 288, Loss: 3.4478, Train: 1.1921, Val: 1.2592, Test: 1.2355\n",
      "Epoch: 289, Loss: 3.4654, Train: 1.1576, Val: 1.2061, Test: 1.1868\n",
      "Epoch: 290, Loss: 3.4675, Train: 1.1605, Val: 1.1695, Test: 1.1725\n",
      "Epoch: 291, Loss: 3.4859, Train: 1.1882, Val: 1.1982, Test: 1.2047\n",
      "Epoch: 292, Loss: 3.4054, Train: 1.1695, Val: 1.2041, Test: 1.1635\n",
      "Epoch: 293, Loss: 3.4670, Train: 1.1848, Val: 1.1768, Test: 1.1907\n",
      "Epoch: 294, Loss: 3.4369, Train: 1.1520, Val: 1.1644, Test: 1.1758\n",
      "Epoch: 295, Loss: 3.4106, Train: 1.1968, Val: 1.1869, Test: 1.2050\n",
      "Epoch: 296, Loss: 3.4257, Train: 1.1891, Val: 1.2065, Test: 1.1995\n",
      "Epoch: 297, Loss: 3.4130, Train: 1.1624, Val: 1.1794, Test: 1.2019\n",
      "Epoch: 298, Loss: 3.3905, Train: 1.1249, Val: 1.1798, Test: 1.1813\n",
      "Epoch: 299, Loss: 3.4192, Train: 1.1554, Val: 1.1863, Test: 1.1346\n",
      "Epoch: 300, Loss: 3.4317, Train: 1.1674, Val: 1.1952, Test: 1.1891\n",
      "Epoch: 301, Loss: 3.3840, Train: 1.1599, Val: 1.1930, Test: 1.2139\n",
      "Epoch: 302, Loss: 3.4121, Train: 1.1444, Val: 1.1564, Test: 1.1782\n",
      "Epoch: 303, Loss: 3.4173, Train: 1.1691, Val: 1.1956, Test: 1.1900\n",
      "Epoch: 304, Loss: 3.3876, Train: 1.1669, Val: 1.2063, Test: 1.2351\n",
      "Epoch: 305, Loss: 3.3514, Train: 1.1149, Val: 1.1606, Test: 1.1689\n",
      "Epoch: 306, Loss: 3.3867, Train: 1.1338, Val: 1.1671, Test: 1.1371\n",
      "Epoch: 307, Loss: 3.4247, Train: 1.2615, Val: 1.2669, Test: 1.2698\n",
      "Epoch: 308, Loss: 3.4544, Train: 1.2017, Val: 1.2128, Test: 1.2367\n",
      "Epoch: 309, Loss: 3.4313, Train: 1.0950, Val: 1.0885, Test: 1.0804\n",
      "Epoch: 310, Loss: 3.5312, Train: 1.1479, Val: 1.1701, Test: 1.1832\n",
      "Epoch: 311, Loss: 3.3792, Train: 1.2728, Val: 1.3024, Test: 1.2861\n",
      "Epoch: 312, Loss: 3.4288, Train: 1.2089, Val: 1.2206, Test: 1.2347\n",
      "Epoch: 313, Loss: 3.3642, Train: 1.1896, Val: 1.1751, Test: 1.1727\n",
      "Epoch: 314, Loss: 3.4466, Train: 1.2292, Val: 1.2454, Test: 1.2259\n",
      "Epoch: 315, Loss: 3.5014, Train: 1.1685, Val: 1.1874, Test: 1.1859\n",
      "Epoch: 316, Loss: 3.4522, Train: 1.1153, Val: 1.1451, Test: 1.1250\n",
      "Epoch: 317, Loss: 3.5064, Train: 1.1456, Val: 1.1881, Test: 1.1983\n",
      "Epoch: 318, Loss: 3.4731, Train: 1.2063, Val: 1.2349, Test: 1.2213\n",
      "Epoch: 319, Loss: 3.4560, Train: 1.1787, Val: 1.2046, Test: 1.2125\n",
      "Epoch: 320, Loss: 3.3867, Train: 1.1177, Val: 1.1496, Test: 1.1329\n",
      "Epoch: 321, Loss: 3.3579, Train: 1.1309, Val: 1.1258, Test: 1.1386\n",
      "Epoch: 322, Loss: 3.3657, Train: 1.1882, Val: 1.2268, Test: 1.2053\n",
      "Epoch: 323, Loss: 3.3978, Train: 1.1775, Val: 1.2016, Test: 1.1809\n",
      "Epoch: 324, Loss: 3.3860, Train: 1.1219, Val: 1.1650, Test: 1.1408\n",
      "Epoch: 325, Loss: 3.4400, Train: 1.1680, Val: 1.1921, Test: 1.1926\n",
      "Epoch: 326, Loss: 3.3556, Train: 1.2290, Val: 1.2268, Test: 1.2485\n",
      "Epoch: 327, Loss: 3.4666, Train: 1.1373, Val: 1.1667, Test: 1.1803\n",
      "Epoch: 328, Loss: 3.3719, Train: 1.0733, Val: 1.1281, Test: 1.1370\n",
      "Epoch: 329, Loss: 3.4529, Train: 1.1701, Val: 1.1877, Test: 1.1885\n",
      "Epoch: 330, Loss: 3.2973, Train: 1.1892, Val: 1.2326, Test: 1.2435\n",
      "Epoch: 331, Loss: 3.3408, Train: 1.1762, Val: 1.2173, Test: 1.2034\n",
      "Epoch: 332, Loss: 3.2855, Train: 1.1420, Val: 1.1845, Test: 1.1518\n",
      "Epoch: 333, Loss: 3.3702, Train: 1.1020, Val: 1.1382, Test: 1.1703\n",
      "Epoch: 334, Loss: 3.3642, Train: 1.1548, Val: 1.1906, Test: 1.1885\n",
      "Epoch: 335, Loss: 3.3500, Train: 1.1831, Val: 1.2390, Test: 1.2084\n",
      "Epoch: 336, Loss: 3.3224, Train: 1.1185, Val: 1.2096, Test: 1.1638\n",
      "Epoch: 337, Loss: 3.4018, Train: 1.1086, Val: 1.1544, Test: 1.1500\n",
      "Epoch: 338, Loss: 3.3293, Train: 1.1886, Val: 1.2159, Test: 1.2140\n",
      "Epoch: 339, Loss: 3.3521, Train: 1.1812, Val: 1.2147, Test: 1.2103\n",
      "Epoch: 340, Loss: 3.3325, Train: 1.1193, Val: 1.1714, Test: 1.1737\n",
      "Epoch: 341, Loss: 3.3671, Train: 1.0904, Val: 1.1234, Test: 1.1202\n",
      "Epoch: 342, Loss: 3.3673, Train: 1.2392, Val: 1.2615, Test: 1.2342\n",
      "Epoch: 343, Loss: 3.3845, Train: 1.2070, Val: 1.2305, Test: 1.2156\n",
      "Epoch: 344, Loss: 3.3844, Train: 1.0966, Val: 1.1387, Test: 1.1591\n",
      "Epoch: 345, Loss: 3.4029, Train: 1.1773, Val: 1.1943, Test: 1.1936\n",
      "Epoch: 346, Loss: 3.3195, Train: 1.2515, Val: 1.2520, Test: 1.2548\n",
      "Epoch: 347, Loss: 3.3384, Train: 1.1387, Val: 1.1820, Test: 1.1647\n",
      "Epoch: 348, Loss: 3.3921, Train: 1.0811, Val: 1.1094, Test: 1.1077\n",
      "Epoch: 349, Loss: 3.3587, Train: 1.2270, Val: 1.2300, Test: 1.2387\n",
      "Epoch: 350, Loss: 3.3732, Train: 1.2284, Val: 1.2751, Test: 1.2461\n",
      "Epoch: 351, Loss: 3.4061, Train: 1.0888, Val: 1.1173, Test: 1.0924\n",
      "Epoch: 352, Loss: 3.3963, Train: 1.1622, Val: 1.1933, Test: 1.1803\n",
      "Epoch: 353, Loss: 3.4349, Train: 1.2022, Val: 1.2243, Test: 1.2249\n",
      "Epoch: 354, Loss: 3.4493, Train: 1.1097, Val: 1.1512, Test: 1.1162\n",
      "Epoch: 355, Loss: 3.4043, Train: 1.1307, Val: 1.1805, Test: 1.1574\n",
      "Epoch: 356, Loss: 3.3932, Train: 1.1893, Val: 1.2262, Test: 1.2649\n",
      "Epoch: 357, Loss: 3.3373, Train: 1.1908, Val: 1.2063, Test: 1.2041\n",
      "Epoch: 358, Loss: 3.3208, Train: 1.1064, Val: 1.1718, Test: 1.1483\n",
      "Epoch: 359, Loss: 3.3123, Train: 1.1430, Val: 1.1562, Test: 1.1722\n",
      "Epoch: 360, Loss: 3.3912, Train: 1.1993, Val: 1.2094, Test: 1.1984\n",
      "Epoch: 361, Loss: 3.3320, Train: 1.1202, Val: 1.1835, Test: 1.1754\n",
      "Epoch: 362, Loss: 3.3274, Train: 1.1157, Val: 1.1760, Test: 1.1570\n",
      "Epoch: 363, Loss: 3.3708, Train: 1.1839, Val: 1.2455, Test: 1.2266\n",
      "Epoch: 364, Loss: 3.3201, Train: 1.1987, Val: 1.2138, Test: 1.2397\n",
      "Epoch: 365, Loss: 3.2752, Train: 1.1151, Val: 1.1481, Test: 1.1430\n",
      "Epoch: 366, Loss: 3.2928, Train: 1.1623, Val: 1.1767, Test: 1.1637\n",
      "Epoch: 367, Loss: 3.2801, Train: 1.1369, Val: 1.1806, Test: 1.1803\n",
      "Epoch: 368, Loss: 3.3033, Train: 1.0900, Val: 1.1224, Test: 1.1196\n",
      "Epoch: 369, Loss: 3.3162, Train: 1.0832, Val: 1.1525, Test: 1.1210\n",
      "Epoch: 370, Loss: 3.2776, Train: 1.1723, Val: 1.2105, Test: 1.2070\n",
      "Epoch: 371, Loss: 3.3315, Train: 1.1605, Val: 1.1845, Test: 1.2020\n",
      "Epoch: 372, Loss: 3.2489, Train: 1.1352, Val: 1.1715, Test: 1.1622\n",
      "Epoch: 373, Loss: 3.2748, Train: 1.1751, Val: 1.1719, Test: 1.1871\n",
      "Epoch: 374, Loss: 3.2193, Train: 1.2289, Val: 1.2398, Test: 1.2526\n",
      "Epoch: 375, Loss: 3.3475, Train: 1.1315, Val: 1.1825, Test: 1.1843\n",
      "Epoch: 376, Loss: 3.2088, Train: 1.1141, Val: 1.1449, Test: 1.1158\n",
      "Epoch: 377, Loss: 3.3217, Train: 1.1450, Val: 1.1933, Test: 1.1921\n",
      "Epoch: 378, Loss: 3.2703, Train: 1.1867, Val: 1.2147, Test: 1.2098\n",
      "Epoch: 379, Loss: 3.2866, Train: 1.1231, Val: 1.1446, Test: 1.1318\n",
      "Epoch: 380, Loss: 3.3017, Train: 1.0821, Val: 1.1159, Test: 1.1231\n",
      "Epoch: 381, Loss: 3.3604, Train: 1.1295, Val: 1.1656, Test: 1.1754\n",
      "Epoch: 382, Loss: 3.2133, Train: 1.2053, Val: 1.2337, Test: 1.2406\n",
      "Epoch: 383, Loss: 3.2384, Train: 1.1777, Val: 1.1913, Test: 1.2214\n",
      "Epoch: 384, Loss: 3.2387, Train: 1.1050, Val: 1.1305, Test: 1.1199\n",
      "Epoch: 385, Loss: 3.3000, Train: 1.1336, Val: 1.1392, Test: 1.1215\n",
      "Epoch: 386, Loss: 3.3001, Train: 1.1706, Val: 1.1908, Test: 1.1950\n",
      "Epoch: 387, Loss: 3.2485, Train: 1.1792, Val: 1.1903, Test: 1.1869\n",
      "Epoch: 388, Loss: 3.2483, Train: 1.1192, Val: 1.1585, Test: 1.1388\n",
      "Epoch: 389, Loss: 3.3057, Train: 1.0922, Val: 1.1189, Test: 1.1157\n",
      "Epoch: 390, Loss: 3.2837, Train: 1.2027, Val: 1.2311, Test: 1.2553\n",
      "Epoch: 391, Loss: 3.2915, Train: 1.1934, Val: 1.1825, Test: 1.2314\n",
      "Epoch: 392, Loss: 3.2507, Train: 1.1190, Val: 1.1261, Test: 1.1458\n",
      "Epoch: 393, Loss: 3.2865, Train: 1.1648, Val: 1.2039, Test: 1.2158\n",
      "Epoch: 394, Loss: 3.2445, Train: 1.2098, Val: 1.2379, Test: 1.2219\n",
      "Epoch: 395, Loss: 3.2888, Train: 1.1051, Val: 1.1124, Test: 1.1320\n",
      "Epoch: 396, Loss: 3.2764, Train: 1.0897, Val: 1.1179, Test: 1.1580\n",
      "Epoch: 397, Loss: 3.2813, Train: 1.1826, Val: 1.2137, Test: 1.2020\n",
      "Epoch: 398, Loss: 3.2894, Train: 1.2248, Val: 1.2026, Test: 1.2070\n",
      "Epoch: 399, Loss: 3.2962, Train: 1.1120, Val: 1.1464, Test: 1.1804\n",
      "Epoch: 400, Loss: 3.2535, Train: 1.1526, Val: 1.1774, Test: 1.2079\n",
      "Epoch: 401, Loss: 3.1871, Train: 1.1345, Val: 1.1534, Test: 1.1609\n",
      "Epoch: 402, Loss: 3.2388, Train: 1.1119, Val: 1.1500, Test: 1.1259\n",
      "Epoch: 403, Loss: 3.2636, Train: 1.1291, Val: 1.1704, Test: 1.1426\n",
      "Epoch: 404, Loss: 3.1951, Train: 1.1726, Val: 1.2250, Test: 1.1993\n",
      "Epoch: 405, Loss: 3.2269, Train: 1.1355, Val: 1.1880, Test: 1.1805\n",
      "Epoch: 406, Loss: 3.2219, Train: 1.1449, Val: 1.1769, Test: 1.1766\n",
      "Epoch: 407, Loss: 3.2462, Train: 1.0958, Val: 1.1303, Test: 1.1375\n",
      "Epoch: 408, Loss: 3.1993, Train: 1.1202, Val: 1.1738, Test: 1.1584\n",
      "Epoch: 409, Loss: 3.2307, Train: 1.1535, Val: 1.1938, Test: 1.1698\n",
      "Epoch: 410, Loss: 3.2650, Train: 1.1373, Val: 1.1574, Test: 1.1381\n",
      "Epoch: 411, Loss: 3.1702, Train: 1.1274, Val: 1.1530, Test: 1.1649\n",
      "Epoch: 412, Loss: 3.2679, Train: 1.2300, Val: 1.2170, Test: 1.2197\n",
      "Epoch: 413, Loss: 3.2004, Train: 1.1673, Val: 1.1994, Test: 1.1905\n",
      "Epoch: 414, Loss: 3.2483, Train: 1.0709, Val: 1.1075, Test: 1.1168\n",
      "Epoch: 415, Loss: 3.2170, Train: 1.1644, Val: 1.2070, Test: 1.1867\n",
      "Epoch: 416, Loss: 3.2376, Train: 1.1929, Val: 1.2404, Test: 1.2115\n",
      "Epoch: 417, Loss: 3.2355, Train: 1.0632, Val: 1.1483, Test: 1.1384\n",
      "Epoch: 418, Loss: 3.2398, Train: 1.0741, Val: 1.1241, Test: 1.0978\n",
      "Epoch: 419, Loss: 3.2176, Train: 1.1705, Val: 1.2001, Test: 1.1797\n",
      "Epoch: 420, Loss: 3.2174, Train: 1.2034, Val: 1.2739, Test: 1.2621\n",
      "Epoch: 421, Loss: 3.2917, Train: 1.1167, Val: 1.1672, Test: 1.1556\n",
      "Epoch: 422, Loss: 3.2080, Train: 1.0820, Val: 1.1150, Test: 1.1049\n",
      "Epoch: 423, Loss: 3.3025, Train: 1.1007, Val: 1.1103, Test: 1.1457\n",
      "Epoch: 424, Loss: 3.2361, Train: 1.1785, Val: 1.2142, Test: 1.1867\n",
      "Epoch: 425, Loss: 3.2521, Train: 1.1537, Val: 1.1601, Test: 1.1856\n",
      "Epoch: 426, Loss: 3.1683, Train: 1.1325, Val: 1.1734, Test: 1.1547\n",
      "Epoch: 427, Loss: 3.2278, Train: 1.1720, Val: 1.2137, Test: 1.1905\n",
      "Epoch: 428, Loss: 3.2270, Train: 1.1620, Val: 1.2056, Test: 1.1855\n",
      "Epoch: 429, Loss: 3.2358, Train: 1.1138, Val: 1.1679, Test: 1.1747\n",
      "Epoch: 430, Loss: 3.2253, Train: 1.1685, Val: 1.1934, Test: 1.1649\n",
      "Epoch: 431, Loss: 3.2583, Train: 1.1232, Val: 1.1575, Test: 1.1505\n",
      "Epoch: 432, Loss: 3.2134, Train: 1.1062, Val: 1.1514, Test: 1.1713\n",
      "Epoch: 433, Loss: 3.2858, Train: 1.2104, Val: 1.2205, Test: 1.2200\n",
      "Epoch: 434, Loss: 3.2432, Train: 1.1269, Val: 1.1570, Test: 1.1571\n",
      "Epoch: 435, Loss: 3.1978, Train: 1.1272, Val: 1.1578, Test: 1.1715\n",
      "Epoch: 436, Loss: 3.3319, Train: 1.1405, Val: 1.1397, Test: 1.1562\n",
      "Epoch: 437, Loss: 3.2459, Train: 1.1595, Val: 1.2221, Test: 1.2168\n",
      "Epoch: 438, Loss: 3.2003, Train: 1.1747, Val: 1.2094, Test: 1.1962\n",
      "Epoch: 439, Loss: 3.2422, Train: 1.1059, Val: 1.1399, Test: 1.1620\n",
      "Epoch: 440, Loss: 3.2580, Train: 1.1135, Val: 1.1487, Test: 1.1263\n",
      "Epoch: 441, Loss: 3.1864, Train: 1.1135, Val: 1.1503, Test: 1.1474\n",
      "Epoch: 442, Loss: 3.2200, Train: 1.1442, Val: 1.1531, Test: 1.1745\n",
      "Epoch: 443, Loss: 3.1381, Train: 1.1371, Val: 1.1890, Test: 1.1702\n",
      "Epoch: 444, Loss: 3.2019, Train: 1.1804, Val: 1.2513, Test: 1.2372\n",
      "Epoch: 445, Loss: 3.1721, Train: 1.1382, Val: 1.1735, Test: 1.1512\n",
      "Epoch: 446, Loss: 3.1777, Train: 1.1323, Val: 1.1174, Test: 1.1334\n",
      "Epoch: 447, Loss: 3.1982, Train: 1.1274, Val: 1.1618, Test: 1.1881\n",
      "Epoch: 448, Loss: 3.1846, Train: 1.1404, Val: 1.1595, Test: 1.1639\n",
      "Epoch: 449, Loss: 3.1909, Train: 1.1158, Val: 1.1591, Test: 1.1837\n",
      "Epoch: 450, Loss: 3.1652, Train: 1.1113, Val: 1.1909, Test: 1.1759\n",
      "Epoch: 451, Loss: 3.1503, Train: 1.1671, Val: 1.1781, Test: 1.1781\n",
      "Epoch: 452, Loss: 3.1241, Train: 1.1533, Val: 1.2197, Test: 1.1963\n",
      "Epoch: 453, Loss: 3.1637, Train: 1.1192, Val: 1.1551, Test: 1.1244\n",
      "Epoch: 454, Loss: 3.1400, Train: 1.0808, Val: 1.1251, Test: 1.1403\n",
      "Epoch: 455, Loss: 3.2165, Train: 1.1195, Val: 1.1938, Test: 1.1556\n",
      "Epoch: 456, Loss: 3.1625, Train: 1.1544, Val: 1.2203, Test: 1.2159\n",
      "Epoch: 457, Loss: 3.1806, Train: 1.1640, Val: 1.1633, Test: 1.1650\n",
      "Epoch: 458, Loss: 3.2223, Train: 1.1387, Val: 1.1261, Test: 1.1591\n",
      "Epoch: 459, Loss: 3.2006, Train: 1.1057, Val: 1.1487, Test: 1.1460\n",
      "Epoch: 460, Loss: 3.1478, Train: 1.1605, Val: 1.1999, Test: 1.2017\n",
      "Epoch: 461, Loss: 3.1867, Train: 1.0956, Val: 1.1673, Test: 1.1374\n",
      "Epoch: 462, Loss: 3.1231, Train: 1.1238, Val: 1.1136, Test: 1.1442\n",
      "Epoch: 463, Loss: 3.1894, Train: 1.2373, Val: 1.2503, Test: 1.2684\n",
      "Epoch: 464, Loss: 3.2931, Train: 1.1163, Val: 1.1596, Test: 1.1895\n",
      "Epoch: 465, Loss: 3.1349, Train: 1.0857, Val: 1.1259, Test: 1.1402\n",
      "Epoch: 466, Loss: 3.1874, Train: 1.0929, Val: 1.1732, Test: 1.1740\n",
      "Epoch: 467, Loss: 3.1771, Train: 1.1522, Val: 1.1975, Test: 1.2450\n",
      "Epoch: 468, Loss: 3.1685, Train: 1.1490, Val: 1.2024, Test: 1.2047\n",
      "Epoch: 469, Loss: 3.1603, Train: 1.1540, Val: 1.1637, Test: 1.1527\n",
      "Epoch: 470, Loss: 3.1341, Train: 1.1515, Val: 1.1782, Test: 1.2055\n",
      "Epoch: 471, Loss: 3.1098, Train: 1.1044, Val: 1.1720, Test: 1.1726\n",
      "Epoch: 472, Loss: 3.1414, Train: 1.1179, Val: 1.1419, Test: 1.1660\n",
      "Epoch: 473, Loss: 3.1097, Train: 1.1128, Val: 1.1689, Test: 1.1687\n",
      "Epoch: 474, Loss: 3.2006, Train: 1.1505, Val: 1.1920, Test: 1.1812\n",
      "Epoch: 475, Loss: 3.1221, Train: 1.1630, Val: 1.2125, Test: 1.2266\n",
      "Epoch: 476, Loss: 3.2010, Train: 1.0713, Val: 1.1099, Test: 1.1180\n",
      "Epoch: 477, Loss: 3.1316, Train: 1.1690, Val: 1.2198, Test: 1.2114\n",
      "Epoch: 478, Loss: 3.1435, Train: 1.1864, Val: 1.2189, Test: 1.2122\n",
      "Epoch: 479, Loss: 3.0992, Train: 1.0717, Val: 1.1412, Test: 1.1457\n",
      "Epoch: 480, Loss: 3.1230, Train: 1.0742, Val: 1.1569, Test: 1.1279\n",
      "Epoch: 481, Loss: 3.1045, Train: 1.1554, Val: 1.1870, Test: 1.2430\n",
      "Epoch: 482, Loss: 3.1189, Train: 1.1029, Val: 1.1535, Test: 1.1546\n",
      "Epoch: 483, Loss: 3.0536, Train: 1.0949, Val: 1.1158, Test: 1.1308\n",
      "Epoch: 484, Loss: 3.1516, Train: 1.1847, Val: 1.2228, Test: 1.2323\n",
      "Epoch: 485, Loss: 3.1923, Train: 1.1566, Val: 1.2138, Test: 1.2069\n",
      "Epoch: 486, Loss: 3.1587, Train: 1.0485, Val: 1.1334, Test: 1.0873\n",
      "Epoch: 487, Loss: 3.1977, Train: 1.1193, Val: 1.1660, Test: 1.1478\n",
      "Epoch: 488, Loss: 3.0837, Train: 1.2573, Val: 1.2425, Test: 1.2666\n",
      "Epoch: 489, Loss: 3.1459, Train: 1.1256, Val: 1.1399, Test: 1.1596\n",
      "Epoch: 490, Loss: 3.0685, Train: 1.0673, Val: 1.1011, Test: 1.0928\n",
      "Epoch: 491, Loss: 3.2185, Train: 1.1530, Val: 1.1871, Test: 1.1877\n",
      "Epoch: 492, Loss: 3.1056, Train: 1.1804, Val: 1.2182, Test: 1.2249\n",
      "Epoch: 493, Loss: 3.0898, Train: 1.0816, Val: 1.1350, Test: 1.1418\n",
      "Epoch: 494, Loss: 3.0661, Train: 1.0914, Val: 1.1565, Test: 1.1277\n",
      "Epoch: 495, Loss: 3.0853, Train: 1.2067, Val: 1.2677, Test: 1.2706\n",
      "Epoch: 496, Loss: 3.1091, Train: 1.1916, Val: 1.1900, Test: 1.2270\n",
      "Epoch: 497, Loss: 3.0889, Train: 1.0800, Val: 1.1389, Test: 1.1014\n",
      "Epoch: 498, Loss: 3.2296, Train: 1.1008, Val: 1.1568, Test: 1.1580\n",
      "Epoch: 499, Loss: 3.0913, Train: 1.1823, Val: 1.2059, Test: 1.2212\n",
      "Epoch: 500, Loss: 3.1425, Train: 1.1176, Val: 1.1811, Test: 1.1867\n",
      "Epoch: 501, Loss: 3.1190, Train: 1.0900, Val: 1.1106, Test: 1.1024\n",
      "Epoch: 502, Loss: 3.1752, Train: 1.1679, Val: 1.2193, Test: 1.2554\n",
      "Epoch: 503, Loss: 3.1010, Train: 1.1872, Val: 1.2814, Test: 1.2495\n",
      "Epoch: 504, Loss: 3.2497, Train: 1.0823, Val: 1.1212, Test: 1.1379\n",
      "Epoch: 505, Loss: 3.0943, Train: 1.0313, Val: 1.0934, Test: 1.0781\n",
      "Epoch: 506, Loss: 3.1946, Train: 1.1610, Val: 1.1823, Test: 1.2226\n",
      "Epoch: 507, Loss: 3.0505, Train: 1.2138, Val: 1.2492, Test: 1.2643\n",
      "Epoch: 508, Loss: 3.2560, Train: 1.0824, Val: 1.1206, Test: 1.0907\n",
      "Epoch: 509, Loss: 3.1112, Train: 1.0942, Val: 1.1470, Test: 1.1306\n",
      "Epoch: 510, Loss: 3.1080, Train: 1.2100, Val: 1.2080, Test: 1.2435\n",
      "Epoch: 511, Loss: 3.2113, Train: 1.1231, Val: 1.1773, Test: 1.1602\n",
      "Epoch: 512, Loss: 3.0206, Train: 1.0881, Val: 1.1256, Test: 1.1203\n",
      "Epoch: 513, Loss: 3.0813, Train: 1.1031, Val: 1.1540, Test: 1.1521\n",
      "Epoch: 514, Loss: 3.0891, Train: 1.1169, Val: 1.1897, Test: 1.1811\n",
      "Epoch: 515, Loss: 3.0896, Train: 1.1277, Val: 1.1585, Test: 1.1533\n",
      "Epoch: 516, Loss: 3.0673, Train: 1.0968, Val: 1.1489, Test: 1.1365\n",
      "Epoch: 517, Loss: 3.0809, Train: 1.1164, Val: 1.1691, Test: 1.1955\n",
      "Epoch: 518, Loss: 3.0548, Train: 1.1982, Val: 1.2115, Test: 1.2325\n",
      "Epoch: 519, Loss: 3.1431, Train: 1.1147, Val: 1.1606, Test: 1.1741\n",
      "Epoch: 520, Loss: 3.0396, Train: 1.0391, Val: 1.0813, Test: 1.0933\n",
      "Epoch: 521, Loss: 3.1403, Train: 1.0725, Val: 1.1507, Test: 1.1496\n",
      "Epoch: 522, Loss: 3.0295, Train: 1.1624, Val: 1.2113, Test: 1.2134\n",
      "Epoch: 523, Loss: 3.0887, Train: 1.1831, Val: 1.2056, Test: 1.2519\n",
      "Epoch: 524, Loss: 3.1023, Train: 1.1164, Val: 1.1511, Test: 1.1723\n",
      "Epoch: 525, Loss: 3.0795, Train: 1.0831, Val: 1.1185, Test: 1.1355\n",
      "Epoch: 526, Loss: 3.0476, Train: 1.0956, Val: 1.1411, Test: 1.1225\n",
      "Epoch: 527, Loss: 3.0340, Train: 1.1174, Val: 1.1765, Test: 1.1881\n",
      "Epoch: 528, Loss: 3.0376, Train: 1.1184, Val: 1.1891, Test: 1.1435\n",
      "Epoch: 529, Loss: 3.0124, Train: 1.1263, Val: 1.1990, Test: 1.1763\n",
      "Epoch: 530, Loss: 3.0239, Train: 1.1727, Val: 1.1709, Test: 1.1623\n",
      "Epoch: 531, Loss: 3.0311, Train: 1.1167, Val: 1.1741, Test: 1.1568\n",
      "Epoch: 532, Loss: 3.0290, Train: 1.1261, Val: 1.1421, Test: 1.1624\n",
      "Epoch: 533, Loss: 3.0075, Train: 1.1213, Val: 1.1596, Test: 1.1475\n",
      "Epoch: 534, Loss: 3.0288, Train: 1.0656, Val: 1.1071, Test: 1.1205\n",
      "Epoch: 535, Loss: 3.0165, Train: 1.1209, Val: 1.1778, Test: 1.2065\n",
      "Epoch: 536, Loss: 3.0058, Train: 1.1801, Val: 1.2605, Test: 1.2105\n",
      "Epoch: 537, Loss: 3.0638, Train: 1.0910, Val: 1.1466, Test: 1.1429\n",
      "Epoch: 538, Loss: 3.0129, Train: 1.0469, Val: 1.1096, Test: 1.1150\n",
      "Epoch: 539, Loss: 3.1173, Train: 1.2048, Val: 1.2521, Test: 1.2360\n",
      "Epoch: 540, Loss: 3.0991, Train: 1.2107, Val: 1.2971, Test: 1.2723\n",
      "Epoch: 541, Loss: 3.2174, Train: 1.1029, Val: 1.1310, Test: 1.1120\n",
      "Epoch: 542, Loss: 3.1267, Train: 1.0080, Val: 1.0604, Test: 1.0592\n",
      "Epoch: 543, Loss: 3.1964, Train: 1.1244, Val: 1.1870, Test: 1.1859\n",
      "Epoch: 544, Loss: 3.1324, Train: 1.1879, Val: 1.2248, Test: 1.2416\n",
      "Epoch: 545, Loss: 3.0878, Train: 1.1618, Val: 1.1686, Test: 1.2030\n",
      "Epoch: 546, Loss: 3.0746, Train: 1.0971, Val: 1.1602, Test: 1.1496\n",
      "Epoch: 547, Loss: 3.0633, Train: 1.1304, Val: 1.1877, Test: 1.1752\n",
      "Epoch: 548, Loss: 3.0158, Train: 1.1732, Val: 1.2122, Test: 1.2094\n",
      "Epoch: 549, Loss: 3.0849, Train: 1.0900, Val: 1.1323, Test: 1.1436\n",
      "Epoch: 550, Loss: 3.0697, Train: 1.0919, Val: 1.1321, Test: 1.1278\n",
      "Epoch: 551, Loss: 3.0450, Train: 1.1712, Val: 1.2215, Test: 1.2304\n",
      "Epoch: 552, Loss: 3.0350, Train: 1.1815, Val: 1.2226, Test: 1.2704\n",
      "Epoch: 553, Loss: 3.0816, Train: 1.0765, Val: 1.1231, Test: 1.1322\n",
      "Epoch: 554, Loss: 3.0990, Train: 1.0519, Val: 1.0953, Test: 1.1182\n",
      "Epoch: 555, Loss: 3.1204, Train: 1.1533, Val: 1.1953, Test: 1.2159\n",
      "Epoch: 556, Loss: 3.0883, Train: 1.2325, Val: 1.2365, Test: 1.2428\n",
      "Epoch: 557, Loss: 3.1683, Train: 1.0878, Val: 1.1132, Test: 1.1129\n",
      "Epoch: 558, Loss: 3.0423, Train: 1.0725, Val: 1.1066, Test: 1.0862\n",
      "Epoch: 559, Loss: 3.1086, Train: 1.1354, Val: 1.1765, Test: 1.1702\n",
      "Epoch: 560, Loss: 3.0234, Train: 1.1685, Val: 1.2080, Test: 1.2318\n",
      "Epoch: 561, Loss: 3.1388, Train: 1.1082, Val: 1.1433, Test: 1.1438\n",
      "Epoch: 562, Loss: 3.0411, Train: 1.0430, Val: 1.1061, Test: 1.1036\n",
      "Epoch: 563, Loss: 3.0548, Train: 1.0919, Val: 1.1505, Test: 1.1396\n",
      "Epoch: 564, Loss: 2.9751, Train: 1.1808, Val: 1.2039, Test: 1.2176\n",
      "Epoch: 565, Loss: 3.0212, Train: 1.1301, Val: 1.1459, Test: 1.1736\n",
      "Epoch: 566, Loss: 2.9842, Train: 1.0823, Val: 1.1315, Test: 1.1558\n",
      "Epoch: 567, Loss: 3.0235, Train: 1.0973, Val: 1.1605, Test: 1.1394\n",
      "Epoch: 568, Loss: 2.9986, Train: 1.1368, Val: 1.1832, Test: 1.1928\n",
      "Epoch: 569, Loss: 3.0673, Train: 1.1639, Val: 1.1926, Test: 1.1791\n",
      "Epoch: 570, Loss: 2.9876, Train: 1.1183, Val: 1.1691, Test: 1.1492\n",
      "Epoch: 571, Loss: 2.9445, Train: 1.0998, Val: 1.1443, Test: 1.1565\n",
      "Epoch: 572, Loss: 3.0023, Train: 1.1347, Val: 1.2000, Test: 1.1792\n",
      "Epoch: 573, Loss: 3.0273, Train: 1.1112, Val: 1.1717, Test: 1.1762\n",
      "Epoch: 574, Loss: 2.9551, Train: 1.0707, Val: 1.1499, Test: 1.1518\n",
      "Epoch: 575, Loss: 2.9952, Train: 1.1213, Val: 1.1642, Test: 1.1799\n",
      "Epoch: 576, Loss: 3.0153, Train: 1.0810, Val: 1.1314, Test: 1.1570\n",
      "Epoch: 577, Loss: 2.9559, Train: 1.1155, Val: 1.1648, Test: 1.1505\n",
      "Epoch: 578, Loss: 2.9579, Train: 1.1200, Val: 1.2027, Test: 1.1716\n",
      "Epoch: 579, Loss: 2.9972, Train: 1.0438, Val: 1.1286, Test: 1.1028\n",
      "Epoch: 580, Loss: 2.9772, Train: 1.0868, Val: 1.1559, Test: 1.1454\n",
      "Epoch: 581, Loss: 2.9777, Train: 1.1530, Val: 1.1723, Test: 1.1736\n",
      "Epoch: 582, Loss: 2.9807, Train: 1.1155, Val: 1.1540, Test: 1.1579\n",
      "Epoch: 583, Loss: 3.0124, Train: 1.0646, Val: 1.1348, Test: 1.1009\n",
      "Epoch: 584, Loss: 3.0449, Train: 1.1928, Val: 1.2310, Test: 1.2173\n",
      "Epoch: 585, Loss: 3.0184, Train: 1.1736, Val: 1.2147, Test: 1.2003\n",
      "Epoch: 586, Loss: 2.9669, Train: 1.0620, Val: 1.1274, Test: 1.0989\n",
      "Epoch: 587, Loss: 2.9874, Train: 1.0673, Val: 1.1102, Test: 1.1175\n",
      "Epoch: 588, Loss: 2.9989, Train: 1.1335, Val: 1.1788, Test: 1.1745\n",
      "Epoch: 589, Loss: 2.9692, Train: 1.1556, Val: 1.2200, Test: 1.1878\n",
      "Epoch: 590, Loss: 2.9807, Train: 1.0966, Val: 1.1615, Test: 1.1617\n",
      "Epoch: 591, Loss: 2.9355, Train: 1.0709, Val: 1.1067, Test: 1.1269\n",
      "Epoch: 592, Loss: 3.0085, Train: 1.1384, Val: 1.1792, Test: 1.1830\n",
      "Epoch: 593, Loss: 2.9780, Train: 1.1637, Val: 1.2173, Test: 1.2167\n",
      "Epoch: 594, Loss: 2.9870, Train: 1.0885, Val: 1.1341, Test: 1.1379\n",
      "Epoch: 595, Loss: 2.9288, Train: 1.0442, Val: 1.1195, Test: 1.0826\n",
      "Epoch: 596, Loss: 2.9709, Train: 1.1438, Val: 1.2014, Test: 1.2003\n",
      "Epoch: 597, Loss: 2.9856, Train: 1.1450, Val: 1.2218, Test: 1.1791\n",
      "Epoch: 598, Loss: 3.0492, Train: 1.0313, Val: 1.0916, Test: 1.0956\n",
      "Epoch: 599, Loss: 3.0650, Train: 1.0665, Val: 1.1143, Test: 1.1045\n",
      "Epoch: 600, Loss: 3.0021, Train: 1.1565, Val: 1.1887, Test: 1.1778\n",
      "Epoch: 601, Loss: 3.0177, Train: 1.1435, Val: 1.1933, Test: 1.1842\n",
      "Epoch: 602, Loss: 2.9550, Train: 1.0913, Val: 1.1372, Test: 1.1526\n",
      "Epoch: 603, Loss: 2.9901, Train: 1.1004, Val: 1.1549, Test: 1.1513\n",
      "Epoch: 604, Loss: 2.9332, Train: 1.1270, Val: 1.1949, Test: 1.1462\n",
      "Epoch: 605, Loss: 2.9542, Train: 1.1052, Val: 1.1490, Test: 1.1595\n",
      "Epoch: 606, Loss: 2.9405, Train: 1.0998, Val: 1.1362, Test: 1.1260\n",
      "Epoch: 607, Loss: 2.9853, Train: 1.1382, Val: 1.1816, Test: 1.1712\n",
      "Epoch: 608, Loss: 2.9562, Train: 1.1304, Val: 1.1743, Test: 1.1810\n",
      "Epoch: 609, Loss: 2.9871, Train: 1.1192, Val: 1.1485, Test: 1.1522\n",
      "Epoch: 610, Loss: 2.9224, Train: 1.1028, Val: 1.1305, Test: 1.1462\n",
      "Epoch: 611, Loss: 2.9358, Train: 1.0783, Val: 1.1424, Test: 1.1673\n",
      "Epoch: 612, Loss: 2.9227, Train: 1.1253, Val: 1.1814, Test: 1.1588\n",
      "Epoch: 613, Loss: 2.9660, Train: 1.0917, Val: 1.1473, Test: 1.1386\n",
      "Epoch: 614, Loss: 2.9335, Train: 1.1141, Val: 1.1448, Test: 1.1513\n",
      "Epoch: 615, Loss: 2.9162, Train: 1.1133, Val: 1.1639, Test: 1.1585\n",
      "Epoch: 616, Loss: 2.9493, Train: 1.0742, Val: 1.1300, Test: 1.1340\n",
      "Epoch: 617, Loss: 2.9117, Train: 1.1477, Val: 1.1753, Test: 1.1634\n",
      "Epoch: 618, Loss: 2.9508, Train: 1.0938, Val: 1.1409, Test: 1.1419\n",
      "Epoch: 619, Loss: 2.9212, Train: 1.0978, Val: 1.1340, Test: 1.1284\n",
      "Epoch: 620, Loss: 2.9128, Train: 1.1163, Val: 1.1719, Test: 1.1673\n",
      "Epoch: 621, Loss: 2.9235, Train: 1.0884, Val: 1.1299, Test: 1.1430\n",
      "Epoch: 622, Loss: 2.9204, Train: 1.0846, Val: 1.1569, Test: 1.1768\n",
      "Epoch: 623, Loss: 2.9377, Train: 1.1169, Val: 1.1866, Test: 1.1985\n",
      "Epoch: 624, Loss: 2.9114, Train: 1.1266, Val: 1.1732, Test: 1.1799\n",
      "Epoch: 625, Loss: 2.9087, Train: 1.0645, Val: 1.1363, Test: 1.1129\n",
      "Epoch: 626, Loss: 2.9541, Train: 1.0817, Val: 1.1530, Test: 1.1426\n",
      "Epoch: 627, Loss: 2.9834, Train: 1.1396, Val: 1.1790, Test: 1.1738\n",
      "Epoch: 628, Loss: 2.8943, Train: 1.1682, Val: 1.1800, Test: 1.2031\n",
      "Epoch: 629, Loss: 3.0513, Train: 1.1135, Val: 1.1678, Test: 1.1605\n",
      "Epoch: 630, Loss: 3.0019, Train: 1.0641, Val: 1.1191, Test: 1.1047\n",
      "Epoch: 631, Loss: 2.9198, Train: 1.1064, Val: 1.1683, Test: 1.1579\n",
      "Epoch: 632, Loss: 2.9788, Train: 1.1344, Val: 1.1970, Test: 1.1763\n",
      "Epoch: 633, Loss: 2.9604, Train: 1.1421, Val: 1.1716, Test: 1.1832\n",
      "Epoch: 634, Loss: 2.9695, Train: 1.1269, Val: 1.1868, Test: 1.1815\n",
      "Epoch: 635, Loss: 2.9402, Train: 1.1380, Val: 1.1869, Test: 1.1692\n",
      "Epoch: 636, Loss: 2.8500, Train: 1.0976, Val: 1.1537, Test: 1.1305\n",
      "Epoch: 637, Loss: 2.9293, Train: 1.0786, Val: 1.1424, Test: 1.1265\n",
      "Epoch: 638, Loss: 2.8622, Train: 1.0878, Val: 1.1434, Test: 1.1227\n",
      "Epoch: 639, Loss: 2.8891, Train: 1.1475, Val: 1.2197, Test: 1.1792\n",
      "Epoch: 640, Loss: 2.9156, Train: 1.1293, Val: 1.1775, Test: 1.1840\n",
      "Epoch: 641, Loss: 2.9725, Train: 1.0600, Val: 1.1245, Test: 1.0937\n",
      "Epoch: 642, Loss: 2.9467, Train: 1.0885, Val: 1.1355, Test: 1.1225\n",
      "Epoch: 643, Loss: 2.9695, Train: 1.1695, Val: 1.2412, Test: 1.2069\n",
      "Epoch: 644, Loss: 3.0337, Train: 1.0988, Val: 1.1232, Test: 1.1340\n",
      "Epoch: 645, Loss: 2.9281, Train: 1.0853, Val: 1.1241, Test: 1.1190\n",
      "Epoch: 646, Loss: 2.9270, Train: 1.1507, Val: 1.1966, Test: 1.1967\n",
      "Epoch: 647, Loss: 2.9072, Train: 1.1145, Val: 1.1769, Test: 1.1624\n",
      "Epoch: 648, Loss: 2.9209, Train: 1.0437, Val: 1.1042, Test: 1.1012\n",
      "Epoch: 649, Loss: 2.9175, Train: 1.0637, Val: 1.1285, Test: 1.1174\n",
      "Epoch: 650, Loss: 2.8296, Train: 1.1130, Val: 1.1808, Test: 1.1722\n",
      "Epoch: 651, Loss: 2.8967, Train: 1.1025, Val: 1.1617, Test: 1.1538\n",
      "Epoch: 652, Loss: 2.8888, Train: 1.0658, Val: 1.1352, Test: 1.1362\n",
      "Epoch: 653, Loss: 2.8800, Train: 1.1171, Val: 1.1504, Test: 1.1691\n",
      "Epoch: 654, Loss: 2.8680, Train: 1.1200, Val: 1.1813, Test: 1.1760\n",
      "Epoch: 655, Loss: 2.8540, Train: 1.0957, Val: 1.1445, Test: 1.1303\n",
      "Epoch: 656, Loss: 2.9172, Train: 1.1047, Val: 1.1273, Test: 1.1541\n",
      "Epoch: 657, Loss: 2.8647, Train: 1.1043, Val: 1.1610, Test: 1.1707\n",
      "Epoch: 658, Loss: 2.8432, Train: 1.1213, Val: 1.1628, Test: 1.1750\n",
      "Epoch: 659, Loss: 2.9290, Train: 1.1146, Val: 1.1276, Test: 1.1374\n",
      "Epoch: 660, Loss: 2.8878, Train: 1.0650, Val: 1.1159, Test: 1.1236\n",
      "Epoch: 661, Loss: 2.8841, Train: 1.1089, Val: 1.1633, Test: 1.1535\n",
      "Epoch: 662, Loss: 2.8337, Train: 1.1255, Val: 1.1932, Test: 1.1480\n",
      "Epoch: 663, Loss: 2.8545, Train: 1.0952, Val: 1.1452, Test: 1.1386\n",
      "Epoch: 664, Loss: 2.8447, Train: 1.1113, Val: 1.1602, Test: 1.1425\n",
      "Epoch: 665, Loss: 2.8685, Train: 1.0825, Val: 1.1466, Test: 1.1461\n",
      "Epoch: 666, Loss: 2.9781, Train: 1.0573, Val: 1.0971, Test: 1.1154\n",
      "Epoch: 667, Loss: 2.9346, Train: 1.0956, Val: 1.1567, Test: 1.1650\n",
      "Epoch: 668, Loss: 2.9115, Train: 1.1005, Val: 1.1580, Test: 1.1607\n",
      "Epoch: 669, Loss: 2.8373, Train: 1.1119, Val: 1.1695, Test: 1.1825\n",
      "Epoch: 670, Loss: 2.9011, Train: 1.0607, Val: 1.1177, Test: 1.1142\n",
      "Epoch: 671, Loss: 2.8845, Train: 1.1993, Val: 1.2317, Test: 1.2708\n",
      "Epoch: 672, Loss: 2.9627, Train: 1.0894, Val: 1.1443, Test: 1.1495\n",
      "Epoch: 673, Loss: 2.8887, Train: 1.0216, Val: 1.0667, Test: 1.0612\n",
      "Epoch: 674, Loss: 3.0206, Train: 1.1187, Val: 1.1752, Test: 1.1933\n",
      "Epoch: 675, Loss: 2.8415, Train: 1.1953, Val: 1.2557, Test: 1.2334\n",
      "Epoch: 676, Loss: 2.9560, Train: 1.0980, Val: 1.1557, Test: 1.1594\n",
      "Epoch: 677, Loss: 2.9185, Train: 1.0242, Val: 1.0711, Test: 1.0716\n",
      "Epoch: 678, Loss: 2.9985, Train: 1.0815, Val: 1.1194, Test: 1.1085\n",
      "Epoch: 679, Loss: 2.8550, Train: 1.1606, Val: 1.2056, Test: 1.2355\n",
      "Epoch: 680, Loss: 3.0163, Train: 1.1109, Val: 1.1579, Test: 1.1443\n",
      "Epoch: 681, Loss: 2.8941, Train: 1.0451, Val: 1.1226, Test: 1.1285\n",
      "Epoch: 682, Loss: 2.9753, Train: 1.1603, Val: 1.2391, Test: 1.2319\n",
      "Epoch: 683, Loss: 2.9118, Train: 1.1467, Val: 1.1928, Test: 1.2041\n",
      "Epoch: 684, Loss: 2.8477, Train: 1.0585, Val: 1.1009, Test: 1.1007\n",
      "Epoch: 685, Loss: 2.8512, Train: 1.0709, Val: 1.1017, Test: 1.1118\n",
      "Epoch: 686, Loss: 2.9576, Train: 1.1068, Val: 1.1579, Test: 1.1404\n",
      "Epoch: 687, Loss: 2.8557, Train: 1.1183, Val: 1.1514, Test: 1.1619\n",
      "Epoch: 688, Loss: 2.8160, Train: 1.1018, Val: 1.1524, Test: 1.1429\n",
      "Epoch: 689, Loss: 2.8746, Train: 1.0579, Val: 1.1279, Test: 1.1331\n",
      "Epoch: 690, Loss: 2.9150, Train: 1.0987, Val: 1.1378, Test: 1.1684\n",
      "Epoch: 691, Loss: 2.8527, Train: 1.1117, Val: 1.1603, Test: 1.1660\n",
      "Epoch: 692, Loss: 2.8643, Train: 1.0837, Val: 1.1206, Test: 1.1270\n",
      "Epoch: 693, Loss: 2.8235, Train: 1.0589, Val: 1.1280, Test: 1.1178\n",
      "Epoch: 694, Loss: 2.8827, Train: 1.0896, Val: 1.1647, Test: 1.1410\n",
      "Epoch: 695, Loss: 2.8339, Train: 1.1301, Val: 1.1763, Test: 1.1966\n",
      "Epoch: 696, Loss: 2.8841, Train: 1.0833, Val: 1.1450, Test: 1.1531\n",
      "Epoch: 697, Loss: 2.9009, Train: 1.0581, Val: 1.1171, Test: 1.1218\n",
      "Epoch: 698, Loss: 2.8343, Train: 1.1199, Val: 1.1615, Test: 1.1708\n",
      "Epoch: 699, Loss: 2.8960, Train: 1.1220, Val: 1.1623, Test: 1.1710\n",
      "Epoch: 700, Loss: 2.8084, Train: 1.1011, Val: 1.1851, Test: 1.1777\n",
      "Epoch: 701, Loss: 2.8499, Train: 1.0942, Val: 1.1508, Test: 1.1445\n",
      "Epoch: 702, Loss: 2.8472, Train: 1.0704, Val: 1.1062, Test: 1.1485\n",
      "Epoch: 703, Loss: 2.8948, Train: 1.1256, Val: 1.1870, Test: 1.1831\n",
      "Epoch: 704, Loss: 2.8885, Train: 1.1046, Val: 1.1580, Test: 1.1418\n",
      "Epoch: 705, Loss: 2.8465, Train: 1.1190, Val: 1.1787, Test: 1.1468\n",
      "Epoch: 706, Loss: 2.8784, Train: 1.0711, Val: 1.1277, Test: 1.1233\n",
      "Epoch: 707, Loss: 2.8558, Train: 1.1573, Val: 1.2108, Test: 1.2030\n",
      "Epoch: 708, Loss: 2.8663, Train: 1.1104, Val: 1.1755, Test: 1.1498\n",
      "Epoch: 709, Loss: 2.8470, Train: 1.0164, Val: 1.0747, Test: 1.0727\n",
      "Epoch: 710, Loss: 2.9277, Train: 1.1058, Val: 1.1594, Test: 1.1517\n",
      "Epoch: 711, Loss: 2.8422, Train: 1.1475, Val: 1.2078, Test: 1.2253\n",
      "Epoch: 712, Loss: 2.8511, Train: 1.0752, Val: 1.1320, Test: 1.1511\n",
      "Epoch: 713, Loss: 2.8390, Train: 1.0505, Val: 1.1101, Test: 1.1103\n",
      "Epoch: 714, Loss: 2.8837, Train: 1.1231, Val: 1.1597, Test: 1.2023\n",
      "Epoch: 715, Loss: 2.8240, Train: 1.1200, Val: 1.1774, Test: 1.1894\n",
      "Epoch: 716, Loss: 2.7934, Train: 1.0528, Val: 1.1257, Test: 1.1299\n",
      "Epoch: 717, Loss: 2.8430, Train: 1.0682, Val: 1.1279, Test: 1.1085\n",
      "Epoch: 718, Loss: 2.9295, Train: 1.0936, Val: 1.1520, Test: 1.1204\n",
      "Epoch: 719, Loss: 2.8418, Train: 1.1014, Val: 1.1591, Test: 1.1461\n",
      "Epoch: 720, Loss: 2.8877, Train: 1.0970, Val: 1.1484, Test: 1.1580\n",
      "Epoch: 721, Loss: 2.8185, Train: 1.0863, Val: 1.1506, Test: 1.1772\n",
      "Epoch: 722, Loss: 2.8068, Train: 1.1192, Val: 1.1970, Test: 1.1881\n",
      "Epoch: 723, Loss: 2.8920, Train: 1.0511, Val: 1.0947, Test: 1.1038\n",
      "Epoch: 724, Loss: 2.8602, Train: 1.0351, Val: 1.0931, Test: 1.0975\n",
      "Epoch: 725, Loss: 2.8793, Train: 1.1603, Val: 1.2402, Test: 1.2390\n",
      "Epoch: 726, Loss: 2.9022, Train: 1.1326, Val: 1.2015, Test: 1.1890\n",
      "Epoch: 727, Loss: 2.8337, Train: 1.0541, Val: 1.1138, Test: 1.1187\n",
      "Epoch: 728, Loss: 2.9203, Train: 1.1095, Val: 1.1500, Test: 1.1385\n",
      "Epoch: 729, Loss: 2.8260, Train: 1.1314, Val: 1.2064, Test: 1.2053\n",
      "Epoch: 730, Loss: 2.8533, Train: 1.0929, Val: 1.1425, Test: 1.1646\n",
      "Epoch: 731, Loss: 2.8607, Train: 1.0229, Val: 1.0570, Test: 1.0754\n",
      "Epoch: 732, Loss: 2.8199, Train: 1.0837, Val: 1.1395, Test: 1.1181\n",
      "Epoch: 733, Loss: 2.8187, Train: 1.1488, Val: 1.1919, Test: 1.2087\n",
      "Epoch: 734, Loss: 2.8683, Train: 1.0996, Val: 1.1326, Test: 1.1424\n",
      "Epoch: 735, Loss: 2.7713, Train: 1.0552, Val: 1.1010, Test: 1.1224\n",
      "Epoch: 736, Loss: 2.8071, Train: 1.0706, Val: 1.1254, Test: 1.1289\n",
      "Epoch: 737, Loss: 2.7874, Train: 1.0856, Val: 1.1467, Test: 1.1446\n",
      "Epoch: 738, Loss: 2.8027, Train: 1.0817, Val: 1.1423, Test: 1.1400\n",
      "Epoch: 739, Loss: 2.7703, Train: 1.1270, Val: 1.1613, Test: 1.1548\n",
      "Epoch: 740, Loss: 2.8132, Train: 1.0732, Val: 1.1109, Test: 1.1316\n",
      "Epoch: 741, Loss: 2.7799, Train: 1.0612, Val: 1.1303, Test: 1.1421\n",
      "Epoch: 742, Loss: 2.7991, Train: 1.0747, Val: 1.1258, Test: 1.1363\n",
      "Epoch: 743, Loss: 2.7732, Train: 1.1325, Val: 1.1684, Test: 1.1853\n",
      "Epoch: 744, Loss: 2.8374, Train: 1.0699, Val: 1.1119, Test: 1.1252\n",
      "Epoch: 745, Loss: 2.8614, Train: 1.0762, Val: 1.1237, Test: 1.1278\n",
      "Epoch: 746, Loss: 2.7863, Train: 1.1122, Val: 1.1779, Test: 1.1655\n",
      "Epoch: 747, Loss: 2.8398, Train: 1.0873, Val: 1.1348, Test: 1.1429\n",
      "Epoch: 748, Loss: 2.8434, Train: 1.0714, Val: 1.1211, Test: 1.1230\n",
      "Epoch: 749, Loss: 2.7854, Train: 1.1030, Val: 1.1795, Test: 1.1447\n",
      "Epoch: 750, Loss: 2.8064, Train: 1.1264, Val: 1.2114, Test: 1.1839\n",
      "Epoch: 751, Loss: 2.8611, Train: 1.0574, Val: 1.1190, Test: 1.1065\n",
      "Epoch: 752, Loss: 2.7673, Train: 1.0585, Val: 1.1036, Test: 1.0910\n",
      "Epoch: 753, Loss: 2.8334, Train: 1.1087, Val: 1.1620, Test: 1.1775\n",
      "Epoch: 754, Loss: 2.8009, Train: 1.1102, Val: 1.1612, Test: 1.1528\n",
      "Epoch: 755, Loss: 2.7948, Train: 1.1044, Val: 1.1558, Test: 1.1834\n",
      "Epoch: 756, Loss: 2.7723, Train: 1.0793, Val: 1.1331, Test: 1.1348\n",
      "Epoch: 757, Loss: 2.7874, Train: 1.0506, Val: 1.1202, Test: 1.1047\n",
      "Epoch: 758, Loss: 2.7703, Train: 1.0839, Val: 1.1511, Test: 1.1467\n",
      "Epoch: 759, Loss: 2.7465, Train: 1.1035, Val: 1.1734, Test: 1.1677\n",
      "Epoch: 760, Loss: 2.7651, Train: 1.0705, Val: 1.1159, Test: 1.1405\n",
      "Epoch: 761, Loss: 2.7777, Train: 1.0734, Val: 1.1330, Test: 1.1304\n",
      "Epoch: 762, Loss: 2.7550, Train: 1.1281, Val: 1.1885, Test: 1.1598\n",
      "Epoch: 763, Loss: 2.7631, Train: 1.0881, Val: 1.1541, Test: 1.1639\n",
      "Epoch: 764, Loss: 2.7322, Train: 1.0369, Val: 1.0824, Test: 1.0910\n",
      "Epoch: 765, Loss: 2.8058, Train: 1.1039, Val: 1.1539, Test: 1.1565\n",
      "Epoch: 766, Loss: 2.7795, Train: 1.1300, Val: 1.2070, Test: 1.2222\n",
      "Epoch: 767, Loss: 2.7854, Train: 1.0711, Val: 1.1233, Test: 1.1416\n",
      "Epoch: 768, Loss: 2.7952, Train: 1.0627, Val: 1.0985, Test: 1.1177\n",
      "Epoch: 769, Loss: 2.8013, Train: 1.1205, Val: 1.1834, Test: 1.1606\n",
      "Epoch: 770, Loss: 2.7818, Train: 1.1261, Val: 1.1656, Test: 1.2020\n",
      "Epoch: 771, Loss: 2.8243, Train: 1.0353, Val: 1.0923, Test: 1.0905\n",
      "Epoch: 772, Loss: 2.8322, Train: 1.0999, Val: 1.1517, Test: 1.1619\n",
      "Epoch: 773, Loss: 2.8347, Train: 1.1548, Val: 1.2096, Test: 1.1956\n",
      "Epoch: 774, Loss: 2.7981, Train: 1.0989, Val: 1.1728, Test: 1.1612\n",
      "Epoch: 775, Loss: 2.8113, Train: 1.0052, Val: 1.0870, Test: 1.0771\n",
      "Epoch: 776, Loss: 2.8521, Train: 1.0366, Val: 1.1172, Test: 1.1135\n",
      "Epoch: 777, Loss: 2.7956, Train: 1.1521, Val: 1.2290, Test: 1.1968\n",
      "Epoch: 778, Loss: 2.8371, Train: 1.0915, Val: 1.1437, Test: 1.1435\n",
      "Epoch: 779, Loss: 2.7789, Train: 1.0722, Val: 1.1187, Test: 1.1130\n",
      "Epoch: 780, Loss: 2.8307, Train: 1.1371, Val: 1.1940, Test: 1.2065\n",
      "Epoch: 781, Loss: 2.8927, Train: 1.0686, Val: 1.1433, Test: 1.1335\n",
      "Epoch: 782, Loss: 2.7481, Train: 1.0094, Val: 1.0685, Test: 1.0705\n",
      "Epoch: 783, Loss: 2.9393, Train: 1.1006, Val: 1.1655, Test: 1.1604\n",
      "Epoch: 784, Loss: 2.7870, Train: 1.1556, Val: 1.1981, Test: 1.1899\n",
      "Epoch: 785, Loss: 2.8378, Train: 1.0794, Val: 1.1339, Test: 1.1369\n",
      "Epoch: 786, Loss: 2.8157, Train: 1.0785, Val: 1.1654, Test: 1.1596\n",
      "Epoch: 787, Loss: 2.7716, Train: 1.1469, Val: 1.1891, Test: 1.1931\n",
      "Epoch: 788, Loss: 2.8590, Train: 1.0611, Val: 1.1227, Test: 1.1108\n",
      "Epoch: 789, Loss: 2.7939, Train: 1.0474, Val: 1.1094, Test: 1.1109\n",
      "Epoch: 790, Loss: 2.8209, Train: 1.0948, Val: 1.1429, Test: 1.1664\n",
      "Epoch: 791, Loss: 2.7647, Train: 1.1326, Val: 1.1878, Test: 1.1990\n",
      "Epoch: 792, Loss: 2.7698, Train: 1.0724, Val: 1.1268, Test: 1.1382\n",
      "Epoch: 793, Loss: 2.7615, Train: 1.0396, Val: 1.0929, Test: 1.1063\n",
      "Epoch: 794, Loss: 2.8080, Train: 1.0916, Val: 1.1597, Test: 1.1348\n",
      "Epoch: 795, Loss: 2.7765, Train: 1.1267, Val: 1.1724, Test: 1.1680\n",
      "Epoch: 796, Loss: 2.8117, Train: 1.0593, Val: 1.1228, Test: 1.1083\n",
      "Epoch: 797, Loss: 2.8167, Train: 1.0751, Val: 1.1509, Test: 1.1419\n",
      "Epoch: 798, Loss: 2.7771, Train: 1.1305, Val: 1.1952, Test: 1.1807\n",
      "Epoch: 799, Loss: 2.7851, Train: 1.0589, Val: 1.1298, Test: 1.1448\n",
      "Epoch: 800, Loss: 2.7872, Train: 1.0032, Val: 1.0728, Test: 1.0693\n",
      "Epoch: 801, Loss: 2.8944, Train: 1.1051, Val: 1.1746, Test: 1.1789\n",
      "Epoch: 802, Loss: 2.7692, Train: 1.1431, Val: 1.1962, Test: 1.2031\n",
      "Epoch: 803, Loss: 2.8436, Train: 1.0686, Val: 1.1270, Test: 1.1080\n",
      "Epoch: 804, Loss: 2.8525, Train: 1.0561, Val: 1.1120, Test: 1.1161\n",
      "Epoch: 805, Loss: 2.7758, Train: 1.0982, Val: 1.1807, Test: 1.1691\n",
      "Epoch: 806, Loss: 2.7789, Train: 1.0827, Val: 1.1315, Test: 1.1547\n",
      "Epoch: 807, Loss: 2.9218, Train: 1.0761, Val: 1.1157, Test: 1.1220\n",
      "Epoch: 808, Loss: 2.8045, Train: 1.0830, Val: 1.1392, Test: 1.1543\n",
      "Epoch: 809, Loss: 2.8330, Train: 1.1212, Val: 1.1743, Test: 1.1688\n",
      "Epoch: 810, Loss: 2.7666, Train: 1.0787, Val: 1.1272, Test: 1.1563\n",
      "Epoch: 811, Loss: 2.7770, Train: 1.0475, Val: 1.1208, Test: 1.1055\n",
      "Epoch: 812, Loss: 2.8237, Train: 1.0573, Val: 1.1325, Test: 1.1308\n",
      "Epoch: 813, Loss: 2.7811, Train: 1.1424, Val: 1.1730, Test: 1.1964\n",
      "Epoch: 814, Loss: 2.7772, Train: 1.1107, Val: 1.1827, Test: 1.1789\n",
      "Epoch: 815, Loss: 2.8138, Train: 1.0673, Val: 1.1161, Test: 1.1180\n",
      "Epoch: 816, Loss: 2.7907, Train: 1.0726, Val: 1.1455, Test: 1.1172\n",
      "Epoch: 817, Loss: 2.7693, Train: 1.1103, Val: 1.1595, Test: 1.1455\n",
      "Epoch: 818, Loss: 2.7368, Train: 1.0623, Val: 1.1134, Test: 1.1094\n",
      "Epoch: 819, Loss: 2.7510, Train: 1.0840, Val: 1.1444, Test: 1.1487\n",
      "Epoch: 820, Loss: 2.7826, Train: 1.1046, Val: 1.1484, Test: 1.1782\n",
      "Epoch: 821, Loss: 2.8067, Train: 1.1045, Val: 1.1616, Test: 1.1649\n",
      "Epoch: 822, Loss: 2.7555, Train: 1.0953, Val: 1.1575, Test: 1.1817\n",
      "Epoch: 823, Loss: 2.7603, Train: 1.0890, Val: 1.1297, Test: 1.1498\n",
      "Epoch: 824, Loss: 2.8023, Train: 1.0307, Val: 1.0807, Test: 1.0755\n",
      "Epoch: 825, Loss: 2.7557, Train: 1.0956, Val: 1.1388, Test: 1.1337\n",
      "Epoch: 826, Loss: 2.7542, Train: 1.1927, Val: 1.2213, Test: 1.2370\n",
      "Epoch: 827, Loss: 2.8242, Train: 1.1029, Val: 1.1445, Test: 1.1514\n",
      "Epoch: 828, Loss: 2.7315, Train: 1.0249, Val: 1.0777, Test: 1.0843\n",
      "Epoch: 829, Loss: 2.8471, Train: 1.1309, Val: 1.1598, Test: 1.1796\n",
      "Epoch: 830, Loss: 2.7838, Train: 1.1112, Val: 1.1628, Test: 1.1878\n",
      "Epoch: 831, Loss: 2.7947, Train: 1.0466, Val: 1.1165, Test: 1.1178\n",
      "Epoch: 832, Loss: 2.7805, Train: 1.0478, Val: 1.1090, Test: 1.1060\n",
      "Epoch: 833, Loss: 2.7611, Train: 1.1039, Val: 1.1606, Test: 1.1603\n",
      "Epoch: 834, Loss: 2.7483, Train: 1.0952, Val: 1.1747, Test: 1.1548\n",
      "Epoch: 835, Loss: 2.8049, Train: 1.0431, Val: 1.1110, Test: 1.0940\n",
      "Epoch: 836, Loss: 2.8557, Train: 1.1093, Val: 1.1483, Test: 1.1243\n",
      "Epoch: 837, Loss: 2.7249, Train: 1.1391, Val: 1.2099, Test: 1.1798\n",
      "Epoch: 838, Loss: 2.8054, Train: 1.0457, Val: 1.1159, Test: 1.1087\n",
      "Epoch: 839, Loss: 2.7450, Train: 1.0436, Val: 1.1102, Test: 1.1058\n",
      "Epoch: 840, Loss: 2.8047, Train: 1.1012, Val: 1.1404, Test: 1.1364\n",
      "Epoch: 841, Loss: 2.7291, Train: 1.0856, Val: 1.1363, Test: 1.1593\n",
      "Epoch: 842, Loss: 2.7233, Train: 1.0887, Val: 1.1245, Test: 1.1095\n",
      "Epoch: 843, Loss: 2.7552, Train: 1.0947, Val: 1.1488, Test: 1.1469\n",
      "Epoch: 844, Loss: 2.8089, Train: 1.0869, Val: 1.1481, Test: 1.1584\n",
      "Epoch: 845, Loss: 2.7320, Train: 1.0862, Val: 1.1179, Test: 1.1378\n",
      "Epoch: 846, Loss: 2.7373, Train: 1.0557, Val: 1.1198, Test: 1.1270\n",
      "Epoch: 847, Loss: 2.7994, Train: 1.0491, Val: 1.0999, Test: 1.1150\n",
      "Epoch: 848, Loss: 2.7348, Train: 1.0885, Val: 1.1309, Test: 1.1792\n",
      "Epoch: 849, Loss: 2.7686, Train: 1.0808, Val: 1.1473, Test: 1.1425\n",
      "Epoch: 850, Loss: 2.6877, Train: 1.0467, Val: 1.1231, Test: 1.1116\n",
      "Epoch: 851, Loss: 2.7367, Train: 1.0694, Val: 1.1306, Test: 1.1363\n",
      "Epoch: 852, Loss: 2.7419, Train: 1.1342, Val: 1.2072, Test: 1.2296\n",
      "Epoch: 853, Loss: 2.7390, Train: 1.0841, Val: 1.1486, Test: 1.1552\n",
      "Epoch: 854, Loss: 2.7069, Train: 1.0476, Val: 1.1011, Test: 1.0905\n",
      "Epoch: 855, Loss: 2.7521, Train: 1.0725, Val: 1.1146, Test: 1.1356\n",
      "Epoch: 856, Loss: 2.7233, Train: 1.0678, Val: 1.1289, Test: 1.1350\n",
      "Epoch: 857, Loss: 2.7034, Train: 1.0472, Val: 1.1082, Test: 1.1160\n",
      "Epoch: 858, Loss: 2.7487, Train: 1.0557, Val: 1.1071, Test: 1.1015\n",
      "Epoch: 859, Loss: 2.6937, Train: 1.1068, Val: 1.1784, Test: 1.1673\n",
      "Epoch: 860, Loss: 2.7256, Train: 1.1146, Val: 1.1791, Test: 1.1926\n",
      "Epoch: 861, Loss: 2.7237, Train: 1.0438, Val: 1.0986, Test: 1.1182\n",
      "Epoch: 862, Loss: 2.6935, Train: 1.0699, Val: 1.1390, Test: 1.1202\n",
      "Epoch: 863, Loss: 2.7447, Train: 1.0882, Val: 1.1175, Test: 1.1401\n",
      "Epoch: 864, Loss: 2.7556, Train: 1.0849, Val: 1.1444, Test: 1.1505\n",
      "Epoch: 865, Loss: 2.7268, Train: 1.0561, Val: 1.1081, Test: 1.1158\n",
      "Epoch: 866, Loss: 2.7237, Train: 1.0753, Val: 1.1240, Test: 1.1343\n",
      "Epoch: 867, Loss: 2.6915, Train: 1.0888, Val: 1.1616, Test: 1.1690\n",
      "Epoch: 868, Loss: 2.7205, Train: 1.0731, Val: 1.1330, Test: 1.1241\n",
      "Epoch: 869, Loss: 2.7428, Train: 1.0492, Val: 1.0978, Test: 1.0838\n",
      "Epoch: 870, Loss: 2.7287, Train: 1.1191, Val: 1.1905, Test: 1.1755\n",
      "Epoch: 871, Loss: 2.7417, Train: 1.0908, Val: 1.1615, Test: 1.1695\n",
      "Epoch: 872, Loss: 2.7050, Train: 1.0006, Val: 1.0605, Test: 1.0785\n",
      "Epoch: 873, Loss: 2.8453, Train: 1.1085, Val: 1.1530, Test: 1.1787\n",
      "Epoch: 874, Loss: 2.7916, Train: 1.1358, Val: 1.1636, Test: 1.1865\n",
      "Epoch: 875, Loss: 2.7054, Train: 1.0682, Val: 1.1138, Test: 1.1313\n",
      "Epoch: 876, Loss: 2.7648, Train: 1.0628, Val: 1.1159, Test: 1.1460\n",
      "Epoch: 877, Loss: 2.7602, Train: 1.0946, Val: 1.1501, Test: 1.1551\n",
      "Epoch: 878, Loss: 2.7496, Train: 1.0631, Val: 1.1188, Test: 1.1425\n",
      "Epoch: 879, Loss: 2.7145, Train: 1.0598, Val: 1.1060, Test: 1.1226\n",
      "Epoch: 880, Loss: 2.6984, Train: 1.0569, Val: 1.1350, Test: 1.1288\n",
      "Epoch: 881, Loss: 2.7252, Train: 1.0834, Val: 1.1618, Test: 1.1816\n",
      "Epoch: 882, Loss: 2.7042, Train: 1.0796, Val: 1.1592, Test: 1.1496\n",
      "Epoch: 883, Loss: 2.7318, Train: 1.0259, Val: 1.0880, Test: 1.0820\n",
      "Epoch: 884, Loss: 2.7910, Train: 1.0918, Val: 1.1407, Test: 1.1637\n",
      "Epoch: 885, Loss: 2.6914, Train: 1.0709, Val: 1.1516, Test: 1.1361\n",
      "Epoch: 886, Loss: 2.7294, Train: 1.0187, Val: 1.0788, Test: 1.0923\n",
      "Epoch: 887, Loss: 2.7364, Train: 1.0754, Val: 1.1312, Test: 1.1260\n",
      "Epoch: 888, Loss: 2.6861, Train: 1.1171, Val: 1.1501, Test: 1.1724\n",
      "Epoch: 889, Loss: 2.7780, Train: 1.0487, Val: 1.1085, Test: 1.1170\n",
      "Epoch: 890, Loss: 2.7056, Train: 1.0631, Val: 1.1265, Test: 1.1338\n",
      "Epoch: 891, Loss: 2.6831, Train: 1.0912, Val: 1.1411, Test: 1.1416\n",
      "Epoch: 892, Loss: 2.6481, Train: 1.0563, Val: 1.1309, Test: 1.1417\n",
      "Epoch: 893, Loss: 2.6688, Train: 1.0322, Val: 1.0844, Test: 1.0889\n",
      "Epoch: 894, Loss: 2.7128, Train: 1.0888, Val: 1.1475, Test: 1.1595\n",
      "Epoch: 895, Loss: 2.7446, Train: 1.1026, Val: 1.1732, Test: 1.1794\n",
      "Epoch: 896, Loss: 2.7214, Train: 1.0624, Val: 1.1324, Test: 1.1215\n",
      "Epoch: 897, Loss: 2.6745, Train: 1.0559, Val: 1.1251, Test: 1.1170\n",
      "Epoch: 898, Loss: 2.6701, Train: 1.1108, Val: 1.1787, Test: 1.1587\n",
      "Epoch: 899, Loss: 2.7371, Train: 1.0372, Val: 1.0880, Test: 1.0959\n",
      "Epoch: 900, Loss: 2.6495, Train: 1.0450, Val: 1.0917, Test: 1.1094\n",
      "Epoch: 901, Loss: 2.7302, Train: 1.1498, Val: 1.1715, Test: 1.1903\n",
      "Epoch: 902, Loss: 2.7256, Train: 1.0634, Val: 1.1278, Test: 1.1383\n",
      "Epoch: 903, Loss: 2.6547, Train: 1.0357, Val: 1.0998, Test: 1.0874\n",
      "Epoch: 904, Loss: 2.7391, Train: 1.1326, Val: 1.2009, Test: 1.1996\n",
      "Epoch: 905, Loss: 2.7726, Train: 1.0755, Val: 1.1310, Test: 1.1663\n",
      "Epoch: 906, Loss: 2.6358, Train: 1.0206, Val: 1.0723, Test: 1.0734\n",
      "Epoch: 907, Loss: 2.7266, Train: 1.1092, Val: 1.1634, Test: 1.1730\n",
      "Epoch: 908, Loss: 2.6873, Train: 1.1002, Val: 1.1606, Test: 1.1641\n",
      "Epoch: 909, Loss: 2.7506, Train: 1.0230, Val: 1.1002, Test: 1.0862\n",
      "Epoch: 910, Loss: 2.8034, Train: 1.0448, Val: 1.0944, Test: 1.0881\n",
      "Epoch: 911, Loss: 2.6923, Train: 1.1393, Val: 1.1938, Test: 1.2107\n",
      "Epoch: 912, Loss: 2.7260, Train: 1.1000, Val: 1.1586, Test: 1.1660\n",
      "Epoch: 913, Loss: 2.7198, Train: 1.0027, Val: 1.0669, Test: 1.0725\n",
      "Epoch: 914, Loss: 2.8160, Train: 1.0822, Val: 1.1397, Test: 1.1614\n",
      "Epoch: 915, Loss: 2.6885, Train: 1.1419, Val: 1.1861, Test: 1.2104\n",
      "Epoch: 916, Loss: 2.7226, Train: 1.0766, Val: 1.1438, Test: 1.1501\n",
      "Epoch: 917, Loss: 2.7324, Train: 1.0406, Val: 1.0957, Test: 1.1094\n",
      "Epoch: 918, Loss: 2.7282, Train: 1.0692, Val: 1.1381, Test: 1.1460\n",
      "Epoch: 919, Loss: 2.7763, Train: 1.1142, Val: 1.1416, Test: 1.1451\n",
      "Epoch: 920, Loss: 2.7191, Train: 1.0465, Val: 1.0958, Test: 1.1028\n",
      "Epoch: 921, Loss: 2.7557, Train: 1.0876, Val: 1.1484, Test: 1.1479\n",
      "Epoch: 922, Loss: 2.7103, Train: 1.0893, Val: 1.1468, Test: 1.1296\n",
      "Epoch: 923, Loss: 2.6624, Train: 1.0491, Val: 1.1251, Test: 1.1247\n",
      "Epoch: 924, Loss: 2.6903, Train: 1.0343, Val: 1.1161, Test: 1.1111\n",
      "Epoch: 925, Loss: 2.7034, Train: 1.1016, Val: 1.1389, Test: 1.1540\n",
      "Epoch: 926, Loss: 2.7256, Train: 1.0941, Val: 1.1487, Test: 1.1415\n",
      "Epoch: 927, Loss: 2.6732, Train: 1.0708, Val: 1.1246, Test: 1.1265\n",
      "Epoch: 928, Loss: 2.6896, Train: 1.0758, Val: 1.1292, Test: 1.1270\n",
      "Epoch: 929, Loss: 2.6710, Train: 1.0811, Val: 1.1618, Test: 1.1447\n",
      "Epoch: 930, Loss: 2.6729, Train: 1.0855, Val: 1.1363, Test: 1.1343\n",
      "Epoch: 931, Loss: 2.7441, Train: 1.0549, Val: 1.1142, Test: 1.1238\n",
      "Epoch: 932, Loss: 2.7227, Train: 1.0720, Val: 1.1371, Test: 1.1340\n",
      "Epoch: 933, Loss: 2.6692, Train: 1.0966, Val: 1.1675, Test: 1.1610\n",
      "Epoch: 934, Loss: 2.6819, Train: 1.0558, Val: 1.1240, Test: 1.1173\n",
      "Epoch: 935, Loss: 2.6607, Train: 1.0214, Val: 1.0941, Test: 1.0903\n",
      "Epoch: 936, Loss: 2.6884, Train: 1.0594, Val: 1.1025, Test: 1.1203\n",
      "Epoch: 937, Loss: 2.6729, Train: 1.1501, Val: 1.2112, Test: 1.2008\n",
      "Epoch: 938, Loss: 2.7183, Train: 1.0622, Val: 1.1173, Test: 1.1521\n",
      "Epoch: 939, Loss: 2.6710, Train: 1.0178, Val: 1.0616, Test: 1.0740\n",
      "Epoch: 940, Loss: 2.7420, Train: 1.1067, Val: 1.1479, Test: 1.1892\n",
      "Epoch: 941, Loss: 2.6855, Train: 1.0977, Val: 1.1585, Test: 1.1680\n",
      "Epoch: 942, Loss: 2.7041, Train: 1.0244, Val: 1.1054, Test: 1.0868\n",
      "Epoch: 943, Loss: 2.7086, Train: 1.0709, Val: 1.1446, Test: 1.1481\n",
      "Epoch: 944, Loss: 2.6804, Train: 1.1079, Val: 1.1937, Test: 1.1899\n",
      "Epoch: 945, Loss: 2.6937, Train: 1.0613, Val: 1.1248, Test: 1.1195\n",
      "Epoch: 946, Loss: 2.6704, Train: 1.0251, Val: 1.0911, Test: 1.0843\n",
      "Epoch: 947, Loss: 2.7272, Train: 1.1010, Val: 1.1524, Test: 1.1560\n",
      "Epoch: 948, Loss: 2.6918, Train: 1.0861, Val: 1.1409, Test: 1.1574\n",
      "Epoch: 949, Loss: 2.6804, Train: 1.0693, Val: 1.1230, Test: 1.1169\n",
      "Epoch: 950, Loss: 2.6981, Train: 1.0656, Val: 1.1480, Test: 1.1374\n",
      "Epoch: 951, Loss: 2.6409, Train: 1.1148, Val: 1.1318, Test: 1.1552\n",
      "Epoch: 952, Loss: 2.6735, Train: 1.0699, Val: 1.1542, Test: 1.1414\n",
      "Epoch: 953, Loss: 2.6954, Train: 1.0392, Val: 1.1010, Test: 1.1117\n",
      "Epoch: 954, Loss: 2.7106, Train: 1.0937, Val: 1.1438, Test: 1.1694\n",
      "Epoch: 955, Loss: 2.6113, Train: 1.1265, Val: 1.1646, Test: 1.1840\n",
      "Epoch: 956, Loss: 2.7262, Train: 1.0652, Val: 1.1225, Test: 1.1277\n",
      "Epoch: 957, Loss: 2.6442, Train: 1.0207, Val: 1.0983, Test: 1.0961\n",
      "Epoch: 958, Loss: 2.6690, Train: 1.0536, Val: 1.1159, Test: 1.1002\n",
      "Epoch: 959, Loss: 2.6690, Train: 1.0895, Val: 1.1465, Test: 1.1458\n",
      "Epoch: 960, Loss: 2.6555, Train: 1.0642, Val: 1.1421, Test: 1.1462\n",
      "Epoch: 961, Loss: 2.6843, Train: 1.0412, Val: 1.1158, Test: 1.1087\n",
      "Epoch: 962, Loss: 2.6883, Train: 1.0579, Val: 1.1287, Test: 1.1455\n",
      "Epoch: 963, Loss: 2.6433, Train: 1.0591, Val: 1.1218, Test: 1.1250\n",
      "Epoch: 964, Loss: 2.6662, Train: 1.0468, Val: 1.1059, Test: 1.1003\n",
      "Epoch: 965, Loss: 2.6721, Train: 1.0555, Val: 1.1309, Test: 1.1271\n",
      "Epoch: 966, Loss: 2.6140, Train: 1.0727, Val: 1.1363, Test: 1.1372\n",
      "Epoch: 967, Loss: 2.6630, Train: 1.0848, Val: 1.1341, Test: 1.1493\n",
      "Epoch: 968, Loss: 2.6888, Train: 1.0031, Val: 1.0790, Test: 1.0868\n",
      "Epoch: 969, Loss: 2.7953, Train: 1.1342, Val: 1.1763, Test: 1.1794\n",
      "Epoch: 970, Loss: 2.7332, Train: 1.1025, Val: 1.1724, Test: 1.1806\n",
      "Epoch: 971, Loss: 2.6986, Train: 1.0555, Val: 1.0906, Test: 1.1096\n",
      "Epoch: 972, Loss: 2.7363, Train: 1.0324, Val: 1.0825, Test: 1.0863\n",
      "Epoch: 973, Loss: 2.7029, Train: 1.1011, Val: 1.1664, Test: 1.1627\n",
      "Epoch: 974, Loss: 2.6910, Train: 1.1484, Val: 1.1905, Test: 1.1779\n",
      "Epoch: 975, Loss: 2.7454, Train: 1.0185, Val: 1.0884, Test: 1.0795\n",
      "Epoch: 976, Loss: 2.6846, Train: 1.0182, Val: 1.0685, Test: 1.0843\n",
      "Epoch: 977, Loss: 2.7276, Train: 1.0947, Val: 1.1531, Test: 1.1614\n",
      "Epoch: 978, Loss: 2.7030, Train: 1.0844, Val: 1.1542, Test: 1.1722\n",
      "Epoch: 979, Loss: 2.6399, Train: 1.0625, Val: 1.1466, Test: 1.1476\n",
      "Epoch: 980, Loss: 2.6320, Train: 1.0515, Val: 1.0912, Test: 1.1186\n",
      "Epoch: 981, Loss: 2.6473, Train: 1.0520, Val: 1.1086, Test: 1.1148\n",
      "Epoch: 982, Loss: 2.6323, Train: 1.0591, Val: 1.1245, Test: 1.1335\n",
      "Epoch: 983, Loss: 2.6049, Train: 1.0571, Val: 1.0997, Test: 1.1187\n",
      "Epoch: 984, Loss: 2.6375, Train: 1.0778, Val: 1.1313, Test: 1.1319\n",
      "Epoch: 985, Loss: 2.6374, Train: 1.1137, Val: 1.1611, Test: 1.1936\n",
      "Epoch: 986, Loss: 2.6756, Train: 1.0691, Val: 1.1441, Test: 1.1269\n",
      "Epoch: 987, Loss: 2.6945, Train: 1.0328, Val: 1.1010, Test: 1.1128\n",
      "Epoch: 988, Loss: 2.6639, Train: 1.0766, Val: 1.1204, Test: 1.1498\n",
      "Epoch: 989, Loss: 2.6649, Train: 1.0708, Val: 1.1424, Test: 1.1461\n",
      "Epoch: 990, Loss: 2.6793, Train: 1.0465, Val: 1.0961, Test: 1.0881\n",
      "Epoch: 991, Loss: 2.6448, Train: 1.0448, Val: 1.1064, Test: 1.1369\n",
      "Epoch: 992, Loss: 2.6303, Train: 1.1116, Val: 1.1605, Test: 1.1881\n",
      "Epoch: 993, Loss: 2.7015, Train: 1.0553, Val: 1.1072, Test: 1.1210\n",
      "Epoch: 994, Loss: 2.6742, Train: 1.0713, Val: 1.1165, Test: 1.1470\n",
      "Epoch: 995, Loss: 2.6543, Train: 1.0536, Val: 1.1270, Test: 1.1373\n",
      "Epoch: 996, Loss: 2.6314, Train: 1.0576, Val: 1.1152, Test: 1.1091\n",
      "Epoch: 997, Loss: 2.6710, Train: 1.0499, Val: 1.1229, Test: 1.1032\n",
      "Epoch: 998, Loss: 2.6208, Train: 1.0496, Val: 1.1131, Test: 1.1015\n",
      "Epoch: 999, Loss: 2.6120, Train: 1.0618, Val: 1.1328, Test: 1.1160\n",
      "Epoch: 1000, Loss: 2.6454, Train: 1.0787, Val: 1.1210, Test: 1.1215\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    data,\n",
    "    layer_name=\"SAGE\",\n",
    "    encoder_num_layers=5,\n",
    "    encoder_dropout=0.1,\n",
    "    encoder_skip_connections=True,\n",
    "    decoder_num_layers=4,\n",
    "    hidden_channels=32\n",
    ").to(device)\n",
    "losses = train_test(\n",
    "    model=model,\n",
    "    epochs=1000,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    logging_step=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss({ \"loss\": losses }, ylim=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = make_recommendations(dataset, data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 610/610 [00:13<00:00, 44.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users that got suggestions of movies they have already rated: 3 %\n",
      "Average number of suggested movies per user: 38\n",
      "Average number of rated movies per user: 165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_reviews_suggestions_intersection(dataset, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I observe that when more epochs are being run (better training):\n",
    "* percentage of suggested movies that have already been reviewed is being reduced\n",
    "* average number of suggested movies per user is being reduced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Flirting With Disaster (1996)', 5.0), ('Crumb (1994)', 5.0), ('Living in Oblivion (1995)', 5.0), ('Before Sunrise (1995)', 5.0), ('Eat Drink Man Woman (Yin shi nan nu) (1994)', 5.0), ('Star Wars: Episode IV - A New Hope (1977)', 5.0), ('Like Water for Chocolate (Como agua para chocolate) (1992)', 5.0), ('Shallow Grave (1994)', 5.0), (\"Muriel's Wedding (1994)\", 5.0), ('Fugitive, The (1993)', 5.0), ('In the Name of the Father (1993)', 5.0), ('Manhattan Murder Mystery (1993)', 5.0), ('Six Degrees of Separation (1993)', 5.0), ('Silence of the Lambs, The (1991)', 5.0), ('Fargo (1996)', 5.0), ('Philadelphia Story, The (1940)', 5.0), ('North by Northwest (1959)', 5.0), ('Some Like It Hot (1959)', 5.0), ('Casablanca (1942)', 5.0), ('My Fair Lady (1964)', 5.0), ('Wizard of Oz, The (1939)', 5.0), ('Gone with the Wind (1939)', 5.0), ('Notorious (1946)', 5.0), ('Beautiful Thing (1996)', 5.0), ('Sleeper (1973)', 5.0), (\"Monty Python's Life of Brian (1979)\", 5.0), ('Bonnie and Clyde (1967)', 5.0), ('Dial M for Murder (1954)', 5.0), ('Rebel Without a Cause (1955)', 5.0), ('Monty Python and the Holy Grail (1975)', 5.0), ('Strictly Ballroom (1992)', 5.0), ('Star Wars: Episode V - The Empire Strikes Back (1980)', 5.0), ('Princess Bride, The (1987)', 5.0), ('12 Angry Men (1957)', 5.0), ('Bridge on the River Kwai, The (1957)', 5.0), ('Fantasia (1940)', 5.0), ('High Noon (1952)', 5.0), ('Waiting for Guffman (1996)', 5.0), ('Afterglow (1997)', 5.0), ('Spanish Prisoner, The (1997)', 5.0), ('West Side Story (1961)', 5.0), ('Labyrinth (1986)', 5.0), ('Jungle Book, The (1967)', 5.0), ('Gods Must Be Crazy, The (1980)', 5.0), ('Beetlejuice (1988)', 5.0), ('Strangers on a Train (1951)', 5.0), ('Saboteur (1942)', 5.0), ('Little Voice (1998)', 5.0), (\"Cookie's Fortune (1999)\", 5.0), ('Election (1999)', 5.0), ('Run Lola Run (Lola rennt) (1998)', 5.0), ('Airplane! (1980)', 5.0), ('Black Cat, White Cat (Crna macka, beli macor) (1998)', 5.0), ('American Beauty (1999)', 5.0), ('Hairspray (1988)', 5.0), ('Dead Again (1991)', 5.0), ('All About My Mother (Todo sobre mi madre) (1999)', 5.0), ('Searchers, The (1956)', 5.0), ('Outlaw Josey Wales, The (1976)', 5.0), (\"I'm the One That I Want (2000)\", 5.0), ('Traffic (2000)', 5.0), ('Blow (2001)', 5.0), ('L.I.E. (2001)', 5.0), (\"No Man's Land (2001)\", 5.0), ('Nobody Loves Me (Keiner liebt mich) (1994)', 4.0), ('Adventures of Priscilla, Queen of the Desert, The (1994)', 4.0), ('Bullets Over Broadway (1994)', 4.0), ('Maverick (1994)', 4.0), ('Aladdin (1992)', 4.0), ('Truth About Cats & Dogs, The (1996)', 4.0), ('Lone Star (1996)', 4.0), ('Twelfth Night (1996)', 4.0), (\"Singin' in the Rain (1952)\", 4.0), (\"Breakfast at Tiffany's (1961)\", 4.0), ('Rear Window (1954)', 4.0), ('Sword in the Stone, The (1963)', 4.0), ('Willy Wonka & the Chocolate Factory (1971)', 4.0), ('Grifters, The (1990)', 4.0), ('Goodfellas (1990)', 4.0), ('Psycho (1960)', 4.0), ('Amadeus (1984)', 4.0), ('Stand by Me (1986)', 4.0), ('Groundhog Day (1993)', 4.0), ('Unforgiven (1992)', 4.0), ('This Is Spinal Tap (1984)', 4.0), ('Indiana Jones and the Last Crusade (1989)', 4.0), ('Donnie Brasco (1997)', 4.0), ('Grosse Pointe Blank (1997)', 4.0), ('Austin Powers: International Man of Mystery (1997)', 4.0), ('Full Monty, The (1997)', 4.0), ('Sweet Hereafter, The (1997)', 4.0), ('Big Lebowski, The (1998)', 4.0), ('Bulworth (1998)', 4.0), (\"Can't Hardly Wait (1998)\", 4.0), ('Smoke Signals (1998)', 4.0), ('Breakfast Club, The (1985)', 4.0), ('Blue Velvet (1986)', 4.0), ('Jerk, The (1979)', 4.0), ('Shadow of a Doubt (1943)', 4.0), ('Nights of Cabiria (Notti di Cabiria, Le) (1957)', 4.0), ('Waking Ned Devine (a.k.a. Waking Ned) (1998)', 4.0), ('Romancing the Stone (1984)', 4.0), ('Name of the Rose, The (Name der Rose, Der) (1986)', 4.0), ('Austin Powers: The Spy Who Shagged Me (1999)', 4.0), ('Sixth Sense, The (1999)', 4.0), ('Pajama Game, The (1957)', 4.0), ('High Plains Drifter (1973)', 4.0), ('Being John Malkovich (1999)', 4.0), ('Spaceballs (1987)', 4.0), ('Commitments, The (1991)', 4.0), ('Magnolia (1999)', 4.0), ('Talented Mr. Ripley, The (1999)', 4.0), ('Boys from Brazil, The (1978)', 4.0), ('League of Their Own, A (1992)', 4.0), ('Wonder Boys (2000)', 4.0), ('JFK (1991)', 4.0), ('Erin Brockovich (2000)', 4.0), ('Almost Famous (2000)', 4.0), ('Billy Elliot (2000)', 4.0), ('Crouching Tiger, Hidden Dragon (Wo hu cang long) (2000)', 4.0), ('Before Night Falls (2000)', 4.0), ('Thirteen Days (2000)', 4.0), ('Innerspace (1987)', 4.0), (\"Bridget Jones's Diary (2001)\", 4.0), ('Under the Sand (2000)', 4.0), ('Divided We Fall (Musme si pomhat) (2000)', 4.0), (\"Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\", 4.0), (\"Devil's Backbone, The (Espinazo del diablo, El) (2001)\", 4.0), ('Get Shorty (1995)', 3.0), ('To Die For (1995)', 3.0), ('Mighty Aphrodite (1995)', 3.0), ('Postman, The (Postino, Il) (1994)', 3.0), ('Jeffrey (1995)', 3.0), ('Heavenly Creatures (1994)', 3.0), ('Corrina, Corrina (1994)', 3.0), ('Four Weddings and a Funeral (1994)', 3.0), ('Beauty and the Beast (1991)', 3.0), ('Mission: Impossible (1996)', 3.0), ('Maya Lin: A Strong Clear Vision (1994)', 3.0), ('Love in the Afternoon (1957)', 3.0), ('Everyone Says I Love You (1996)', 3.0), ('Crying Game, The (1992)', 3.0), ('Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)', 3.0), ('Butch Cassidy and the Sundance Kid (1969)', 3.0), ('Men in Black (a.k.a. MIB) (1997)', 3.0), ('My Life in Pink (Ma vie en rose) (1997)', 3.0), ('Opposite of Sex, The (1998)', 3.0), ('Mulan (1998)', 3.0), (\"There's Something About Mary (1998)\", 3.0), ('Pretty in Pink (1986)', 3.0), ('Rushmore (1998)', 3.0), ('Mansfield Park (1999)', 3.0), ('Defending Your Life (1991)', 3.0), ('Blow-Up (Blowup) (1966)', 3.0), ('What About Bob? (1991)', 3.0), ('Best in Show (2000)', 3.0), ('Planes, Trains & Automobiles (1987)', 3.0), ('Gift, The (2000)', 3.0), ('O Brother, Where Art Thou? (2000)', 3.0), ('State and Main (2000)', 3.0), ('Legend of Rita, The (Stille nach dem Schu, Die) (1999)', 3.0), ('In the Mood For Love (Fa yeung nin wa) (2000)', 3.0), ('Series 7: The Contenders (2001)', 3.0), ('Circle, The (Dayereh) (2000)', 3.0), ('Closet, The (Placard, Le) (2001)', 3.0), ('Together (Tillsammans) (2000)', 3.0), (\"Man Who Wasn't There, The (2001)\", 3.0), ('Twelve Monkeys (a.k.a. 12 Monkeys) (1995)', 2.0), ('Seven (a.k.a. Se7en) (1995)', 2.0), ('Safe (1995)', 2.0), ('Ed Wood (1994)', 2.0), ('Barcelona (1994)', 2.0), ('With Honors (1994)', 2.0), ('Tombstone (1993)', 2.0), ('Wild Bunch, The (1969)', 2.0), ('Swingers (1996)', 2.0), ('Fish Called Wanda, A (1988)', 2.0), ('Brazil (1985)', 2.0), ('Night on Earth (1991)', 2.0), ('L.A. Confidential (1997)', 2.0), ('Character (Karakter) (1997)', 2.0), ('Perfect Murder, A (1998)', 2.0), ('Metropolitan (1990)', 2.0), ('Seven Samurai (Shichinin no samurai) (1954)', 2.0), ('Elizabeth (1998)', 2.0), ('Eyes Wide Shut (1999)', 2.0), ('Bowfinger (1999)', 2.0), ('Fight Club (1999)', 2.0), ('Crimes and Misdemeanors (1989)', 2.0), ('High Fidelity (2000)', 2.0), ('East is East (1999)', 2.0), ('Memento (2000)', 2.0), ('Visit, The (2000)', 2.0), ('NeverEnding Story III, The (1994)', 1.0), ('Circle of Friends (1995)', 1.0), ('Pulp Fiction (1994)', 1.0), ('Dazed and Confused (1993)', 1.0), ('Piano, The (1993)', 1.0), ('Sleepless in Seattle (1993)', 1.0), ('English Patient, The (1996)', 1.0), ('Wings of Desire (Himmel ber Berlin, Der) (1987)', 1.0), ('Mars Attacks! (1996)', 1.0), ('Conspiracy Theory (1997)', 1.0), ('Good Will Hunting (1997)', 1.0), (\"Buffalo '66 (a.k.a. Buffalo 66) (1998)\", 1.0), ('Pecker (1998)', 1.0), ('Life Is Beautiful (La Vita  bella) (1997)', 1.0), ('Matrix, The (1999)', 1.0), ('Star Wars: Episode I - The Phantom Menace (1999)', 1.0), ('Thomas Crown Affair, The (1999)', 1.0), ('Galaxy Quest (1999)', 1.0), ('Hook (1991)', 1.0), ('Mr. Mom (1983)', 1.0), ('Chocolat (2000)', 1.0), ('Moulin Rouge (2001)', 1.0), ('Ghost World (2001)', 1.0)]\n",
      "-----------------------\n",
      "['Last Waltz, The (1978)', 'I, the Jury (1982)', 'Animals are Beautiful People (1974)', 'Thoroughly Modern Millie (1967)', 'Jane Eyre (1944)', 'Thousand Clowns, A (1965)', 'Dylan Moran: Monster (2004)', 'Steve Jobs: The Man in the Machine (2015)']\n"
     ]
    }
   ],
   "source": [
    "analyze_user(dataset, results, user_id=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('environ': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d45a311e7c87733790111e2f49f1325c4b053c477981450c90dc62b1d14d0068"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
