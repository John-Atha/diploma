{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing mulitple simple GNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "\n",
    "# for py2neo and utils\n",
    "from py2neo import Graph, Relationship\n",
    "parent_path = pathlib.Path(os.getcwd()).parent.absolute()\n",
    "sys.path.append(str(parent_path))\n",
    "\n",
    "# for PyG\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n",
    "from torch_geometric.nn import to_hetero\n",
    "import torch.nn.functional as F\n",
    "from utils_draft.pyg import load_node, load_edge, SequenceEncoder, IdentityEncoder, ListEncoder\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import SAGEConv, GATv2Conv, GCNConv, TransformerConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to the existing neo4j instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\n",
    "    \"bolt://localhost:7687\",\n",
    "    auth=(\"neo4j\", \"admin\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observing the pre-stored graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Type    |   Count |\n",
      "|---------|---------|\n",
      "| Movies  |    9460 |\n",
      "| Users   |     610 |\n",
      "| Ratings |   96150 |\n"
     ]
    }
   ],
   "source": [
    "movies_num = graph.nodes.match(\"Movie\").count()\n",
    "users_num = graph.nodes.match(\"User\").count()\n",
    "RATES = Relationship.type(\"RATES\")\n",
    "ratings_num = graph.relationships.match(r_type=RATES).count()\n",
    "table = [\n",
    "    [\"Movies\", movies_num],\n",
    "    [\"Users\", users_num],\n",
    "    [\"Ratings\", ratings_num]\n",
    "]\n",
    "print(tabulate(table, headers=[\"Type\", \"Count\"], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the PyG graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items: dict_items([('title', <utils.pyg.SequenceEncoder object at 0x00000226309C5240>), ('genres', <utils.pyg.ListEncoder object at 0x000002262FA2B580>), ('year', <utils.pyg.SequenceEncoder object at 0x00000226309D0CA0>)])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c9d81d51ef42d2ae580ff419077beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5abd806b8544467ae4de0c454e99e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items: dict_items([('username', <utils.pyg.SequenceEncoder object at 0x00000226309C5240>)])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c494d3a8436241119c66fc1bc8299de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "movies_x, movies_mapping = load_node(\n",
    "    graph=graph,\n",
    "    query=\"\"\"\n",
    "        MATCH (m: Movie)\n",
    "        return m.movieId as movieId, m.title as title, m.genres as genres, m.year as year\n",
    "    \"\"\",\n",
    "    index_col=\"movieId\",\n",
    "    encoders={\n",
    "        \"title\": SequenceEncoder(),\n",
    "        \"genres\": ListEncoder(sep=\"|\"),\n",
    "        \"year\": SequenceEncoder(),\n",
    "    }\n",
    ")\n",
    "\n",
    "users_x, users_mapping = load_node(\n",
    "    graph=graph,\n",
    "    query=\"\"\"\n",
    "        MATCH (u:User)-[r:RATES]-(m:Movie)\n",
    "        return u.userId as userId, u.username as username, avg(r.rating) as avg_rating, count(r) as ratings;\n",
    "    \"\"\",\n",
    "    index_col=\"userId\",\n",
    "    encoders={\n",
    "        # \"avg_rating\": IdentityEncoder(dtype=torch.float16),\n",
    "        # \"ratings\": IdentityEncoder(dtype=torch.int64),\n",
    "        \"username\": SequenceEncoder(),\n",
    "    }\n",
    ")\n",
    "\n",
    "edge_index, edge_label = load_edge(\n",
    "    graph=graph,\n",
    "    query=\"\"\"\n",
    "        MATCH (u:User)-[r:RATES]-(m:Movie)\n",
    "        return u.userId as userId, r.rating as rating, r.datetime as datetime, m.movieId as movieId;\n",
    "    \"\"\",\n",
    "    src_index_col=\"userId\",\n",
    "    src_mapping=users_mapping,\n",
    "    dst_index_col=\"movieId\",\n",
    "    dst_mapping=movies_mapping,\n",
    "    encoders={\n",
    "        \"rating\": IdentityEncoder(dtype=torch.long),\n",
    "        # \"datetime\": IdentityEncoder(dtype=torch.long),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "data[\"user\"].x = users_x\n",
    "data[\"movie\"].x = movies_x\n",
    "data[\"user\", \"reviews\", \"movie\"].edge_index = edge_index\n",
    "data[\"user\", \"reviews\", \"movie\"].edge_label = edge_label\n",
    "data.to(device, non_blocking=True)\n",
    "data = ToUndirected()(data)\n",
    "del data[\"movie\", \"rev_reviews\", \"user\"].edge_label\n",
    "\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[(\"user\", \"reviews\", \"movie\")],\n",
    "    rev_edge_types=[(\"movie\", \"rev_reviews\", \"user\")],\n",
    ")\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = {\n",
    "    \"SAGE\": SAGEConv,\n",
    "    \"GAT\": GATv2Conv,\n",
    "    \"GCN\": GCNConv,\n",
    "    \"Transformer\": TransformerConv,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder1(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, layer_name=\"SAGE\"):\n",
    "        super().__init__()\n",
    "        layer = layers.get(layer_name) or SAGEConv\n",
    "        self.conv1 = layer(in_channels, hidden_channels)\n",
    "        self.conv2 = layer(hidden_channels, hidden_channels)\n",
    "        self.conv3 = layer(hidden_channels, hidden_channels)\n",
    "        self.conv4 = layer(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GNNEncoder2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, layer_name=\"SAGE\", num_layers=4):\n",
    "        super().__init__()\n",
    "        layer = layers.get(layer_name) or SAGEConv\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(layer(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers-2):\n",
    "            self.convs.append(layer(hidden_channels, hidden_channels))\n",
    "        self.convs.append(layer(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(len(self.convs)-1):\n",
    "            x = self.convs[i](x, edge_index).relu()\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(-1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin4 = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['user'][row], z_dict['movie'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z).relu()\n",
    "        z = self.lin3(z).relu()\n",
    "        z = self.lin4(z)\n",
    "        return z.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_channels=(-1, -1), hidden_channels=32, out_channels=32, layer_name=\"SAGE\"):\n",
    "        super().__init__()\n",
    "        if layer_name == \"GCN\":\n",
    "            self.encoder = GCNEncoder(\n",
    "                hidden_channels=hidden_channels,\n",
    "                out_channels=out_channels\n",
    "            )\n",
    "        else:\n",
    "            self.encoder = GNNEncoder1(\n",
    "                in_channels=in_channels,\n",
    "                hidden_channels=hidden_channels,\n",
    "                out_channels=out_channels,\n",
    "                layer_name=layer_name,    \n",
    "            )\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.bincount(train_data['user', 'movie'].edge_label)\n",
    "weight = weight.max() / weight\n",
    "\n",
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    weight = 1. if weight is None else weight[target].to(pred.dtype)\n",
    "    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilderTrainerTester():\n",
    "    def __init__(self, train_data, val_data, test_data, in_channels=(-1, -1), hidden_channels=32, out_channels=32, layer_name=\"SAGE\"):\n",
    "        model = Model(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            layer_name=layer_name\n",
    "        ).to(device)\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.layer_name = layer_name\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        weight = torch.bincount(train_data['user', 'movie'].edge_label)\n",
    "        self.weight = weight.max() / weight\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.model(\n",
    "            self.train_data.collect('x'),\n",
    "            self.train_data.edge_index_dict,\n",
    "            self.train_data['user', 'movie'].edge_label_index\n",
    "        )\n",
    "        target = self.train_data['user', 'movie'].edge_label\n",
    "        loss = weighted_mse_loss(pred, target, weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return float(loss)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, data):\n",
    "        self.model.eval()\n",
    "        pred = self.model(\n",
    "            data.collect('x'),\n",
    "            data.edge_index_dict,\n",
    "            data['user', 'movie'].edge_label_index,\n",
    "        )\n",
    "        pred = pred.clamp(min=0, max=5)\n",
    "        target = data['user', 'movie'].edge_label.float()\n",
    "        rmse = F.mse_loss(pred, target).sqrt()\n",
    "        # print(\"Predictions:\", pred)\n",
    "        # print(\"Target:\", target)\n",
    "        return float(rmse)\n",
    "       \n",
    "    def train_test(self, epochs):\n",
    "        # Due to lazy initialization, we need to run one model step so the number\n",
    "        # of parameters can be inferred:\n",
    "        with torch.no_grad():\n",
    "            self.model.encoder(self.train_data.collect('x'), self.train_data.edge_index_dict)\n",
    "        \n",
    "        for epoch in range(1, epochs):\n",
    "            loss = self.train()\n",
    "            train_rmse = self.test(self.train_data)\n",
    "            val_rmse = self.test(self.val_data)\n",
    "            test_rmse = self.test(self.test_data)\n",
    "            if not epoch%10:\n",
    "                print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "                    f'Val: {val_rmse:.4f}, Test: {test_rmse:.4f}')\n",
    "        print(\"hidden_channels:\", self.hidden_channels)\n",
    "        print(\"layers:\", self.layer_name)\n",
    "        print(\"Final epoch:\")\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "            f'Val: {val_rmse:.4f}, Test: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "        \"layer_name\": \"SAGE\",\n",
    "        \"hidden_channels\": 64,\n",
    "    },\n",
    "    {\n",
    "        \"layer_name\": \"GAT\",\n",
    "        \"hidden_channels\": 64,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGE | 64 hidden_channels\n",
      "Epoch: 010, Loss: 6.7702, Train: 1.0875, Val: 1.1037, Test: 1.0978\n",
      "Epoch: 020, Loss: 6.9893, Train: 1.1583, Val: 1.1814, Test: 1.1729\n",
      "Epoch: 030, Loss: 6.0863, Train: 1.3461, Val: 1.3711, Test: 1.3619\n",
      "Epoch: 040, Loss: 5.8485, Train: 1.4209, Val: 1.4449, Test: 1.4367\n",
      "Epoch: 050, Loss: 5.7502, Train: 1.4013, Val: 1.4246, Test: 1.4184\n",
      "Epoch: 060, Loss: 5.5393, Train: 1.2708, Val: 1.2925, Test: 1.2896\n",
      "Epoch: 070, Loss: 5.2944, Train: 1.3040, Val: 1.3250, Test: 1.3252\n",
      "Epoch: 080, Loss: 5.0280, Train: 1.2662, Val: 1.2830, Test: 1.2886\n",
      "Epoch: 090, Loss: 4.6796, Train: 1.2882, Val: 1.3028, Test: 1.3148\n",
      "Epoch: 100, Loss: 4.4098, Train: 1.1301, Val: 1.1498, Test: 1.1591\n",
      "Epoch: 110, Loss: 4.0095, Train: 1.2827, Val: 1.3205, Test: 1.3266\n",
      "Epoch: 120, Loss: 3.8531, Train: 1.2115, Val: 1.2597, Test: 1.2648\n",
      "Epoch: 130, Loss: 3.6745, Train: 1.2019, Val: 1.2621, Test: 1.2672\n",
      "Epoch: 140, Loss: 3.4091, Train: 1.1702, Val: 1.2434, Test: 1.2432\n",
      "Epoch: 150, Loss: 4.4678, Train: 1.3017, Val: 1.3679, Test: 1.3643\n",
      "Epoch: 160, Loss: 3.3518, Train: 1.0808, Val: 1.1649, Test: 1.1657\n",
      "Epoch: 170, Loss: 3.2434, Train: 1.1282, Val: 1.2321, Test: 1.2317\n",
      "Epoch: 180, Loss: 3.0351, Train: 1.0890, Val: 1.1863, Test: 1.1848\n",
      "Epoch: 190, Loss: 2.8816, Train: 1.0652, Val: 1.1804, Test: 1.1749\n",
      "Epoch: 200, Loss: 2.7685, Train: 1.0509, Val: 1.1730, Test: 1.1645\n",
      "Epoch: 210, Loss: 2.7020, Train: 1.0992, Val: 1.2257, Test: 1.2212\n",
      "Epoch: 220, Loss: 2.8395, Train: 0.9940, Val: 1.1071, Test: 1.1076\n",
      "Epoch: 230, Loss: 2.5901, Train: 1.0672, Val: 1.1890, Test: 1.1862\n",
      "Epoch: 240, Loss: 2.5262, Train: 1.0039, Val: 1.1259, Test: 1.1284\n",
      "Epoch: 250, Loss: 2.4635, Train: 1.0431, Val: 1.1780, Test: 1.1799\n",
      "Epoch: 260, Loss: 2.5886, Train: 0.9829, Val: 1.1324, Test: 1.1373\n",
      "Epoch: 270, Loss: 2.5651, Train: 1.0321, Val: 1.1550, Test: 1.1556\n",
      "Epoch: 280, Loss: 2.4789, Train: 1.0351, Val: 1.1684, Test: 1.1719\n",
      "Epoch: 290, Loss: 2.3755, Train: 1.0061, Val: 1.1457, Test: 1.1482\n",
      "Epoch: 300, Loss: 2.3114, Train: 0.9994, Val: 1.1406, Test: 1.1446\n",
      "Epoch: 310, Loss: 2.4266, Train: 0.9643, Val: 1.1138, Test: 1.1165\n",
      "Epoch: 320, Loss: 2.2958, Train: 0.9808, Val: 1.1359, Test: 1.1439\n",
      "Epoch: 330, Loss: 2.5857, Train: 1.0261, Val: 1.1860, Test: 1.1930\n",
      "Epoch: 340, Loss: 3.7731, Train: 1.0276, Val: 1.1144, Test: 1.1107\n",
      "Epoch: 350, Loss: 2.6995, Train: 1.0316, Val: 1.1187, Test: 1.1150\n",
      "Epoch: 360, Loss: 2.5033, Train: 1.0158, Val: 1.1186, Test: 1.1211\n",
      "Epoch: 370, Loss: 2.3869, Train: 1.0094, Val: 1.1216, Test: 1.1215\n",
      "Epoch: 380, Loss: 2.3170, Train: 0.9922, Val: 1.1082, Test: 1.1148\n",
      "Epoch: 390, Loss: 2.2722, Train: 1.0012, Val: 1.1254, Test: 1.1307\n",
      "Epoch: 400, Loss: 2.2248, Train: 0.9888, Val: 1.1150, Test: 1.1208\n",
      "Epoch: 410, Loss: 2.8603, Train: 1.0369, Val: 1.1469, Test: 1.1547\n",
      "Epoch: 420, Loss: 2.4330, Train: 0.9990, Val: 1.1278, Test: 1.1372\n",
      "Epoch: 430, Loss: 2.2594, Train: 0.9881, Val: 1.1174, Test: 1.1252\n",
      "Epoch: 440, Loss: 2.2052, Train: 0.9842, Val: 1.1181, Test: 1.1284\n",
      "Epoch: 450, Loss: 2.1589, Train: 0.9801, Val: 1.1152, Test: 1.1276\n",
      "Epoch: 460, Loss: 2.2496, Train: 0.9848, Val: 1.1263, Test: 1.1400\n",
      "Epoch: 470, Loss: 2.1947, Train: 0.9663, Val: 1.1076, Test: 1.1157\n",
      "Epoch: 480, Loss: 2.1117, Train: 0.9617, Val: 1.1045, Test: 1.1114\n",
      "Epoch: 490, Loss: 2.0727, Train: 0.9588, Val: 1.1051, Test: 1.1113\n",
      "Epoch: 500, Loss: 2.1376, Train: 0.9886, Val: 1.1294, Test: 1.1391\n",
      "Epoch: 510, Loss: 2.0831, Train: 0.9516, Val: 1.1000, Test: 1.1094\n",
      "Epoch: 520, Loss: 2.0194, Train: 0.9404, Val: 1.0992, Test: 1.1034\n",
      "Epoch: 530, Loss: 2.0521, Train: 0.9501, Val: 1.1049, Test: 1.1127\n",
      "Epoch: 540, Loss: 2.0494, Train: 0.9511, Val: 1.1057, Test: 1.1098\n",
      "Epoch: 550, Loss: 2.0521, Train: 0.9417, Val: 1.0972, Test: 1.1045\n",
      "Epoch: 560, Loss: 2.0163, Train: 0.9343, Val: 1.0952, Test: 1.1004\n",
      "Epoch: 570, Loss: 1.9813, Train: 0.9363, Val: 1.0924, Test: 1.1008\n",
      "Epoch: 580, Loss: 1.9762, Train: 0.9488, Val: 1.1106, Test: 1.1196\n",
      "Epoch: 590, Loss: 1.9870, Train: 0.9291, Val: 1.0934, Test: 1.0995\n",
      "Epoch: 600, Loss: 1.9635, Train: 0.9550, Val: 1.1147, Test: 1.1211\n",
      "Epoch: 610, Loss: 2.0293, Train: 0.9947, Val: 1.1373, Test: 1.1506\n",
      "Epoch: 620, Loss: 1.9940, Train: 0.9397, Val: 1.0993, Test: 1.1148\n",
      "Epoch: 630, Loss: 1.9253, Train: 0.9251, Val: 1.0853, Test: 1.0979\n",
      "Epoch: 640, Loss: 1.9445, Train: 0.9308, Val: 1.0969, Test: 1.1023\n",
      "Epoch: 650, Loss: 1.8975, Train: 0.9320, Val: 1.0922, Test: 1.1042\n",
      "Epoch: 660, Loss: 1.9608, Train: 0.9382, Val: 1.0942, Test: 1.1075\n",
      "Epoch: 670, Loss: 1.9172, Train: 0.9349, Val: 1.0947, Test: 1.1081\n",
      "Epoch: 680, Loss: 1.9236, Train: 0.9367, Val: 1.0977, Test: 1.1106\n",
      "Epoch: 690, Loss: 1.9284, Train: 0.9279, Val: 1.0913, Test: 1.1058\n",
      "Epoch: 700, Loss: 1.8972, Train: 0.9127, Val: 1.0868, Test: 1.0997\n",
      "Epoch: 710, Loss: 1.9058, Train: 0.9485, Val: 1.1196, Test: 1.1314\n",
      "Epoch: 720, Loss: 1.8518, Train: 0.9276, Val: 1.0957, Test: 1.1080\n",
      "Epoch: 730, Loss: 1.8523, Train: 0.9261, Val: 1.0999, Test: 1.1108\n",
      "Epoch: 740, Loss: 1.9037, Train: 0.9284, Val: 1.1014, Test: 1.1176\n",
      "Epoch: 750, Loss: 1.9325, Train: 0.9110, Val: 1.0894, Test: 1.1030\n",
      "Epoch: 760, Loss: 1.8685, Train: 0.8990, Val: 1.0807, Test: 1.0934\n",
      "Epoch: 770, Loss: 1.9237, Train: 0.9140, Val: 1.0912, Test: 1.1032\n",
      "Epoch: 780, Loss: 1.9656, Train: 0.9538, Val: 1.1248, Test: 1.1318\n",
      "Epoch: 790, Loss: 1.8470, Train: 0.9079, Val: 1.0875, Test: 1.0947\n",
      "Epoch: 800, Loss: 1.7964, Train: 0.8998, Val: 1.0825, Test: 1.0937\n",
      "Epoch: 810, Loss: 1.8349, Train: 0.9145, Val: 1.1049, Test: 1.1091\n",
      "Epoch: 820, Loss: 1.8305, Train: 0.9320, Val: 1.1003, Test: 1.1133\n",
      "Epoch: 830, Loss: 1.8759, Train: 0.9327, Val: 1.1081, Test: 1.1214\n",
      "Epoch: 840, Loss: 1.8212, Train: 0.9176, Val: 1.1028, Test: 1.1098\n",
      "Epoch: 850, Loss: 1.8036, Train: 0.8957, Val: 1.0826, Test: 1.0921\n",
      "Epoch: 860, Loss: 1.7456, Train: 0.9042, Val: 1.0943, Test: 1.1027\n",
      "Epoch: 870, Loss: 1.7876, Train: 0.8965, Val: 1.0879, Test: 1.0956\n",
      "Epoch: 880, Loss: 2.7003, Train: 1.1043, Val: 1.2278, Test: 1.2396\n",
      "Epoch: 890, Loss: 2.2361, Train: 0.9980, Val: 1.1353, Test: 1.1403\n",
      "Epoch: 900, Loss: 2.0313, Train: 0.9682, Val: 1.1190, Test: 1.1224\n",
      "Epoch: 910, Loss: 1.9112, Train: 0.9297, Val: 1.0995, Test: 1.0965\n",
      "Epoch: 920, Loss: 1.8488, Train: 0.9083, Val: 1.0800, Test: 1.0824\n",
      "Epoch: 930, Loss: 1.9413, Train: 0.9325, Val: 1.1039, Test: 1.1097\n",
      "Epoch: 940, Loss: 1.8045, Train: 0.9103, Val: 1.0863, Test: 1.0939\n",
      "Epoch: 950, Loss: 1.7641, Train: 0.9049, Val: 1.0834, Test: 1.0896\n",
      "Epoch: 960, Loss: 1.7543, Train: 0.8916, Val: 1.0768, Test: 1.0846\n",
      "Epoch: 970, Loss: 1.7826, Train: 0.9211, Val: 1.0950, Test: 1.1011\n",
      "Epoch: 980, Loss: 1.7475, Train: 0.8877, Val: 1.0665, Test: 1.0790\n",
      "Epoch: 990, Loss: 1.7980, Train: 0.8901, Val: 1.0717, Test: 1.0800\n",
      "hidden_channels: 64\n",
      "layers: SAGE\n",
      "Final epoch:\n",
      "Epoch: 999, Loss: 1.7361, Train: 0.9003, Val: 1.0809, Test: 1.0921\n",
      "\n",
      "---------------------------------------------------------\n",
      "GAT | 64 hidden_channels\n",
      "Epoch: 010, Loss: 15.6307, Train: 3.2221, Val: 3.2372, Test: 3.2245\n",
      "Epoch: 020, Loss: 13.9045, Train: 2.7342, Val: 2.7644, Test: 2.7506\n",
      "Epoch: 030, Loss: 8.5306, Train: 2.1301, Val: 2.1780, Test: 2.1631\n",
      "Epoch: 040, Loss: 6.1493, Train: 1.6098, Val: 1.6740, Test: 1.6586\n",
      "Epoch: 050, Loss: 6.2627, Train: 1.3087, Val: 1.3820, Test: 1.3672\n",
      "Epoch: 060, Loss: 6.1552, Train: 1.2937, Val: 1.3656, Test: 1.3510\n",
      "Epoch: 070, Loss: 6.1150, Train: 1.3590, Val: 1.4278, Test: 1.4132\n",
      "Epoch: 080, Loss: 6.1077, Train: 1.3669, Val: 1.4351, Test: 1.4205\n",
      "Epoch: 090, Loss: 6.0563, Train: 1.3624, Val: 1.4282, Test: 1.4152\n",
      "Epoch: 100, Loss: 5.9906, Train: 1.3585, Val: 1.4181, Test: 1.4078\n",
      "Epoch: 110, Loss: 5.8416, Train: 1.2981, Val: 1.3549, Test: 1.3414\n",
      "Epoch: 120, Loss: 5.7213, Train: 1.3877, Val: 1.4258, Test: 1.4278\n",
      "Epoch: 130, Loss: 5.4873, Train: 1.3650, Val: 1.3863, Test: 1.3839\n",
      "Epoch: 140, Loss: 5.2913, Train: 1.3483, Val: 1.3675, Test: 1.3558\n",
      "Epoch: 150, Loss: 5.2204, Train: 1.2608, Val: 1.2802, Test: 1.2700\n",
      "Epoch: 160, Loss: 5.0474, Train: 1.2899, Val: 1.3034, Test: 1.2927\n",
      "Epoch: 170, Loss: 4.9448, Train: 1.3688, Val: 1.3807, Test: 1.3725\n",
      "Epoch: 180, Loss: 4.8523, Train: 1.3588, Val: 1.3661, Test: 1.3568\n",
      "Epoch: 190, Loss: 4.6364, Train: 1.3427, Val: 1.3497, Test: 1.3380\n",
      "Epoch: 200, Loss: 4.4569, Train: 1.2457, Val: 1.2517, Test: 1.2317\n",
      "Epoch: 210, Loss: 4.2856, Train: 1.2635, Val: 1.2754, Test: 1.2493\n",
      "Epoch: 220, Loss: 4.2234, Train: 1.1327, Val: 1.1457, Test: 1.1393\n",
      "Epoch: 230, Loss: 4.0468, Train: 1.1777, Val: 1.1914, Test: 1.1830\n",
      "Epoch: 240, Loss: 4.0140, Train: 1.2585, Val: 1.2747, Test: 1.2529\n",
      "Epoch: 250, Loss: 3.8675, Train: 1.1996, Val: 1.2149, Test: 1.2046\n",
      "Epoch: 260, Loss: 3.8728, Train: 1.3322, Val: 1.3527, Test: 1.3369\n",
      "Epoch: 270, Loss: 3.7988, Train: 1.2422, Val: 1.2648, Test: 1.2551\n",
      "Epoch: 280, Loss: 3.7290, Train: 1.1629, Val: 1.1847, Test: 1.1755\n",
      "Epoch: 290, Loss: 3.7103, Train: 1.1249, Val: 1.1491, Test: 1.1447\n",
      "Epoch: 300, Loss: 3.6437, Train: 1.1649, Val: 1.1915, Test: 1.1821\n",
      "Epoch: 310, Loss: 3.5987, Train: 1.1882, Val: 1.2154, Test: 1.2063\n",
      "Epoch: 320, Loss: 3.5796, Train: 1.1324, Val: 1.1580, Test: 1.1544\n",
      "Epoch: 330, Loss: 3.5571, Train: 1.2591, Val: 1.2838, Test: 1.2714\n",
      "Epoch: 340, Loss: 3.4863, Train: 1.1944, Val: 1.2227, Test: 1.2202\n",
      "Epoch: 350, Loss: 3.5828, Train: 1.0860, Val: 1.1161, Test: 1.1187\n",
      "Epoch: 360, Loss: 3.4709, Train: 1.1029, Val: 1.1315, Test: 1.1343\n",
      "Epoch: 370, Loss: 3.4221, Train: 1.1911, Val: 1.2182, Test: 1.2253\n",
      "Epoch: 380, Loss: 3.4209, Train: 1.1040, Val: 1.1337, Test: 1.1386\n",
      "Epoch: 390, Loss: 3.4097, Train: 1.2396, Val: 1.2672, Test: 1.2725\n",
      "Epoch: 400, Loss: 3.3654, Train: 1.1690, Val: 1.1991, Test: 1.2005\n",
      "Epoch: 410, Loss: 3.3987, Train: 1.0964, Val: 1.1289, Test: 1.1445\n",
      "Epoch: 420, Loss: 3.3790, Train: 1.1382, Val: 1.1712, Test: 1.1803\n",
      "Epoch: 430, Loss: 3.2788, Train: 1.1317, Val: 1.1688, Test: 1.1854\n",
      "Epoch: 440, Loss: 3.7005, Train: 1.3325, Val: 1.3555, Test: 1.3630\n",
      "Epoch: 450, Loss: 3.5753, Train: 1.1828, Val: 1.2054, Test: 1.2142\n",
      "Epoch: 460, Loss: 3.3759, Train: 1.1934, Val: 1.2234, Test: 1.2370\n",
      "Epoch: 470, Loss: 3.2868, Train: 1.1254, Val: 1.1595, Test: 1.1809\n",
      "Epoch: 480, Loss: 3.2248, Train: 1.1313, Val: 1.1660, Test: 1.1882\n",
      "Epoch: 490, Loss: 3.2030, Train: 1.0875, Val: 1.1287, Test: 1.1433\n",
      "Epoch: 500, Loss: 3.1932, Train: 1.1039, Val: 1.1485, Test: 1.1633\n",
      "Epoch: 510, Loss: 3.1713, Train: 1.1018, Val: 1.1472, Test: 1.1634\n",
      "Epoch: 520, Loss: 3.1494, Train: 1.0779, Val: 1.1251, Test: 1.1386\n",
      "Epoch: 530, Loss: 3.1365, Train: 1.1403, Val: 1.1805, Test: 1.1974\n",
      "Epoch: 540, Loss: 3.0898, Train: 1.0976, Val: 1.1456, Test: 1.1600\n",
      "Epoch: 550, Loss: 3.0401, Train: 1.1368, Val: 1.1818, Test: 1.2045\n",
      "Epoch: 560, Loss: 3.0816, Train: 1.0673, Val: 1.1196, Test: 1.1374\n",
      "Epoch: 570, Loss: 3.0875, Train: 1.1048, Val: 1.1573, Test: 1.1758\n",
      "Epoch: 580, Loss: 3.0904, Train: 1.1259, Val: 1.1723, Test: 1.2000\n",
      "Epoch: 590, Loss: 2.9881, Train: 1.0871, Val: 1.1393, Test: 1.1672\n",
      "Epoch: 600, Loss: 2.9996, Train: 1.1660, Val: 1.2125, Test: 1.2460\n",
      "Epoch: 610, Loss: 2.9416, Train: 1.1020, Val: 1.1532, Test: 1.1906\n",
      "Epoch: 620, Loss: 2.9073, Train: 1.0777, Val: 1.1345, Test: 1.1671\n",
      "Epoch: 630, Loss: 3.0268, Train: 1.2504, Val: 1.2885, Test: 1.3279\n",
      "Epoch: 640, Loss: 3.0109, Train: 1.0706, Val: 1.1251, Test: 1.1537\n",
      "Epoch: 650, Loss: 2.8766, Train: 1.1084, Val: 1.1598, Test: 1.1954\n",
      "Epoch: 660, Loss: 2.8496, Train: 1.0692, Val: 1.1298, Test: 1.1630\n",
      "Epoch: 670, Loss: 2.8123, Train: 1.0729, Val: 1.1304, Test: 1.1673\n",
      "Epoch: 680, Loss: 2.9014, Train: 1.1210, Val: 1.1694, Test: 1.2151\n",
      "Epoch: 690, Loss: 2.8816, Train: 1.1339, Val: 1.1843, Test: 1.2226\n",
      "Epoch: 700, Loss: 2.7920, Train: 1.0916, Val: 1.1491, Test: 1.1888\n",
      "Epoch: 710, Loss: 2.8086, Train: 1.0509, Val: 1.1105, Test: 1.1529\n",
      "Epoch: 720, Loss: 2.8600, Train: 1.1034, Val: 1.1594, Test: 1.2066\n",
      "Epoch: 730, Loss: 2.8053, Train: 1.0814, Val: 1.1402, Test: 1.1800\n",
      "Epoch: 740, Loss: 2.8426, Train: 1.1224, Val: 1.1763, Test: 1.2220\n",
      "Epoch: 750, Loss: 2.7807, Train: 1.0726, Val: 1.1285, Test: 1.1704\n",
      "Epoch: 760, Loss: 2.7377, Train: 1.0665, Val: 1.1272, Test: 1.1689\n",
      "Epoch: 770, Loss: 2.7101, Train: 1.0821, Val: 1.1434, Test: 1.1812\n",
      "Epoch: 780, Loss: 2.7339, Train: 1.0473, Val: 1.1112, Test: 1.1472\n",
      "Epoch: 790, Loss: 2.7165, Train: 1.0592, Val: 1.1206, Test: 1.1643\n",
      "Epoch: 800, Loss: 2.7417, Train: 1.1306, Val: 1.1934, Test: 1.2332\n",
      "Epoch: 810, Loss: 2.6683, Train: 1.0876, Val: 1.1482, Test: 1.1908\n",
      "Epoch: 820, Loss: 2.6706, Train: 1.0817, Val: 1.1469, Test: 1.1922\n",
      "Epoch: 830, Loss: 2.9062, Train: 1.0696, Val: 1.1348, Test: 1.1624\n",
      "Epoch: 840, Loss: 2.7632, Train: 1.0551, Val: 1.1109, Test: 1.1554\n",
      "Epoch: 850, Loss: 2.6489, Train: 1.0796, Val: 1.1467, Test: 1.1826\n",
      "Epoch: 860, Loss: 2.6138, Train: 1.0537, Val: 1.1221, Test: 1.1578\n",
      "Epoch: 870, Loss: 2.7143, Train: 1.0652, Val: 1.1327, Test: 1.1769\n",
      "Epoch: 880, Loss: 2.6139, Train: 1.0608, Val: 1.1300, Test: 1.1698\n",
      "Epoch: 890, Loss: 2.6200, Train: 1.0173, Val: 1.0841, Test: 1.1188\n",
      "Epoch: 900, Loss: 2.6897, Train: 1.0345, Val: 1.0982, Test: 1.1434\n",
      "Epoch: 910, Loss: 2.6816, Train: 1.0643, Val: 1.1311, Test: 1.1673\n",
      "Epoch: 920, Loss: 2.6104, Train: 1.1018, Val: 1.1698, Test: 1.2111\n",
      "Epoch: 930, Loss: 2.5716, Train: 1.0279, Val: 1.0970, Test: 1.1471\n",
      "Epoch: 940, Loss: 2.5191, Train: 1.0645, Val: 1.1344, Test: 1.1834\n",
      "Epoch: 950, Loss: 2.9672, Train: 1.1980, Val: 1.2493, Test: 1.2673\n",
      "Epoch: 960, Loss: 2.6621, Train: 1.0905, Val: 1.1557, Test: 1.1898\n",
      "Epoch: 970, Loss: 2.5554, Train: 1.0256, Val: 1.0957, Test: 1.1378\n",
      "Epoch: 980, Loss: 2.4889, Train: 1.0254, Val: 1.0993, Test: 1.1494\n",
      "Epoch: 990, Loss: 2.5110, Train: 1.0524, Val: 1.1298, Test: 1.1721\n",
      "hidden_channels: 64\n",
      "layers: GAT\n",
      "Final epoch:\n",
      "Epoch: 999, Loss: 2.5263, Train: 1.0701, Val: 1.1438, Test: 1.1918\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    hidden_channels = model.get(\"hidden_channels\")\n",
    "    layer_name = model.get(\"layer_name\")\n",
    "    print(layer_name, \"|\", hidden_channels, \"hidden_channels\")\n",
    "    my_model = ModelBuilderTrainerTester(\n",
    "        layer_name=layer_name,\n",
    "        train_data=train_data,\n",
    "        val_data=val_data,\n",
    "        test_data=test_data,\n",
    "        hidden_channels=hidden_channels,\n",
    "    )\n",
    "    my_model.train_test(epochs=1000)\n",
    "    print(\"\\n---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer | 64 hidden_channels\n",
      "Epoch: 010, Loss: 15.9526, Train: 3.1762, Val: 3.1707, Test: 3.1727\n",
      "Epoch: 020, Loss: 6.6684, Train: 1.1358, Val: 1.1307, Test: 1.1330\n",
      "Epoch: 030, Loss: 6.1654, Train: 1.3282, Val: 1.3329, Test: 1.3337\n",
      "Epoch: 040, Loss: 6.2509, Train: 1.5258, Val: 1.5287, Test: 1.5298\n",
      "Epoch: 050, Loss: 6.1423, Train: 1.3833, Val: 1.3861, Test: 1.3873\n",
      "Epoch: 060, Loss: 6.1302, Train: 1.3290, Val: 1.3322, Test: 1.3333\n",
      "Epoch: 070, Loss: 6.0765, Train: 1.3520, Val: 1.3562, Test: 1.3570\n",
      "Epoch: 080, Loss: 5.9007, Train: 1.2967, Val: 1.3042, Test: 1.3038\n",
      "Epoch: 090, Loss: 5.3109, Train: 1.1792, Val: 1.1922, Test: 1.1790\n",
      "Epoch: 100, Loss: 4.9238, Train: 1.3878, Val: 1.3940, Test: 1.3828\n",
      "Epoch: 110, Loss: 4.6709, Train: 1.2180, Val: 1.2282, Test: 1.2132\n",
      "Epoch: 120, Loss: 4.3941, Train: 1.2368, Val: 1.2458, Test: 1.2340\n",
      "Epoch: 130, Loss: 4.2831, Train: 1.3016, Val: 1.3078, Test: 1.2959\n",
      "Epoch: 140, Loss: 3.9700, Train: 1.2541, Val: 1.2640, Test: 1.2522\n",
      "Epoch: 150, Loss: 3.8447, Train: 1.1491, Val: 1.1595, Test: 1.1498\n",
      "Epoch: 160, Loss: 3.7701, Train: 1.2500, Val: 1.2564, Test: 1.2475\n",
      "Epoch: 170, Loss: 3.6967, Train: 1.1703, Val: 1.1757, Test: 1.1696\n",
      "Epoch: 180, Loss: 3.6250, Train: 1.2259, Val: 1.2264, Test: 1.2250\n",
      "Epoch: 190, Loss: 3.5554, Train: 1.1859, Val: 1.1881, Test: 1.1855\n",
      "Epoch: 200, Loss: 3.5460, Train: 1.2515, Val: 1.2518, Test: 1.2493\n",
      "Epoch: 210, Loss: 3.4975, Train: 1.1411, Val: 1.1464, Test: 1.1453\n",
      "Epoch: 220, Loss: 3.4617, Train: 1.1561, Val: 1.1599, Test: 1.1590\n",
      "Epoch: 230, Loss: 3.4411, Train: 1.2306, Val: 1.2284, Test: 1.2315\n",
      "Epoch: 240, Loss: 3.4534, Train: 1.1393, Val: 1.1441, Test: 1.1469\n",
      "Epoch: 250, Loss: 3.4071, Train: 1.1335, Val: 1.1412, Test: 1.1458\n",
      "Epoch: 260, Loss: 3.4128, Train: 1.1482, Val: 1.1506, Test: 1.1523\n",
      "Epoch: 270, Loss: 3.3116, Train: 1.1325, Val: 1.1397, Test: 1.1439\n",
      "Epoch: 280, Loss: 3.3648, Train: 1.0697, Val: 1.0826, Test: 1.0821\n",
      "Epoch: 290, Loss: 3.2799, Train: 1.1232, Val: 1.1317, Test: 1.1353\n",
      "Epoch: 300, Loss: 3.1907, Train: 1.1411, Val: 1.1530, Test: 1.1563\n",
      "Epoch: 310, Loss: 3.1045, Train: 1.1074, Val: 1.1241, Test: 1.1294\n",
      "Epoch: 320, Loss: 3.1624, Train: 1.0829, Val: 1.1046, Test: 1.1109\n",
      "Epoch: 330, Loss: 3.0585, Train: 1.1105, Val: 1.1319, Test: 1.1409\n",
      "Epoch: 340, Loss: 3.0134, Train: 1.0808, Val: 1.1074, Test: 1.1149\n",
      "Epoch: 350, Loss: 2.9865, Train: 1.0766, Val: 1.1060, Test: 1.1136\n",
      "Epoch: 360, Loss: 2.9401, Train: 1.0911, Val: 1.1222, Test: 1.1296\n",
      "Epoch: 370, Loss: 2.9373, Train: 1.0615, Val: 1.0950, Test: 1.1028\n",
      "Epoch: 380, Loss: 2.9359, Train: 1.1239, Val: 1.1550, Test: 1.1620\n",
      "Epoch: 390, Loss: 2.8520, Train: 1.1029, Val: 1.1379, Test: 1.1475\n",
      "Epoch: 400, Loss: 2.8375, Train: 1.0942, Val: 1.1296, Test: 1.1446\n",
      "Epoch: 410, Loss: 2.8544, Train: 1.1444, Val: 1.1751, Test: 1.1892\n",
      "Epoch: 420, Loss: 2.8702, Train: 1.0897, Val: 1.1295, Test: 1.1355\n",
      "Epoch: 430, Loss: 2.8103, Train: 1.0579, Val: 1.1014, Test: 1.1107\n",
      "Epoch: 440, Loss: 2.7899, Train: 1.0935, Val: 1.1353, Test: 1.1493\n",
      "Epoch: 450, Loss: 2.8172, Train: 1.0266, Val: 1.0755, Test: 1.0860\n",
      "Epoch: 460, Loss: 2.8268, Train: 1.0487, Val: 1.0941, Test: 1.1041\n",
      "Epoch: 470, Loss: 2.7546, Train: 1.0684, Val: 1.1117, Test: 1.1265\n",
      "Epoch: 480, Loss: 2.7426, Train: 1.1069, Val: 1.1473, Test: 1.1655\n",
      "Epoch: 490, Loss: 2.8018, Train: 1.0677, Val: 1.1122, Test: 1.1229\n",
      "Epoch: 500, Loss: 2.7593, Train: 1.0610, Val: 1.1096, Test: 1.1224\n",
      "Epoch: 510, Loss: 2.7262, Train: 1.0490, Val: 1.0994, Test: 1.1099\n",
      "Epoch: 520, Loss: 2.6838, Train: 1.0878, Val: 1.1341, Test: 1.1505\n",
      "Epoch: 530, Loss: 2.7405, Train: 1.0464, Val: 1.0993, Test: 1.1104\n",
      "Epoch: 540, Loss: 2.7062, Train: 1.0562, Val: 1.1109, Test: 1.1228\n",
      "Epoch: 550, Loss: 2.6599, Train: 1.0476, Val: 1.1059, Test: 1.1186\n",
      "Epoch: 560, Loss: 2.6495, Train: 1.0704, Val: 1.1222, Test: 1.1393\n",
      "Epoch: 570, Loss: 2.6441, Train: 1.0618, Val: 1.1168, Test: 1.1333\n",
      "Epoch: 580, Loss: 2.7418, Train: 1.0489, Val: 1.1072, Test: 1.1194\n",
      "Epoch: 590, Loss: 2.7190, Train: 1.0447, Val: 1.1045, Test: 1.1162\n",
      "Epoch: 600, Loss: 2.6952, Train: 1.0425, Val: 1.1048, Test: 1.1152\n",
      "Epoch: 610, Loss: 2.6334, Train: 1.0383, Val: 1.1018, Test: 1.1159\n",
      "Epoch: 620, Loss: 2.6936, Train: 1.1128, Val: 1.1667, Test: 1.1900\n",
      "Epoch: 630, Loss: 2.6036, Train: 1.0654, Val: 1.1260, Test: 1.1421\n",
      "Epoch: 640, Loss: 2.6094, Train: 1.0562, Val: 1.1199, Test: 1.1343\n",
      "Epoch: 650, Loss: 2.6166, Train: 1.0208, Val: 1.0931, Test: 1.1051\n",
      "Epoch: 660, Loss: 2.6506, Train: 1.0249, Val: 1.0939, Test: 1.1051\n",
      "Epoch: 670, Loss: 2.6350, Train: 1.0466, Val: 1.1202, Test: 1.1259\n",
      "Epoch: 680, Loss: 2.6190, Train: 1.0445, Val: 1.1178, Test: 1.1274\n",
      "Epoch: 690, Loss: 2.5713, Train: 1.0362, Val: 1.1075, Test: 1.1227\n",
      "Epoch: 700, Loss: 2.6194, Train: 1.0821, Val: 1.1483, Test: 1.1629\n",
      "Epoch: 710, Loss: 2.5784, Train: 1.0725, Val: 1.1407, Test: 1.1563\n",
      "Epoch: 720, Loss: 2.6539, Train: 1.0246, Val: 1.1024, Test: 1.1046\n",
      "Epoch: 730, Loss: 2.6851, Train: 1.0004, Val: 1.0778, Test: 1.0847\n",
      "Epoch: 740, Loss: 2.5842, Train: 1.0662, Val: 1.1343, Test: 1.1504\n",
      "Epoch: 750, Loss: 2.5470, Train: 1.0665, Val: 1.1417, Test: 1.1525\n",
      "Epoch: 760, Loss: 2.5444, Train: 1.0526, Val: 1.1278, Test: 1.1451\n",
      "Epoch: 770, Loss: 2.5776, Train: 1.1012, Val: 1.1700, Test: 1.1905\n",
      "Epoch: 780, Loss: 2.5772, Train: 1.0813, Val: 1.1514, Test: 1.1701\n",
      "Epoch: 790, Loss: 2.6333, Train: 1.1238, Val: 1.1873, Test: 1.1981\n",
      "Epoch: 800, Loss: 2.6035, Train: 1.0544, Val: 1.1231, Test: 1.1384\n",
      "Epoch: 810, Loss: 2.5369, Train: 1.0386, Val: 1.1130, Test: 1.1309\n",
      "Epoch: 820, Loss: 2.5259, Train: 1.0600, Val: 1.1363, Test: 1.1601\n",
      "Epoch: 830, Loss: 2.5398, Train: 1.0249, Val: 1.1025, Test: 1.1171\n",
      "Epoch: 840, Loss: 2.5140, Train: 1.0557, Val: 1.1337, Test: 1.1527\n",
      "Epoch: 850, Loss: 2.5556, Train: 1.0861, Val: 1.1605, Test: 1.1861\n",
      "Epoch: 860, Loss: 2.5290, Train: 1.0465, Val: 1.1247, Test: 1.1448\n",
      "Epoch: 870, Loss: 2.5251, Train: 1.0168, Val: 1.1003, Test: 1.1184\n",
      "Epoch: 880, Loss: 2.5083, Train: 1.0334, Val: 1.1115, Test: 1.1300\n",
      "Epoch: 890, Loss: 2.5470, Train: 1.0986, Val: 1.1724, Test: 1.1932\n",
      "Epoch: 900, Loss: 2.5456, Train: 1.1108, Val: 1.1799, Test: 1.2071\n",
      "Epoch: 910, Loss: 2.5633, Train: 1.1022, Val: 1.1717, Test: 1.1901\n",
      "Epoch: 920, Loss: 2.5839, Train: 1.0545, Val: 1.1350, Test: 1.1472\n",
      "Epoch: 930, Loss: 2.5207, Train: 1.0368, Val: 1.1170, Test: 1.1303\n",
      "Epoch: 940, Loss: 2.4793, Train: 1.0322, Val: 1.1123, Test: 1.1336\n",
      "Epoch: 950, Loss: 2.4784, Train: 1.0501, Val: 1.1253, Test: 1.1450\n",
      "Epoch: 960, Loss: 2.5203, Train: 1.0327, Val: 1.1090, Test: 1.1296\n",
      "Epoch: 970, Loss: 2.4896, Train: 1.0268, Val: 1.1080, Test: 1.1268\n",
      "Epoch: 980, Loss: 2.4649, Train: 1.0392, Val: 1.1181, Test: 1.1355\n",
      "Epoch: 990, Loss: 2.4581, Train: 1.0340, Val: 1.1173, Test: 1.1350\n",
      "hidden_channels: 64\n",
      "layers: Transformer\n",
      "Final epoch:\n",
      "Epoch: 999, Loss: 2.4861, Train: 1.0099, Val: 1.0952, Test: 1.1079\n"
     ]
    }
   ],
   "source": [
    "layer_name = \"Transformer\"\n",
    "in_channels = -1\n",
    "hidden_channels = 64\n",
    "print(layer_name, \"|\", hidden_channels, \"hidden_channels\")\n",
    "my_model = ModelBuilderTrainerTester(\n",
    "    layer_name=layer_name,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    in_channels=-1,\n",
    "    hidden_channels=64,\n",
    ")\n",
    "my_model.train_test(epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('environ': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30f2f1cc4e8893d79f67050195b6a4f8b5eb4fae2ee540a87781398e7c71c355"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
