{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, LazyLinear, Sequential, BatchNorm1d, ReLU\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import MovieLens\n",
    "from torch_geometric.nn import SAGEConv, to_hetero, LightGCN\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "parent_path = pathlib.Path(os.getcwd()).parent.absolute()\n",
    "sys.path.append(str(parent_path))\n",
    "from utils.Neo4jMovieLensMetaData import Neo4jMovieLensMetaData\n",
    "from utils.gnn_simple import Model\n",
    "from utils.visualize import plot_loss, plot_test, plot_val\n",
    "from utils.EarlyStopper import EarlyStopper\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies have features...\n",
      "Encoding title...\n",
      "Encoding original_title...\n",
      "Encoding fastRP_genres...\n",
      "Encoding fastRP_keywords...\n",
      "Encoding fastRP_cast...\n",
      "Encoding fastRP_crew...\n",
      "Encoding fastRP_production_companies...\n",
      "Encoding fastRP_production_countries...\n",
      "Encoding fastRP_spoken_languages...\n",
      "[torch.Size([9067, 384]), torch.Size([9067, 384]), torch.Size([9067, 256]), torch.Size([9067, 256]), torch.Size([9067, 256]), torch.Size([9067, 256]), torch.Size([9067, 256]), torch.Size([9067, 256]), torch.Size([9067, 256])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = osp.join(osp.dirname(osp.abspath('')), '../../data/MovieLensNeo4jMetaData')\n",
    "dataset = Neo4jMovieLensMetaData(\n",
    "    path,\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    database_url=\"bolt://localhost:7687\",\n",
    "    database_username=\"neo4j\",\n",
    "    database_password=\"admin\",\n",
    "    force_pre_process=True,\n",
    "    force_db_restore=False,\n",
    "    text_features=[\"title\", \"original_title\"],\n",
    "    list_features=[],\n",
    "    fastRP_features=[\"fastRP_genres\", \"fastRP_keywords\", \"fastRP_cast\", \"fastRP_crew\", \"fastRP_production_companies\", \"fastRP_production_countries\", \"fastRP_spoken_languages\"],\n",
    "    numeric_features=[],\n",
    "    node2vec_features=[],\n",
    "    SAGE_features=[],\n",
    ")\n",
    "\n",
    "# path = osp.join('../../data/MovieLens')\n",
    "# dataset = MovieLens(path, model_name='all-MiniLM-L6-v2')\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "# Add user node features for message passing:\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes\n",
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label.\n",
    "\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100718])\n",
      "torch.Size([2, 80576])\n",
      "torch.Size([2, 80576])\n",
      "torch.Size([2, 90647])\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_index_dict.get((\"user\" , \"rates\", \"movie\")).shape)\n",
    "print(train_data.edge_index_dict.get((\"user\" , \"rates\", \"movie\")).shape)\n",
    "print(val_data.edge_index_dict.get((\"user\" , \"rates\", \"movie\")).shape)\n",
    "print(test_data.edge_index_dict.get((\"user\" , \"rates\", \"movie\")).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100718])\n",
      "torch.Size([80576])\n",
      "torch.Size([10071])\n",
      "torch.Size([10071])\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_label_dict.get((\"user\" , \"rates\", \"movie\")).shape)\n",
    "print(train_data.edge_label_dict.get((\"user\" , \"rates\", \"movie\")).shape)\n",
    "print(val_data.edge_label_dict.get((\"user\" , \"rates\", \"movie\")).shape)\n",
    "print(test_data.edge_label_dict.get((\"user\" , \"rates\", \"movie\")).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9067, 2560])\n",
      "torch.Size([672, 672])\n",
      "------------------\n",
      "torch.Size([9067, 2560])\n",
      "torch.Size([672, 672])\n",
      "------------------\n",
      "torch.Size([9067, 2560])\n",
      "torch.Size([672, 672])\n",
      "------------------\n",
      "torch.Size([9067, 2560])\n",
      "torch.Size([672, 672])\n"
     ]
    }
   ],
   "source": [
    "print(data.x_dict.get(\"movie\").shape)\n",
    "print(data.x_dict.get(\"user\").shape)\n",
    "print(\"------------------\")\n",
    "print(train_data.x_dict.get(\"movie\").shape)\n",
    "print(train_data.x_dict.get(\"user\").shape)\n",
    "print(\"------------------\")\n",
    "print(val_data.x_dict.get(\"movie\").shape)\n",
    "print(val_data.x_dict.get(\"user\").shape)\n",
    "print(\"------------------\")\n",
    "print(test_data.x_dict.get(\"movie\").shape)\n",
    "print(test_data.x_dict.get(\"user\").shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_predictions = []\n",
    "losses_ = {}\n",
    "\n",
    "def train_test(model, epochs, train_data, test_data, val_data, logging_step, lr=0.01, use_weighted_loss=False, use_rounding=False):\n",
    "\n",
    "    # Due to lazy initialization, we need to run one model step so the number\n",
    "    # of parameters can be inferred:\n",
    "    with torch.no_grad():\n",
    "        model.encoder(train_data.x_dict, train_data.edge_index_dict)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    weight = None\n",
    "    if use_weighted_loss:\n",
    "        weight = torch.bincount(train_data['user', 'movie'].edge_label)\n",
    "        weight = weight.max() / weight\n",
    "\n",
    "    def weighted_rmse_loss(pred, target, weight=None):\n",
    "        weight = 1. if weight is None else weight[target].to(pred.dtype)\n",
    "        # return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()\n",
    "        return (weight * (pred - target.to(pred.dtype)).pow(2)).mean().sqrt()\n",
    "    \n",
    "    def train(log=False):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                        train_data['user', 'movie'].edge_label_index)\n",
    "        # print(pred[:10])\n",
    "        target = train_data['user', 'movie'].edge_label\n",
    "\n",
    "        loss = weighted_rmse_loss(pred, target, weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return float(loss)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(data, log=False):\n",
    "        model.eval()\n",
    "        pred = model(data.x_dict, data.edge_index_dict,\n",
    "                    data['user', 'movie'].edge_label_index)\n",
    "        pred = pred.clamp(min=0, max=5)\n",
    "        if use_rounding:\n",
    "            # round the tensor values to steps of 0.5\n",
    "            pred = torch.round(pred * 2) / 2\n",
    "\n",
    "        target = data['user', 'movie'].edge_label.float()\n",
    "        rmse = F.mse_loss(pred, target).sqrt()\n",
    "        if log:\n",
    "            predictions = pred.detach().numpy()\n",
    "            plt.hist(predictions, range=[0, 5])\n",
    "            plt.show()\n",
    "        return float(rmse)\n",
    "    \n",
    "    early_stopper = EarlyStopper(patience=30, min_delta=0.01)\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # add learning rate decay to optimizer\n",
    "        optimizer.param_groups[0]['lr'] = lr / (1 + 0.05 * epoch)\n",
    "\n",
    "        loss = train(log=not(epoch%20))\n",
    "        train_rmse = test(train_data)\n",
    "        val_rmse = test(val_data)\n",
    "        test_rmse = test(test_data, log=False)\n",
    "        losses.append((loss, train_rmse, val_rmse, test_rmse))\n",
    "        if (logging_step and not epoch%logging_step) or (not logging_step):\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "                f'Val: {val_rmse:.4f}, Test: {test_rmse:.4f}')\n",
    "        if epoch > 30 and early_stopper.early_stop(val_rmse):\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "    \n",
    "    last_losses = losses[-1]\n",
    "    losses = losses + [last_losses] * (epochs - len(losses))\n",
    "\n",
    "    losses_[\"LIGHT\"] = losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without rounding with 0.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGE\n",
      "Aggregation: None\n",
      "Epoch: 001, Loss: 3.6136, Train: 3.5677, Val: 3.5530, Test: 3.5663\n",
      "Epoch: 002, Loss: 3.5677, Train: 3.5276, Val: 3.5129, Test: 3.5261\n",
      "Epoch: 003, Loss: 3.5276, Train: 3.4877, Val: 3.4731, Test: 3.4862\n",
      "Epoch: 004, Loss: 3.4877, Train: 3.4485, Val: 3.4339, Test: 3.4469\n",
      "Epoch: 005, Loss: 3.4485, Train: 3.4070, Val: 3.3924, Test: 3.4054\n",
      "Epoch: 006, Loss: 3.4070, Train: 3.3543, Val: 3.3399, Test: 3.3527\n",
      "Epoch: 007, Loss: 3.3543, Train: 3.2823, Val: 3.2681, Test: 3.2808\n",
      "Epoch: 008, Loss: 3.2823, Train: 3.1721, Val: 3.1585, Test: 3.1709\n",
      "Epoch: 009, Loss: 3.1721, Train: 2.9796, Val: 2.9672, Test: 2.9790\n",
      "Epoch: 010, Loss: 2.9796, Train: 2.6381, Val: 2.6283, Test: 2.6386\n",
      "Epoch: 011, Loss: 2.6381, Train: 2.0498, Val: 2.0454, Test: 2.0521\n",
      "Epoch: 012, Loss: 2.0498, Train: 1.2253, Val: 1.2326, Test: 1.2244\n",
      "Epoch: 013, Loss: 1.2253, Train: 1.9316, Val: 1.9275, Test: 1.9038\n",
      "Epoch: 014, Loss: 2.1876, Train: 1.7886, Val: 1.7892, Test: 1.7645\n",
      "Epoch: 015, Loss: 1.7921, Train: 1.1620, Val: 1.1713, Test: 1.1496\n",
      "Epoch: 016, Loss: 1.1620, Train: 1.2351, Val: 1.2418, Test: 1.2342\n",
      "Epoch: 017, Loss: 1.2351, Train: 1.4757, Val: 1.4780, Test: 1.4773\n",
      "Epoch: 018, Loss: 1.4757, Train: 1.5967, Val: 1.5972, Test: 1.5987\n",
      "Epoch: 019, Loss: 1.5967, Train: 1.6041, Val: 1.6045, Test: 1.6061\n",
      "Epoch: 020, Loss: 1.6041, Train: 1.5198, Val: 1.5213, Test: 1.5215\n",
      "Epoch: 021, Loss: 1.5198, Train: 1.3636, Val: 1.3675, Test: 1.3643\n",
      "Epoch: 022, Loss: 1.3636, Train: 1.1835, Val: 1.1910, Test: 1.1812\n",
      "Epoch: 023, Loss: 1.1835, Train: 1.1151, Val: 1.1248, Test: 1.1061\n",
      "Epoch: 024, Loss: 1.1151, Train: 1.2514, Val: 1.2591, Test: 1.2356\n",
      "Epoch: 025, Loss: 1.2514, Train: 1.3617, Val: 1.3678, Test: 1.3435\n",
      "Epoch: 026, Loss: 1.3617, Train: 1.2922, Val: 1.2993, Test: 1.2755\n",
      "Epoch: 027, Loss: 1.2922, Train: 1.1550, Val: 1.1642, Test: 1.1427\n",
      "Epoch: 028, Loss: 1.1550, Train: 1.1085, Val: 1.1179, Test: 1.1016\n",
      "Epoch: 029, Loss: 1.1085, Train: 1.1554, Val: 1.1633, Test: 1.1522\n",
      "Epoch: 030, Loss: 1.1554, Train: 1.2118, Val: 1.2184, Test: 1.2102\n",
      "Epoch: 031, Loss: 1.2118, Train: 1.2332, Val: 1.2393, Test: 1.2321\n",
      "Epoch: 032, Loss: 1.2332, Train: 1.2135, Val: 1.2200, Test: 1.2120\n",
      "Epoch: 033, Loss: 1.2135, Train: 1.1660, Val: 1.1736, Test: 1.1633\n",
      "Epoch: 034, Loss: 1.1660, Train: 1.1183, Val: 1.1272, Test: 1.1133\n",
      "Epoch: 035, Loss: 1.1183, Train: 1.1038, Val: 1.1136, Test: 1.0957\n",
      "Epoch: 036, Loss: 1.1038, Train: 1.1305, Val: 1.1401, Test: 1.1195\n",
      "Epoch: 037, Loss: 1.1305, Train: 1.1618, Val: 1.1709, Test: 1.1491\n",
      "Epoch: 038, Loss: 1.1618, Train: 1.1602, Val: 1.1693, Test: 1.1476\n",
      "Epoch: 039, Loss: 1.1602, Train: 1.1302, Val: 1.1399, Test: 1.1192\n",
      "Epoch: 040, Loss: 1.1302, Train: 1.1040, Val: 1.1139, Test: 1.0953\n",
      "Epoch: 041, Loss: 1.1040, Train: 1.1010, Val: 1.1107, Test: 1.0947\n",
      "Epoch: 042, Loss: 1.1010, Train: 1.1144, Val: 1.1236, Test: 1.1097\n",
      "Epoch: 043, Loss: 1.1144, Train: 1.1273, Val: 1.1361, Test: 1.1235\n",
      "Epoch: 044, Loss: 1.1273, Train: 1.1292, Val: 1.1379, Test: 1.1255\n",
      "Epoch: 045, Loss: 1.1292, Train: 1.1197, Val: 1.1287, Test: 1.1156\n",
      "Epoch: 046, Loss: 1.1197, Train: 1.1056, Val: 1.1151, Test: 1.1004\n",
      "Epoch: 047, Loss: 1.1056, Train: 1.0963, Val: 1.1062, Test: 1.0898\n",
      "Epoch: 048, Loss: 1.0963, Train: 1.0972, Val: 1.1073, Test: 1.0892\n",
      "Epoch: 049, Loss: 1.0972, Train: 1.1048, Val: 1.1149, Test: 1.0956\n",
      "Epoch: 050, Loss: 1.1048, Train: 1.1099, Val: 1.1200, Test: 1.1003\n",
      "Epoch: 051, Loss: 1.1099, Train: 1.1073, Val: 1.1174, Test: 1.0978\n",
      "Epoch: 052, Loss: 1.1073, Train: 1.0995, Val: 1.1097, Test: 1.0908\n",
      "Epoch: 053, Loss: 1.0995, Train: 1.0932, Val: 1.1035, Test: 1.0857\n",
      "Epoch: 054, Loss: 1.0932, Train: 1.0923, Val: 1.1024, Test: 1.0859\n",
      "Epoch: 055, Loss: 1.0923, Train: 1.0952, Val: 1.1052, Test: 1.0896\n",
      "Epoch: 056, Loss: 1.0952, Train: 1.0980, Val: 1.1079, Test: 1.0930\n",
      "Epoch: 057, Loss: 1.0980, Train: 1.0981, Val: 1.1080, Test: 1.0932\n",
      "Epoch: 058, Loss: 1.0981, Train: 1.0953, Val: 1.1053, Test: 1.0901\n",
      "Epoch: 059, Loss: 1.0953, Train: 1.0914, Val: 1.1016, Test: 1.0857\n",
      "Epoch: 060, Loss: 1.0914, Train: 1.0889, Val: 1.0993, Test: 1.0825\n",
      "Epoch: 061, Loss: 1.0889, Train: 1.0888, Val: 1.0993, Test: 1.0817\n",
      "Epoch: 062, Loss: 1.0888, Train: 1.0902, Val: 1.1007, Test: 1.0826\n",
      "Epoch: 063, Loss: 1.0902, Train: 1.0911, Val: 1.1017, Test: 1.0833\n",
      "Epoch: 064, Loss: 1.0911, Train: 1.0903, Val: 1.1010, Test: 1.0826\n",
      "Epoch: 065, Loss: 1.0903, Train: 1.0883, Val: 1.0989, Test: 1.0808\n",
      "Epoch: 066, Loss: 1.0883, Train: 1.0863, Val: 1.0969, Test: 1.0794\n",
      "Epoch: 067, Loss: 1.0863, Train: 1.0854, Val: 1.0960, Test: 1.0790\n",
      "Epoch: 068, Loss: 1.0854, Train: 1.0855, Val: 1.0961, Test: 1.0797\n",
      "Epoch: 069, Loss: 1.0855, Train: 1.0860, Val: 1.0965, Test: 1.0804\n",
      "Epoch: 070, Loss: 1.0860, Train: 1.0859, Val: 1.0965, Test: 1.0804\n",
      "Epoch: 071, Loss: 1.0859, Train: 1.0850, Val: 1.0957, Test: 1.0795\n",
      "Epoch: 072, Loss: 1.0850, Train: 1.0838, Val: 1.0945, Test: 1.0781\n",
      "Epoch: 073, Loss: 1.0838, Train: 1.0827, Val: 1.0935, Test: 1.0767\n",
      "Epoch: 074, Loss: 1.0827, Train: 1.0822, Val: 1.0930, Test: 1.0758\n",
      "Epoch: 075, Loss: 1.0822, Train: 1.0821, Val: 1.0930, Test: 1.0755\n",
      "Epoch: 076, Loss: 1.0821, Train: 1.0821, Val: 1.0930, Test: 1.0753\n",
      "Epoch: 077, Loss: 1.0821, Train: 1.0817, Val: 1.0927, Test: 1.0750\n",
      "Epoch: 078, Loss: 1.0817, Train: 1.0810, Val: 1.0921, Test: 1.0744\n",
      "Epoch: 079, Loss: 1.0810, Train: 1.0802, Val: 1.0912, Test: 1.0738\n",
      "Epoch: 080, Loss: 1.0802, Train: 1.0795, Val: 1.0906, Test: 1.0734\n",
      "Epoch: 081, Loss: 1.0795, Train: 1.0791, Val: 1.0902, Test: 1.0733\n",
      "Epoch: 082, Loss: 1.0791, Train: 1.0789, Val: 1.0900, Test: 1.0732\n",
      "Epoch: 083, Loss: 1.0789, Train: 1.0786, Val: 1.0897, Test: 1.0731\n",
      "Epoch: 084, Loss: 1.0786, Train: 1.0782, Val: 1.0893, Test: 1.0727\n",
      "Epoch: 085, Loss: 1.0782, Train: 1.0776, Val: 1.0888, Test: 1.0721\n",
      "Epoch: 086, Loss: 1.0776, Train: 1.0770, Val: 1.0883, Test: 1.0714\n",
      "Epoch: 087, Loss: 1.0770, Train: 1.0765, Val: 1.0878, Test: 1.0708\n",
      "Epoch: 088, Loss: 1.0765, Train: 1.0761, Val: 1.0875, Test: 1.0703\n",
      "Epoch: 089, Loss: 1.0761, Train: 1.0758, Val: 1.0872, Test: 1.0699\n",
      "Epoch: 090, Loss: 1.0758, Train: 1.0755, Val: 1.0869, Test: 1.0695\n",
      "Epoch: 091, Loss: 1.0755, Train: 1.0751, Val: 1.0865, Test: 1.0691\n",
      "Epoch: 092, Loss: 1.0751, Train: 1.0746, Val: 1.0860, Test: 1.0687\n",
      "Epoch: 093, Loss: 1.0746, Train: 1.0741, Val: 1.0856, Test: 1.0684\n",
      "Epoch: 094, Loss: 1.0741, Train: 1.0737, Val: 1.0851, Test: 1.0681\n",
      "Epoch: 095, Loss: 1.0737, Train: 1.0733, Val: 1.0848, Test: 1.0678\n",
      "Epoch: 096, Loss: 1.0733, Train: 1.0729, Val: 1.0844, Test: 1.0676\n",
      "Epoch: 097, Loss: 1.0729, Train: 1.0725, Val: 1.0841, Test: 1.0673\n",
      "Epoch: 098, Loss: 1.0725, Train: 1.0721, Val: 1.0837, Test: 1.0669\n",
      "Epoch: 099, Loss: 1.0721, Train: 1.0717, Val: 1.0833, Test: 1.0664\n",
      "Epoch: 100, Loss: 1.0717, Train: 1.0713, Val: 1.0829, Test: 1.0660\n",
      "Epoch: 101, Loss: 1.0713, Train: 1.0709, Val: 1.0826, Test: 1.0655\n",
      "Epoch: 102, Loss: 1.0709, Train: 1.0705, Val: 1.0822, Test: 1.0651\n",
      "Epoch: 103, Loss: 1.0705, Train: 1.0701, Val: 1.0819, Test: 1.0647\n",
      "Epoch: 104, Loss: 1.0701, Train: 1.0697, Val: 1.0815, Test: 1.0643\n",
      "Epoch: 105, Loss: 1.0697, Train: 1.0693, Val: 1.0812, Test: 1.0640\n",
      "Epoch: 106, Loss: 1.0693, Train: 1.0689, Val: 1.0808, Test: 1.0636\n",
      "Epoch: 107, Loss: 1.0689, Train: 1.0685, Val: 1.0804, Test: 1.0633\n",
      "Epoch: 108, Loss: 1.0685, Train: 1.0681, Val: 1.0801, Test: 1.0630\n",
      "Epoch: 109, Loss: 1.0681, Train: 1.0678, Val: 1.0797, Test: 1.0627\n",
      "Epoch: 110, Loss: 1.0678, Train: 1.0674, Val: 1.0794, Test: 1.0624\n",
      "Epoch: 111, Loss: 1.0674, Train: 1.0670, Val: 1.0790, Test: 1.0620\n",
      "Epoch: 112, Loss: 1.0670, Train: 1.0666, Val: 1.0787, Test: 1.0617\n",
      "Epoch: 113, Loss: 1.0666, Train: 1.0662, Val: 1.0783, Test: 1.0613\n",
      "Epoch: 114, Loss: 1.0662, Train: 1.0659, Val: 1.0779, Test: 1.0609\n",
      "Epoch: 115, Loss: 1.0659, Train: 1.0655, Val: 1.0776, Test: 1.0605\n",
      "Epoch: 116, Loss: 1.0655, Train: 1.0651, Val: 1.0773, Test: 1.0602\n",
      "Epoch: 117, Loss: 1.0651, Train: 1.0647, Val: 1.0769, Test: 1.0598\n",
      "Epoch: 118, Loss: 1.0647, Train: 1.0644, Val: 1.0766, Test: 1.0594\n",
      "Epoch: 119, Loss: 1.0644, Train: 1.0640, Val: 1.0762, Test: 1.0591\n",
      "Epoch: 120, Loss: 1.0640, Train: 1.0636, Val: 1.0759, Test: 1.0588\n",
      "Epoch: 121, Loss: 1.0636, Train: 1.0632, Val: 1.0755, Test: 1.0584\n",
      "Epoch: 122, Loss: 1.0632, Train: 1.0629, Val: 1.0752, Test: 1.0581\n",
      "Epoch: 123, Loss: 1.0629, Train: 1.0625, Val: 1.0749, Test: 1.0578\n",
      "Epoch: 124, Loss: 1.0625, Train: 1.0621, Val: 1.0745, Test: 1.0575\n",
      "Epoch: 125, Loss: 1.0621, Train: 1.0618, Val: 1.0742, Test: 1.0571\n",
      "Epoch: 126, Loss: 1.0618, Train: 1.0614, Val: 1.0738, Test: 1.0568\n",
      "Epoch: 127, Loss: 1.0614, Train: 1.0610, Val: 1.0735, Test: 1.0565\n",
      "Epoch: 128, Loss: 1.0610, Train: 1.0607, Val: 1.0732, Test: 1.0561\n",
      "Epoch: 129, Loss: 1.0607, Train: 1.0603, Val: 1.0728, Test: 1.0558\n",
      "Epoch: 130, Loss: 1.0603, Train: 1.0599, Val: 1.0725, Test: 1.0554\n",
      "Epoch: 131, Loss: 1.0599, Train: 1.0596, Val: 1.0722, Test: 1.0551\n",
      "Epoch: 132, Loss: 1.0596, Train: 1.0592, Val: 1.0718, Test: 1.0547\n",
      "Epoch: 133, Loss: 1.0592, Train: 1.0588, Val: 1.0715, Test: 1.0544\n",
      "Epoch: 134, Loss: 1.0588, Train: 1.0585, Val: 1.0712, Test: 1.0541\n",
      "Epoch: 135, Loss: 1.0585, Train: 1.0581, Val: 1.0708, Test: 1.0537\n",
      "Epoch: 136, Loss: 1.0581, Train: 1.0577, Val: 1.0705, Test: 1.0534\n",
      "Epoch: 137, Loss: 1.0577, Train: 1.0574, Val: 1.0702, Test: 1.0531\n",
      "Epoch: 138, Loss: 1.0574, Train: 1.0570, Val: 1.0699, Test: 1.0528\n",
      "Epoch: 139, Loss: 1.0570, Train: 1.0567, Val: 1.0695, Test: 1.0524\n",
      "Epoch: 140, Loss: 1.0567, Train: 1.0563, Val: 1.0692, Test: 1.0521\n",
      "Epoch: 141, Loss: 1.0563, Train: 1.0560, Val: 1.0689, Test: 1.0518\n",
      "Epoch: 142, Loss: 1.0560, Train: 1.0556, Val: 1.0686, Test: 1.0515\n",
      "Epoch: 143, Loss: 1.0556, Train: 1.0552, Val: 1.0682, Test: 1.0511\n",
      "Epoch: 144, Loss: 1.0552, Train: 1.0549, Val: 1.0679, Test: 1.0508\n",
      "Epoch: 145, Loss: 1.0549, Train: 1.0545, Val: 1.0676, Test: 1.0505\n",
      "Epoch: 146, Loss: 1.0545, Train: 1.0542, Val: 1.0673, Test: 1.0501\n",
      "Epoch: 147, Loss: 1.0542, Train: 1.0538, Val: 1.0670, Test: 1.0498\n",
      "Epoch: 148, Loss: 1.0538, Train: 1.0535, Val: 1.0666, Test: 1.0495\n",
      "Epoch: 149, Loss: 1.0535, Train: 1.0531, Val: 1.0663, Test: 1.0492\n",
      "Epoch: 150, Loss: 1.0531, Train: 1.0528, Val: 1.0660, Test: 1.0489\n",
      "Epoch: 151, Loss: 1.0528, Train: 1.0524, Val: 1.0657, Test: 1.0485\n",
      "Epoch: 152, Loss: 1.0524, Train: 1.0521, Val: 1.0654, Test: 1.0482\n",
      "Epoch: 153, Loss: 1.0521, Train: 1.0517, Val: 1.0651, Test: 1.0479\n",
      "Epoch: 154, Loss: 1.0517, Train: 1.0514, Val: 1.0647, Test: 1.0476\n",
      "Epoch: 155, Loss: 1.0514, Train: 1.0511, Val: 1.0644, Test: 1.0473\n",
      "Epoch: 156, Loss: 1.0511, Train: 1.0507, Val: 1.0641, Test: 1.0470\n",
      "Epoch: 157, Loss: 1.0507, Train: 1.0504, Val: 1.0638, Test: 1.0467\n",
      "Epoch: 158, Loss: 1.0504, Train: 1.0500, Val: 1.0635, Test: 1.0463\n",
      "Epoch: 159, Loss: 1.0500, Train: 1.0497, Val: 1.0632, Test: 1.0460\n",
      "Epoch: 160, Loss: 1.0497, Train: 1.0493, Val: 1.0629, Test: 1.0457\n",
      "Epoch: 161, Loss: 1.0493, Train: 1.0490, Val: 1.0626, Test: 1.0454\n",
      "Epoch: 162, Loss: 1.0490, Train: 1.0487, Val: 1.0623, Test: 1.0451\n",
      "Epoch: 163, Loss: 1.0487, Train: 1.0483, Val: 1.0620, Test: 1.0448\n",
      "Epoch: 164, Loss: 1.0483, Train: 1.0480, Val: 1.0617, Test: 1.0445\n",
      "Epoch: 165, Loss: 1.0480, Train: 1.0477, Val: 1.0614, Test: 1.0442\n",
      "Epoch: 166, Loss: 1.0477, Train: 1.0473, Val: 1.0610, Test: 1.0439\n",
      "Epoch: 167, Loss: 1.0473, Train: 1.0470, Val: 1.0607, Test: 1.0436\n",
      "Epoch: 168, Loss: 1.0470, Train: 1.0467, Val: 1.0604, Test: 1.0433\n",
      "Epoch: 169, Loss: 1.0467, Train: 1.0463, Val: 1.0601, Test: 1.0429\n",
      "Epoch: 170, Loss: 1.0463, Train: 1.0460, Val: 1.0598, Test: 1.0426\n",
      "Epoch: 171, Loss: 1.0460, Train: 1.0457, Val: 1.0595, Test: 1.0423\n",
      "Epoch: 172, Loss: 1.0457, Train: 1.0453, Val: 1.0592, Test: 1.0420\n",
      "Epoch: 173, Loss: 1.0453, Train: 1.0450, Val: 1.0589, Test: 1.0417\n",
      "Epoch: 174, Loss: 1.0450, Train: 1.0447, Val: 1.0587, Test: 1.0414\n",
      "Epoch: 175, Loss: 1.0447, Train: 1.0443, Val: 1.0584, Test: 1.0411\n",
      "Epoch: 176, Loss: 1.0443, Train: 1.0440, Val: 1.0581, Test: 1.0408\n",
      "Epoch: 177, Loss: 1.0440, Train: 1.0437, Val: 1.0578, Test: 1.0405\n",
      "Epoch: 178, Loss: 1.0437, Train: 1.0433, Val: 1.0575, Test: 1.0402\n",
      "Epoch: 179, Loss: 1.0433, Train: 1.0430, Val: 1.0572, Test: 1.0399\n",
      "Epoch: 180, Loss: 1.0430, Train: 1.0427, Val: 1.0569, Test: 1.0396\n",
      "Epoch: 181, Loss: 1.0427, Train: 1.0424, Val: 1.0566, Test: 1.0393\n",
      "Epoch: 182, Loss: 1.0424, Train: 1.0420, Val: 1.0563, Test: 1.0390\n",
      "Epoch: 183, Loss: 1.0420, Train: 1.0417, Val: 1.0560, Test: 1.0387\n",
      "Epoch: 184, Loss: 1.0417, Train: 1.0414, Val: 1.0557, Test: 1.0385\n",
      "Epoch: 185, Loss: 1.0414, Train: 1.0411, Val: 1.0554, Test: 1.0382\n",
      "Epoch: 186, Loss: 1.0411, Train: 1.0407, Val: 1.0551, Test: 1.0379\n",
      "Epoch: 187, Loss: 1.0407, Train: 1.0404, Val: 1.0548, Test: 1.0376\n",
      "Epoch: 188, Loss: 1.0404, Train: 1.0401, Val: 1.0546, Test: 1.0373\n",
      "Epoch: 189, Loss: 1.0401, Train: 1.0398, Val: 1.0543, Test: 1.0370\n",
      "Epoch: 190, Loss: 1.0398, Train: 1.0395, Val: 1.0540, Test: 1.0367\n",
      "Epoch: 191, Loss: 1.0395, Train: 1.0391, Val: 1.0537, Test: 1.0364\n",
      "Epoch: 192, Loss: 1.0391, Train: 1.0388, Val: 1.0534, Test: 1.0361\n",
      "Epoch: 193, Loss: 1.0388, Train: 1.0385, Val: 1.0531, Test: 1.0358\n",
      "Epoch: 194, Loss: 1.0385, Train: 1.0382, Val: 1.0529, Test: 1.0355\n",
      "Epoch: 195, Loss: 1.0382, Train: 1.0379, Val: 1.0526, Test: 1.0353\n",
      "Epoch: 196, Loss: 1.0379, Train: 1.0376, Val: 1.0523, Test: 1.0350\n",
      "Epoch: 197, Loss: 1.0376, Train: 1.0372, Val: 1.0520, Test: 1.0347\n",
      "Epoch: 198, Loss: 1.0372, Train: 1.0369, Val: 1.0517, Test: 1.0344\n",
      "Epoch: 199, Loss: 1.0369, Train: 1.0366, Val: 1.0514, Test: 1.0341\n",
      "Epoch: 200, Loss: 1.0366, Train: 1.0363, Val: 1.0512, Test: 1.0338\n",
      "Epoch: 201, Loss: 1.0363, Train: 1.0360, Val: 1.0509, Test: 1.0336\n",
      "Epoch: 202, Loss: 1.0360, Train: 1.0357, Val: 1.0506, Test: 1.0333\n",
      "Epoch: 203, Loss: 1.0357, Train: 1.0354, Val: 1.0503, Test: 1.0330\n",
      "Epoch: 204, Loss: 1.0354, Train: 1.0351, Val: 1.0501, Test: 1.0327\n",
      "Epoch: 205, Loss: 1.0351, Train: 1.0347, Val: 1.0498, Test: 1.0324\n",
      "Epoch: 206, Loss: 1.0347, Train: 1.0344, Val: 1.0495, Test: 1.0322\n",
      "Epoch: 207, Loss: 1.0344, Train: 1.0341, Val: 1.0492, Test: 1.0319\n",
      "Epoch: 208, Loss: 1.0341, Train: 1.0338, Val: 1.0490, Test: 1.0316\n",
      "Epoch: 209, Loss: 1.0338, Train: 1.0335, Val: 1.0487, Test: 1.0313\n",
      "Epoch: 210, Loss: 1.0335, Train: 1.0332, Val: 1.0484, Test: 1.0310\n",
      "Epoch: 211, Loss: 1.0332, Train: 1.0329, Val: 1.0481, Test: 1.0308\n",
      "Epoch: 212, Loss: 1.0329, Train: 1.0326, Val: 1.0479, Test: 1.0305\n",
      "Epoch: 213, Loss: 1.0326, Train: 1.0323, Val: 1.0476, Test: 1.0302\n",
      "Epoch: 214, Loss: 1.0323, Train: 1.0320, Val: 1.0473, Test: 1.0299\n",
      "Epoch: 215, Loss: 1.0320, Train: 1.0317, Val: 1.0471, Test: 1.0297\n",
      "Epoch: 216, Loss: 1.0317, Train: 1.0314, Val: 1.0468, Test: 1.0294\n",
      "Epoch: 217, Loss: 1.0314, Train: 1.0311, Val: 1.0465, Test: 1.0291\n",
      "Epoch: 218, Loss: 1.0311, Train: 1.0308, Val: 1.0463, Test: 1.0289\n",
      "Epoch: 219, Loss: 1.0308, Train: 1.0305, Val: 1.0460, Test: 1.0286\n",
      "Epoch: 220, Loss: 1.0305, Train: 1.0302, Val: 1.0457, Test: 1.0283\n",
      "Epoch: 221, Loss: 1.0302, Train: 1.0299, Val: 1.0455, Test: 1.0281\n",
      "Epoch: 222, Loss: 1.0299, Train: 1.0296, Val: 1.0452, Test: 1.0278\n",
      "Epoch: 223, Loss: 1.0296, Train: 1.0293, Val: 1.0450, Test: 1.0275\n",
      "Epoch: 224, Loss: 1.0293, Train: 1.0290, Val: 1.0447, Test: 1.0273\n",
      "Epoch: 225, Loss: 1.0290, Train: 1.0287, Val: 1.0444, Test: 1.0270\n",
      "Epoch: 226, Loss: 1.0287, Train: 1.0284, Val: 1.0442, Test: 1.0267\n",
      "Epoch: 227, Loss: 1.0284, Train: 1.0281, Val: 1.0439, Test: 1.0265\n",
      "Epoch: 228, Loss: 1.0281, Train: 1.0278, Val: 1.0436, Test: 1.0262\n",
      "Epoch: 229, Loss: 1.0278, Train: 1.0275, Val: 1.0434, Test: 1.0259\n",
      "Epoch: 230, Loss: 1.0275, Train: 1.0272, Val: 1.0431, Test: 1.0257\n",
      "Epoch: 231, Loss: 1.0272, Train: 1.0269, Val: 1.0429, Test: 1.0254\n",
      "Epoch: 232, Loss: 1.0269, Train: 1.0266, Val: 1.0426, Test: 1.0252\n",
      "Epoch: 233, Loss: 1.0266, Train: 1.0263, Val: 1.0424, Test: 1.0249\n",
      "Epoch: 234, Loss: 1.0263, Train: 1.0260, Val: 1.0421, Test: 1.0246\n",
      "Epoch: 235, Loss: 1.0260, Train: 1.0257, Val: 1.0419, Test: 1.0244\n",
      "Epoch: 236, Loss: 1.0257, Train: 1.0254, Val: 1.0416, Test: 1.0241\n",
      "Epoch: 237, Loss: 1.0254, Train: 1.0252, Val: 1.0414, Test: 1.0239\n",
      "Epoch: 238, Loss: 1.0252, Train: 1.0249, Val: 1.0411, Test: 1.0236\n",
      "Epoch: 239, Loss: 1.0249, Train: 1.0246, Val: 1.0409, Test: 1.0234\n",
      "Epoch: 240, Loss: 1.0246, Train: 1.0243, Val: 1.0406, Test: 1.0231\n",
      "Epoch: 241, Loss: 1.0243, Train: 1.0240, Val: 1.0404, Test: 1.0229\n",
      "Epoch: 242, Loss: 1.0240, Train: 1.0237, Val: 1.0401, Test: 1.0226\n",
      "Epoch: 243, Loss: 1.0237, Train: 1.0234, Val: 1.0399, Test: 1.0224\n",
      "Epoch: 244, Loss: 1.0234, Train: 1.0231, Val: 1.0396, Test: 1.0221\n",
      "Epoch: 245, Loss: 1.0231, Train: 1.0229, Val: 1.0394, Test: 1.0219\n",
      "Epoch: 246, Loss: 1.0229, Train: 1.0226, Val: 1.0391, Test: 1.0216\n",
      "Epoch: 247, Loss: 1.0226, Train: 1.0223, Val: 1.0389, Test: 1.0214\n",
      "Epoch: 248, Loss: 1.0223, Train: 1.0220, Val: 1.0386, Test: 1.0211\n",
      "Epoch: 249, Loss: 1.0220, Train: 1.0217, Val: 1.0384, Test: 1.0209\n",
      "Epoch: 250, Loss: 1.0217, Train: 1.0214, Val: 1.0381, Test: 1.0206\n",
      "Epoch: 251, Loss: 1.0214, Train: 1.0212, Val: 1.0379, Test: 1.0204\n",
      "Epoch: 252, Loss: 1.0212, Train: 1.0209, Val: 1.0377, Test: 1.0201\n",
      "Epoch: 253, Loss: 1.0209, Train: 1.0206, Val: 1.0374, Test: 1.0199\n",
      "Epoch: 254, Loss: 1.0206, Train: 1.0203, Val: 1.0372, Test: 1.0196\n",
      "Epoch: 255, Loss: 1.0203, Train: 1.0201, Val: 1.0369, Test: 1.0194\n",
      "Epoch: 256, Loss: 1.0201, Train: 1.0198, Val: 1.0367, Test: 1.0192\n",
      "Epoch: 257, Loss: 1.0198, Train: 1.0195, Val: 1.0364, Test: 1.0189\n",
      "Epoch: 258, Loss: 1.0195, Train: 1.0192, Val: 1.0362, Test: 1.0187\n",
      "Epoch: 259, Loss: 1.0192, Train: 1.0189, Val: 1.0360, Test: 1.0184\n",
      "Epoch: 260, Loss: 1.0189, Train: 1.0187, Val: 1.0357, Test: 1.0182\n",
      "Epoch: 261, Loss: 1.0187, Train: 1.0184, Val: 1.0355, Test: 1.0180\n",
      "Epoch: 262, Loss: 1.0184, Train: 1.0181, Val: 1.0353, Test: 1.0177\n",
      "Epoch: 263, Loss: 1.0181, Train: 1.0178, Val: 1.0350, Test: 1.0175\n",
      "Epoch: 264, Loss: 1.0178, Train: 1.0176, Val: 1.0348, Test: 1.0172\n",
      "Epoch: 265, Loss: 1.0176, Train: 1.0173, Val: 1.0345, Test: 1.0170\n",
      "Epoch: 266, Loss: 1.0173, Train: 1.0170, Val: 1.0343, Test: 1.0168\n",
      "Epoch: 267, Loss: 1.0170, Train: 1.0167, Val: 1.0341, Test: 1.0165\n",
      "Epoch: 268, Loss: 1.0167, Train: 1.0165, Val: 1.0338, Test: 1.0163\n",
      "Epoch: 269, Loss: 1.0165, Train: 1.0162, Val: 1.0336, Test: 1.0161\n",
      "Epoch: 270, Loss: 1.0162, Train: 1.0159, Val: 1.0334, Test: 1.0158\n",
      "Epoch: 271, Loss: 1.0159, Train: 1.0157, Val: 1.0331, Test: 1.0156\n",
      "Epoch: 272, Loss: 1.0157, Train: 1.0154, Val: 1.0329, Test: 1.0154\n",
      "Epoch: 273, Loss: 1.0154, Train: 1.0151, Val: 1.0327, Test: 1.0151\n",
      "Epoch: 274, Loss: 1.0151, Train: 1.0149, Val: 1.0325, Test: 1.0149\n",
      "Epoch: 275, Loss: 1.0149, Train: 1.0146, Val: 1.0322, Test: 1.0147\n",
      "Epoch: 276, Loss: 1.0146, Train: 1.0143, Val: 1.0320, Test: 1.0144\n",
      "Epoch: 277, Loss: 1.0143, Train: 1.0140, Val: 1.0318, Test: 1.0142\n",
      "Epoch: 278, Loss: 1.0140, Train: 1.0138, Val: 1.0315, Test: 1.0140\n",
      "Epoch: 279, Loss: 1.0138, Train: 1.0135, Val: 1.0313, Test: 1.0137\n",
      "Epoch: 280, Loss: 1.0135, Train: 1.0132, Val: 1.0311, Test: 1.0135\n",
      "Epoch: 281, Loss: 1.0132, Train: 1.0130, Val: 1.0308, Test: 1.0133\n",
      "Epoch: 282, Loss: 1.0130, Train: 1.0127, Val: 1.0306, Test: 1.0130\n",
      "Epoch: 283, Loss: 1.0127, Train: 1.0124, Val: 1.0304, Test: 1.0128\n",
      "Epoch: 284, Loss: 1.0124, Train: 1.0122, Val: 1.0302, Test: 1.0126\n",
      "Epoch: 285, Loss: 1.0122, Train: 1.0119, Val: 1.0299, Test: 1.0124\n",
      "Epoch: 286, Loss: 1.0119, Train: 1.0116, Val: 1.0297, Test: 1.0121\n",
      "Epoch: 287, Loss: 1.0116, Train: 1.0114, Val: 1.0295, Test: 1.0119\n",
      "Epoch: 288, Loss: 1.0114, Train: 1.0111, Val: 1.0293, Test: 1.0117\n",
      "Epoch: 289, Loss: 1.0111, Train: 1.0108, Val: 1.0290, Test: 1.0114\n",
      "Epoch: 290, Loss: 1.0108, Train: 1.0106, Val: 1.0288, Test: 1.0112\n",
      "Epoch: 291, Loss: 1.0106, Train: 1.0103, Val: 1.0286, Test: 1.0110\n",
      "Epoch: 292, Loss: 1.0103, Train: 1.0100, Val: 1.0284, Test: 1.0108\n",
      "Epoch: 293, Loss: 1.0100, Train: 1.0098, Val: 1.0281, Test: 1.0105\n",
      "Epoch: 294, Loss: 1.0098, Train: 1.0095, Val: 1.0279, Test: 1.0103\n",
      "Epoch: 295, Loss: 1.0095, Train: 1.0092, Val: 1.0277, Test: 1.0101\n",
      "Epoch: 296, Loss: 1.0092, Train: 1.0090, Val: 1.0275, Test: 1.0099\n",
      "Epoch: 297, Loss: 1.0090, Train: 1.0087, Val: 1.0272, Test: 1.0096\n",
      "Epoch: 298, Loss: 1.0087, Train: 1.0084, Val: 1.0270, Test: 1.0094\n",
      "Epoch: 299, Loss: 1.0084, Train: 1.0082, Val: 1.0268, Test: 1.0092\n",
      "Epoch: 300, Loss: 1.0082, Train: 1.0079, Val: 1.0266, Test: 1.0090\n",
      "Epoch: 301, Loss: 1.0079, Train: 1.0077, Val: 1.0263, Test: 1.0087\n",
      "Epoch: 302, Loss: 1.0077, Train: 1.0074, Val: 1.0261, Test: 1.0085\n",
      "Epoch: 303, Loss: 1.0074, Train: 1.0071, Val: 1.0259, Test: 1.0083\n",
      "Epoch: 304, Loss: 1.0071, Train: 1.0069, Val: 1.0257, Test: 1.0080\n",
      "Epoch: 305, Loss: 1.0069, Train: 1.0066, Val: 1.0254, Test: 1.0078\n",
      "Epoch: 306, Loss: 1.0066, Train: 1.0063, Val: 1.0252, Test: 1.0076\n",
      "Epoch: 307, Loss: 1.0063, Train: 1.0061, Val: 1.0250, Test: 1.0074\n",
      "Epoch: 308, Loss: 1.0061, Train: 1.0058, Val: 1.0248, Test: 1.0071\n",
      "Epoch: 309, Loss: 1.0058, Train: 1.0055, Val: 1.0245, Test: 1.0069\n",
      "Epoch: 310, Loss: 1.0055, Train: 1.0053, Val: 1.0243, Test: 1.0067\n",
      "Epoch: 311, Loss: 1.0053, Train: 1.0050, Val: 1.0241, Test: 1.0065\n",
      "Epoch: 312, Loss: 1.0050, Train: 1.0047, Val: 1.0239, Test: 1.0063\n",
      "Epoch: 313, Loss: 1.0047, Train: 1.0045, Val: 1.0236, Test: 1.0060\n",
      "Epoch: 314, Loss: 1.0045, Train: 1.0042, Val: 1.0234, Test: 1.0058\n",
      "Epoch: 315, Loss: 1.0042, Train: 1.0039, Val: 1.0232, Test: 1.0056\n",
      "Epoch: 316, Loss: 1.0039, Train: 1.0037, Val: 1.0230, Test: 1.0054\n",
      "Epoch: 317, Loss: 1.0037, Train: 1.0034, Val: 1.0227, Test: 1.0051\n",
      "Epoch: 318, Loss: 1.0034, Train: 1.0031, Val: 1.0225, Test: 1.0049\n",
      "Epoch: 319, Loss: 1.0031, Train: 1.0029, Val: 1.0223, Test: 1.0047\n",
      "Epoch: 320, Loss: 1.0029, Train: 1.0026, Val: 1.0221, Test: 1.0045\n",
      "Epoch: 321, Loss: 1.0026, Train: 1.0024, Val: 1.0218, Test: 1.0042\n",
      "Epoch: 322, Loss: 1.0024, Train: 1.0021, Val: 1.0216, Test: 1.0040\n",
      "Epoch: 323, Loss: 1.0021, Train: 1.0018, Val: 1.0214, Test: 1.0038\n",
      "Epoch: 324, Loss: 1.0018, Train: 1.0016, Val: 1.0212, Test: 1.0036\n",
      "Epoch: 325, Loss: 1.0016, Train: 1.0013, Val: 1.0209, Test: 1.0033\n",
      "Epoch: 326, Loss: 1.0013, Train: 1.0010, Val: 1.0207, Test: 1.0031\n",
      "Epoch: 327, Loss: 1.0010, Train: 1.0008, Val: 1.0205, Test: 1.0029\n",
      "Epoch: 328, Loss: 1.0008, Train: 1.0005, Val: 1.0203, Test: 1.0027\n",
      "Epoch: 329, Loss: 1.0005, Train: 1.0002, Val: 1.0200, Test: 1.0024\n",
      "Epoch: 330, Loss: 1.0002, Train: 1.0000, Val: 1.0198, Test: 1.0022\n",
      "Epoch: 331, Loss: 1.0000, Train: 0.9997, Val: 1.0196, Test: 1.0020\n",
      "Epoch: 332, Loss: 0.9997, Train: 0.9994, Val: 1.0193, Test: 1.0018\n",
      "Epoch: 333, Loss: 0.9994, Train: 0.9991, Val: 1.0191, Test: 1.0015\n",
      "Epoch: 334, Loss: 0.9991, Train: 0.9989, Val: 1.0189, Test: 1.0013\n",
      "Epoch: 335, Loss: 0.9989, Train: 0.9986, Val: 1.0187, Test: 1.0011\n",
      "Epoch: 336, Loss: 0.9986, Train: 0.9983, Val: 1.0184, Test: 1.0009\n",
      "Epoch: 337, Loss: 0.9983, Train: 0.9981, Val: 1.0182, Test: 1.0006\n",
      "Epoch: 338, Loss: 0.9981, Train: 0.9978, Val: 1.0180, Test: 1.0004\n",
      "Epoch: 339, Loss: 0.9978, Train: 0.9975, Val: 1.0178, Test: 1.0002\n",
      "Epoch: 340, Loss: 0.9975, Train: 0.9973, Val: 1.0175, Test: 1.0000\n",
      "Epoch: 341, Loss: 0.9973, Train: 0.9970, Val: 1.0173, Test: 0.9997\n",
      "Epoch: 342, Loss: 0.9970, Train: 0.9967, Val: 1.0171, Test: 0.9995\n",
      "Epoch: 343, Loss: 0.9967, Train: 0.9965, Val: 1.0168, Test: 0.9993\n",
      "Epoch: 344, Loss: 0.9965, Train: 0.9962, Val: 1.0166, Test: 0.9991\n",
      "Epoch: 345, Loss: 0.9962, Train: 0.9959, Val: 1.0164, Test: 0.9988\n",
      "Epoch: 346, Loss: 0.9959, Train: 0.9957, Val: 1.0162, Test: 0.9986\n",
      "Epoch: 347, Loss: 0.9957, Train: 0.9954, Val: 1.0159, Test: 0.9984\n",
      "Epoch: 348, Loss: 0.9954, Train: 0.9951, Val: 1.0157, Test: 0.9982\n",
      "Epoch: 349, Loss: 0.9951, Train: 0.9948, Val: 1.0155, Test: 0.9979\n",
      "Epoch: 350, Loss: 0.9948, Train: 0.9946, Val: 1.0152, Test: 0.9977\n",
      "Epoch: 351, Loss: 0.9946, Train: 0.9943, Val: 1.0150, Test: 0.9975\n",
      "Epoch: 352, Loss: 0.9943, Train: 0.9940, Val: 1.0148, Test: 0.9973\n",
      "Epoch: 353, Loss: 0.9940, Train: 0.9938, Val: 1.0145, Test: 0.9970\n",
      "Epoch: 354, Loss: 0.9938, Train: 0.9935, Val: 1.0143, Test: 0.9968\n",
      "Epoch: 355, Loss: 0.9935, Train: 0.9932, Val: 1.0141, Test: 0.9966\n",
      "Epoch: 356, Loss: 0.9932, Train: 0.9929, Val: 1.0139, Test: 0.9964\n",
      "Epoch: 357, Loss: 0.9929, Train: 0.9927, Val: 1.0136, Test: 0.9961\n",
      "Epoch: 358, Loss: 0.9927, Train: 0.9924, Val: 1.0134, Test: 0.9959\n",
      "Epoch: 359, Loss: 0.9924, Train: 0.9921, Val: 1.0132, Test: 0.9957\n",
      "Epoch: 360, Loss: 0.9921, Train: 0.9918, Val: 1.0129, Test: 0.9955\n",
      "Epoch: 361, Loss: 0.9918, Train: 0.9916, Val: 1.0127, Test: 0.9952\n",
      "Epoch: 362, Loss: 0.9916, Train: 0.9913, Val: 1.0125, Test: 0.9950\n",
      "Epoch: 363, Loss: 0.9913, Train: 0.9910, Val: 1.0123, Test: 0.9948\n",
      "Epoch: 364, Loss: 0.9910, Train: 0.9908, Val: 1.0120, Test: 0.9946\n",
      "Epoch: 365, Loss: 0.9908, Train: 0.9905, Val: 1.0118, Test: 0.9943\n",
      "Epoch: 366, Loss: 0.9905, Train: 0.9902, Val: 1.0116, Test: 0.9941\n",
      "Epoch: 367, Loss: 0.9902, Train: 0.9900, Val: 1.0113, Test: 0.9939\n",
      "Epoch: 368, Loss: 0.9900, Train: 0.9897, Val: 1.0111, Test: 0.9937\n",
      "Epoch: 369, Loss: 0.9897, Train: 0.9894, Val: 1.0109, Test: 0.9935\n",
      "Epoch: 370, Loss: 0.9894, Train: 0.9892, Val: 1.0107, Test: 0.9932\n",
      "Epoch: 371, Loss: 0.9892, Train: 0.9889, Val: 1.0104, Test: 0.9930\n",
      "Epoch: 372, Loss: 0.9889, Train: 0.9886, Val: 1.0102, Test: 0.9928\n",
      "Epoch: 373, Loss: 0.9886, Train: 0.9883, Val: 1.0100, Test: 0.9926\n",
      "Epoch: 374, Loss: 0.9883, Train: 0.9881, Val: 1.0097, Test: 0.9923\n",
      "Epoch: 375, Loss: 0.9881, Train: 0.9878, Val: 1.0095, Test: 0.9921\n",
      "Epoch: 376, Loss: 0.9878, Train: 0.9875, Val: 1.0093, Test: 0.9919\n",
      "Epoch: 377, Loss: 0.9875, Train: 0.9873, Val: 1.0090, Test: 0.9917\n",
      "Epoch: 378, Loss: 0.9873, Train: 0.9870, Val: 1.0088, Test: 0.9914\n",
      "Epoch: 379, Loss: 0.9870, Train: 0.9867, Val: 1.0086, Test: 0.9912\n",
      "Epoch: 380, Loss: 0.9867, Train: 0.9864, Val: 1.0083, Test: 0.9910\n",
      "Epoch: 381, Loss: 0.9864, Train: 0.9862, Val: 1.0081, Test: 0.9908\n",
      "Epoch: 382, Loss: 0.9862, Train: 0.9859, Val: 1.0079, Test: 0.9905\n",
      "Epoch: 383, Loss: 0.9859, Train: 0.9856, Val: 1.0076, Test: 0.9903\n",
      "Epoch: 384, Loss: 0.9856, Train: 0.9854, Val: 1.0074, Test: 0.9901\n",
      "Epoch: 385, Loss: 0.9854, Train: 0.9851, Val: 1.0072, Test: 0.9899\n",
      "Epoch: 386, Loss: 0.9851, Train: 0.9848, Val: 1.0069, Test: 0.9896\n",
      "Epoch: 387, Loss: 0.9848, Train: 0.9845, Val: 1.0067, Test: 0.9894\n",
      "Epoch: 388, Loss: 0.9845, Train: 0.9843, Val: 1.0065, Test: 0.9892\n",
      "Epoch: 389, Loss: 0.9843, Train: 0.9840, Val: 1.0062, Test: 0.9889\n",
      "Epoch: 390, Loss: 0.9840, Train: 0.9837, Val: 1.0060, Test: 0.9887\n",
      "Epoch: 391, Loss: 0.9837, Train: 0.9834, Val: 1.0058, Test: 0.9885\n",
      "Epoch: 392, Loss: 0.9834, Train: 0.9832, Val: 1.0055, Test: 0.9883\n",
      "Epoch: 393, Loss: 0.9832, Train: 0.9829, Val: 1.0053, Test: 0.9880\n",
      "Epoch: 394, Loss: 0.9829, Train: 0.9826, Val: 1.0051, Test: 0.9878\n",
      "Epoch: 395, Loss: 0.9826, Train: 0.9823, Val: 1.0048, Test: 0.9876\n",
      "Epoch: 396, Loss: 0.9823, Train: 0.9821, Val: 1.0046, Test: 0.9874\n",
      "Epoch: 397, Loss: 0.9821, Train: 0.9818, Val: 1.0044, Test: 0.9871\n",
      "Epoch: 398, Loss: 0.9818, Train: 0.9815, Val: 1.0041, Test: 0.9869\n",
      "Epoch: 399, Loss: 0.9815, Train: 0.9812, Val: 1.0039, Test: 0.9867\n",
      "Epoch: 400, Loss: 0.9812, Train: 0.9810, Val: 1.0037, Test: 0.9865\n",
      "Epoch: 401, Loss: 0.9810, Train: 0.9807, Val: 1.0034, Test: 0.9862\n",
      "Epoch: 402, Loss: 0.9807, Train: 0.9804, Val: 1.0032, Test: 0.9860\n",
      "Epoch: 403, Loss: 0.9804, Train: 0.9801, Val: 1.0030, Test: 0.9858\n",
      "Epoch: 404, Loss: 0.9801, Train: 0.9799, Val: 1.0027, Test: 0.9855\n",
      "Epoch: 405, Loss: 0.9799, Train: 0.9796, Val: 1.0025, Test: 0.9853\n",
      "Epoch: 406, Loss: 0.9796, Train: 0.9793, Val: 1.0023, Test: 0.9851\n",
      "Epoch: 407, Loss: 0.9793, Train: 0.9790, Val: 1.0020, Test: 0.9849\n",
      "Epoch: 408, Loss: 0.9790, Train: 0.9788, Val: 1.0018, Test: 0.9846\n",
      "Epoch: 409, Loss: 0.9788, Train: 0.9785, Val: 1.0016, Test: 0.9844\n",
      "Epoch: 410, Loss: 0.9785, Train: 0.9782, Val: 1.0013, Test: 0.9842\n",
      "Epoch: 411, Loss: 0.9782, Train: 0.9779, Val: 1.0011, Test: 0.9840\n",
      "Epoch: 412, Loss: 0.9779, Train: 0.9777, Val: 1.0009, Test: 0.9837\n",
      "Epoch: 413, Loss: 0.9777, Train: 0.9774, Val: 1.0006, Test: 0.9835\n",
      "Epoch: 414, Loss: 0.9774, Train: 0.9771, Val: 1.0004, Test: 0.9833\n",
      "Epoch: 415, Loss: 0.9771, Train: 0.9768, Val: 1.0002, Test: 0.9830\n",
      "Epoch: 416, Loss: 0.9768, Train: 0.9766, Val: 0.9999, Test: 0.9828\n",
      "Epoch: 417, Loss: 0.9766, Train: 0.9763, Val: 0.9997, Test: 0.9826\n",
      "Epoch: 418, Loss: 0.9763, Train: 0.9760, Val: 0.9995, Test: 0.9824\n",
      "Epoch: 419, Loss: 0.9760, Train: 0.9757, Val: 0.9992, Test: 0.9821\n",
      "Epoch: 420, Loss: 0.9757, Train: 0.9754, Val: 0.9990, Test: 0.9819\n",
      "Epoch: 421, Loss: 0.9754, Train: 0.9752, Val: 0.9987, Test: 0.9817\n",
      "Epoch: 422, Loss: 0.9752, Train: 0.9749, Val: 0.9985, Test: 0.9815\n",
      "Epoch: 423, Loss: 0.9749, Train: 0.9746, Val: 0.9983, Test: 0.9812\n",
      "Epoch: 424, Loss: 0.9746, Train: 0.9743, Val: 0.9980, Test: 0.9810\n",
      "Epoch: 425, Loss: 0.9743, Train: 0.9741, Val: 0.9978, Test: 0.9808\n",
      "Epoch: 426, Loss: 0.9741, Train: 0.9738, Val: 0.9976, Test: 0.9806\n",
      "Epoch: 427, Loss: 0.9738, Train: 0.9735, Val: 0.9973, Test: 0.9803\n",
      "Epoch: 428, Loss: 0.9735, Train: 0.9732, Val: 0.9971, Test: 0.9801\n",
      "Epoch: 429, Loss: 0.9732, Train: 0.9730, Val: 0.9969, Test: 0.9799\n",
      "Epoch: 430, Loss: 0.9730, Train: 0.9727, Val: 0.9966, Test: 0.9797\n",
      "Epoch: 431, Loss: 0.9727, Train: 0.9724, Val: 0.9964, Test: 0.9794\n",
      "Epoch: 432, Loss: 0.9724, Train: 0.9721, Val: 0.9962, Test: 0.9792\n",
      "Epoch: 433, Loss: 0.9721, Train: 0.9719, Val: 0.9959, Test: 0.9790\n",
      "Epoch: 434, Loss: 0.9719, Train: 0.9716, Val: 0.9957, Test: 0.9788\n",
      "Epoch: 435, Loss: 0.9716, Train: 0.9713, Val: 0.9955, Test: 0.9785\n",
      "Epoch: 436, Loss: 0.9713, Train: 0.9710, Val: 0.9952, Test: 0.9783\n",
      "Epoch: 437, Loss: 0.9710, Train: 0.9708, Val: 0.9950, Test: 0.9781\n",
      "Epoch: 438, Loss: 0.9708, Train: 0.9705, Val: 0.9948, Test: 0.9779\n",
      "Epoch: 439, Loss: 0.9705, Train: 0.9702, Val: 0.9945, Test: 0.9776\n",
      "Epoch: 440, Loss: 0.9702, Train: 0.9699, Val: 0.9943, Test: 0.9774\n",
      "Epoch: 441, Loss: 0.9699, Train: 0.9697, Val: 0.9941, Test: 0.9772\n",
      "Epoch: 442, Loss: 0.9697, Train: 0.9694, Val: 0.9938, Test: 0.9770\n",
      "Epoch: 443, Loss: 0.9694, Train: 0.9691, Val: 0.9936, Test: 0.9767\n",
      "Epoch: 444, Loss: 0.9691, Train: 0.9688, Val: 0.9934, Test: 0.9765\n",
      "Epoch: 445, Loss: 0.9688, Train: 0.9686, Val: 0.9931, Test: 0.9763\n",
      "Epoch: 446, Loss: 0.9686, Train: 0.9683, Val: 0.9929, Test: 0.9761\n",
      "Epoch: 447, Loss: 0.9683, Train: 0.9680, Val: 0.9927, Test: 0.9759\n",
      "Epoch: 448, Loss: 0.9680, Train: 0.9677, Val: 0.9924, Test: 0.9756\n",
      "Epoch: 449, Loss: 0.9677, Train: 0.9675, Val: 0.9922, Test: 0.9754\n",
      "Epoch: 450, Loss: 0.9675, Train: 0.9672, Val: 0.9920, Test: 0.9752\n",
      "Epoch: 451, Loss: 0.9672, Train: 0.9669, Val: 0.9918, Test: 0.9750\n",
      "Epoch: 452, Loss: 0.9669, Train: 0.9667, Val: 0.9915, Test: 0.9747\n",
      "Epoch: 453, Loss: 0.9667, Train: 0.9664, Val: 0.9913, Test: 0.9745\n",
      "Epoch: 454, Loss: 0.9664, Train: 0.9661, Val: 0.9911, Test: 0.9743\n",
      "Epoch: 455, Loss: 0.9661, Train: 0.9658, Val: 0.9908, Test: 0.9741\n",
      "Epoch: 456, Loss: 0.9658, Train: 0.9656, Val: 0.9906, Test: 0.9739\n",
      "Epoch: 457, Loss: 0.9656, Train: 0.9653, Val: 0.9904, Test: 0.9737\n",
      "Epoch: 458, Loss: 0.9653, Train: 0.9650, Val: 0.9901, Test: 0.9734\n",
      "Epoch: 459, Loss: 0.9650, Train: 0.9648, Val: 0.9899, Test: 0.9732\n",
      "Epoch: 460, Loss: 0.9648, Train: 0.9645, Val: 0.9897, Test: 0.9730\n",
      "Epoch: 461, Loss: 0.9645, Train: 0.9642, Val: 0.9895, Test: 0.9728\n",
      "Epoch: 462, Loss: 0.9642, Train: 0.9640, Val: 0.9892, Test: 0.9726\n",
      "Epoch: 463, Loss: 0.9640, Train: 0.9637, Val: 0.9890, Test: 0.9723\n",
      "Epoch: 464, Loss: 0.9637, Train: 0.9634, Val: 0.9888, Test: 0.9721\n",
      "Epoch: 465, Loss: 0.9634, Train: 0.9632, Val: 0.9886, Test: 0.9719\n",
      "Epoch: 466, Loss: 0.9632, Train: 0.9629, Val: 0.9883, Test: 0.9717\n",
      "Epoch: 467, Loss: 0.9629, Train: 0.9626, Val: 0.9881, Test: 0.9715\n",
      "Epoch: 468, Loss: 0.9626, Train: 0.9624, Val: 0.9879, Test: 0.9713\n",
      "Epoch: 469, Loss: 0.9624, Train: 0.9621, Val: 0.9877, Test: 0.9711\n",
      "Epoch: 470, Loss: 0.9621, Train: 0.9618, Val: 0.9874, Test: 0.9709\n",
      "Epoch: 471, Loss: 0.9618, Train: 0.9616, Val: 0.9872, Test: 0.9706\n",
      "Epoch: 472, Loss: 0.9616, Train: 0.9613, Val: 0.9870, Test: 0.9704\n",
      "Epoch: 473, Loss: 0.9613, Train: 0.9610, Val: 0.9868, Test: 0.9702\n",
      "Epoch: 474, Loss: 0.9610, Train: 0.9608, Val: 0.9866, Test: 0.9700\n",
      "Epoch: 475, Loss: 0.9608, Train: 0.9605, Val: 0.9863, Test: 0.9698\n",
      "Epoch: 476, Loss: 0.9605, Train: 0.9602, Val: 0.9861, Test: 0.9696\n",
      "Epoch: 477, Loss: 0.9602, Train: 0.9600, Val: 0.9859, Test: 0.9694\n",
      "Epoch: 478, Loss: 0.9600, Train: 0.9597, Val: 0.9857, Test: 0.9692\n",
      "Epoch: 479, Loss: 0.9597, Train: 0.9595, Val: 0.9855, Test: 0.9690\n",
      "Epoch: 480, Loss: 0.9595, Train: 0.9592, Val: 0.9852, Test: 0.9688\n",
      "Epoch: 481, Loss: 0.9592, Train: 0.9589, Val: 0.9850, Test: 0.9686\n",
      "Epoch: 482, Loss: 0.9589, Train: 0.9587, Val: 0.9848, Test: 0.9683\n",
      "Epoch: 483, Loss: 0.9587, Train: 0.9584, Val: 0.9846, Test: 0.9681\n",
      "Epoch: 484, Loss: 0.9584, Train: 0.9582, Val: 0.9844, Test: 0.9679\n",
      "Epoch: 485, Loss: 0.9582, Train: 0.9579, Val: 0.9842, Test: 0.9677\n",
      "Epoch: 486, Loss: 0.9579, Train: 0.9577, Val: 0.9839, Test: 0.9675\n",
      "Epoch: 487, Loss: 0.9577, Train: 0.9574, Val: 0.9837, Test: 0.9673\n",
      "Epoch: 488, Loss: 0.9574, Train: 0.9572, Val: 0.9835, Test: 0.9671\n",
      "Epoch: 489, Loss: 0.9572, Train: 0.9569, Val: 0.9833, Test: 0.9669\n",
      "Epoch: 490, Loss: 0.9569, Train: 0.9566, Val: 0.9831, Test: 0.9667\n",
      "Epoch: 491, Loss: 0.9566, Train: 0.9564, Val: 0.9829, Test: 0.9665\n",
      "Epoch: 492, Loss: 0.9564, Train: 0.9561, Val: 0.9827, Test: 0.9663\n",
      "Epoch: 493, Loss: 0.9561, Train: 0.9559, Val: 0.9825, Test: 0.9661\n",
      "Epoch: 494, Loss: 0.9559, Train: 0.9556, Val: 0.9823, Test: 0.9659\n",
      "Epoch: 495, Loss: 0.9556, Train: 0.9554, Val: 0.9821, Test: 0.9657\n",
      "Epoch: 496, Loss: 0.9554, Train: 0.9551, Val: 0.9819, Test: 0.9655\n",
      "Epoch: 497, Loss: 0.9551, Train: 0.9549, Val: 0.9816, Test: 0.9653\n",
      "Epoch: 498, Loss: 0.9549, Train: 0.9547, Val: 0.9814, Test: 0.9651\n",
      "Epoch: 499, Loss: 0.9547, Train: 0.9544, Val: 0.9812, Test: 0.9650\n",
      "Epoch: 500, Loss: 0.9544, Train: 0.9542, Val: 0.9810, Test: 0.9648\n",
      "Epoch: 501, Loss: 0.9542, Train: 0.9539, Val: 0.9808, Test: 0.9646\n",
      "Epoch: 502, Loss: 0.9539, Train: 0.9537, Val: 0.9806, Test: 0.9644\n",
      "Epoch: 503, Loss: 0.9537, Train: 0.9534, Val: 0.9804, Test: 0.9642\n",
      "Epoch: 504, Loss: 0.9534, Train: 0.9532, Val: 0.9802, Test: 0.9640\n",
      "Epoch: 505, Loss: 0.9532, Train: 0.9530, Val: 0.9800, Test: 0.9638\n",
      "Epoch: 506, Loss: 0.9530, Train: 0.9527, Val: 0.9798, Test: 0.9636\n",
      "Epoch: 507, Loss: 0.9527, Train: 0.9525, Val: 0.9796, Test: 0.9634\n",
      "Epoch: 508, Loss: 0.9525, Train: 0.9522, Val: 0.9794, Test: 0.9632\n",
      "Epoch: 509, Loss: 0.9522, Train: 0.9520, Val: 0.9792, Test: 0.9631\n",
      "Epoch: 510, Loss: 0.9520, Train: 0.9518, Val: 0.9791, Test: 0.9629\n",
      "Epoch: 511, Loss: 0.9518, Train: 0.9515, Val: 0.9789, Test: 0.9627\n",
      "Epoch: 512, Loss: 0.9515, Train: 0.9513, Val: 0.9787, Test: 0.9625\n",
      "Epoch: 513, Loss: 0.9513, Train: 0.9511, Val: 0.9785, Test: 0.9623\n",
      "Epoch: 514, Loss: 0.9511, Train: 0.9508, Val: 0.9783, Test: 0.9621\n",
      "Epoch: 515, Loss: 0.9508, Train: 0.9506, Val: 0.9781, Test: 0.9620\n",
      "Epoch: 516, Loss: 0.9506, Train: 0.9504, Val: 0.9779, Test: 0.9618\n",
      "Epoch: 517, Loss: 0.9504, Train: 0.9501, Val: 0.9777, Test: 0.9616\n",
      "Epoch: 518, Loss: 0.9501, Train: 0.9499, Val: 0.9775, Test: 0.9614\n",
      "Epoch: 519, Loss: 0.9499, Train: 0.9497, Val: 0.9774, Test: 0.9613\n",
      "Epoch: 520, Loss: 0.9497, Train: 0.9495, Val: 0.9772, Test: 0.9611\n",
      "Epoch: 521, Loss: 0.9495, Train: 0.9492, Val: 0.9770, Test: 0.9609\n",
      "Epoch: 522, Loss: 0.9492, Train: 0.9490, Val: 0.9768, Test: 0.9607\n",
      "Epoch: 523, Loss: 0.9490, Train: 0.9488, Val: 0.9766, Test: 0.9606\n",
      "Epoch: 524, Loss: 0.9488, Train: 0.9486, Val: 0.9764, Test: 0.9604\n",
      "Epoch: 525, Loss: 0.9486, Train: 0.9484, Val: 0.9763, Test: 0.9602\n",
      "Epoch: 526, Loss: 0.9484, Train: 0.9481, Val: 0.9761, Test: 0.9601\n",
      "Epoch: 527, Loss: 0.9481, Train: 0.9479, Val: 0.9759, Test: 0.9599\n",
      "Epoch: 528, Loss: 0.9479, Train: 0.9477, Val: 0.9757, Test: 0.9597\n",
      "Epoch: 529, Loss: 0.9477, Train: 0.9475, Val: 0.9756, Test: 0.9596\n",
      "Epoch: 530, Loss: 0.9475, Train: 0.9473, Val: 0.9754, Test: 0.9594\n",
      "Epoch: 531, Loss: 0.9473, Train: 0.9471, Val: 0.9752, Test: 0.9592\n",
      "Epoch: 532, Loss: 0.9471, Train: 0.9469, Val: 0.9751, Test: 0.9591\n",
      "Epoch: 533, Loss: 0.9469, Train: 0.9467, Val: 0.9749, Test: 0.9589\n",
      "Epoch: 534, Loss: 0.9467, Train: 0.9465, Val: 0.9747, Test: 0.9588\n",
      "Epoch: 535, Loss: 0.9465, Train: 0.9462, Val: 0.9746, Test: 0.9586\n",
      "Epoch: 536, Loss: 0.9462, Train: 0.9460, Val: 0.9744, Test: 0.9584\n",
      "Epoch: 537, Loss: 0.9460, Train: 0.9458, Val: 0.9742, Test: 0.9583\n",
      "Epoch: 538, Loss: 0.9458, Train: 0.9456, Val: 0.9741, Test: 0.9581\n",
      "Epoch: 539, Loss: 0.9456, Train: 0.9454, Val: 0.9739, Test: 0.9580\n",
      "Epoch: 540, Loss: 0.9454, Train: 0.9452, Val: 0.9737, Test: 0.9578\n",
      "Epoch: 541, Loss: 0.9452, Train: 0.9450, Val: 0.9736, Test: 0.9577\n",
      "Epoch: 542, Loss: 0.9450, Train: 0.9449, Val: 0.9734, Test: 0.9575\n",
      "Epoch: 543, Loss: 0.9449, Train: 0.9447, Val: 0.9733, Test: 0.9574\n",
      "Epoch: 544, Loss: 0.9447, Train: 0.9445, Val: 0.9731, Test: 0.9572\n",
      "Epoch: 545, Loss: 0.9445, Train: 0.9443, Val: 0.9730, Test: 0.9571\n",
      "Epoch: 546, Loss: 0.9443, Train: 0.9441, Val: 0.9728, Test: 0.9570\n",
      "Epoch: 547, Loss: 0.9441, Train: 0.9439, Val: 0.9727, Test: 0.9568\n",
      "Epoch: 548, Loss: 0.9439, Train: 0.9437, Val: 0.9725, Test: 0.9567\n",
      "Epoch: 549, Loss: 0.9437, Train: 0.9435, Val: 0.9724, Test: 0.9565\n",
      "Epoch: 550, Loss: 0.9435, Train: 0.9433, Val: 0.9722, Test: 0.9564\n",
      "Epoch: 551, Loss: 0.9433, Train: 0.9431, Val: 0.9721, Test: 0.9563\n",
      "Epoch: 552, Loss: 0.9431, Train: 0.9430, Val: 0.9719, Test: 0.9561\n",
      "Epoch: 553, Loss: 0.9430, Train: 0.9428, Val: 0.9718, Test: 0.9560\n",
      "Epoch: 554, Loss: 0.9428, Train: 0.9426, Val: 0.9716, Test: 0.9558\n",
      "Epoch: 555, Loss: 0.9426, Train: 0.9424, Val: 0.9715, Test: 0.9557\n",
      "Epoch: 556, Loss: 0.9424, Train: 0.9422, Val: 0.9714, Test: 0.9556\n",
      "Epoch: 557, Loss: 0.9422, Train: 0.9421, Val: 0.9712, Test: 0.9554\n",
      "Epoch: 558, Loss: 0.9421, Train: 0.9419, Val: 0.9711, Test: 0.9553\n",
      "Epoch: 559, Loss: 0.9419, Train: 0.9417, Val: 0.9709, Test: 0.9552\n",
      "Epoch: 560, Loss: 0.9417, Train: 0.9415, Val: 0.9708, Test: 0.9551\n",
      "Epoch: 561, Loss: 0.9415, Train: 0.9414, Val: 0.9707, Test: 0.9549\n",
      "Epoch: 562, Loss: 0.9414, Train: 0.9412, Val: 0.9705, Test: 0.9548\n",
      "Epoch: 563, Loss: 0.9412, Train: 0.9410, Val: 0.9704, Test: 0.9547\n",
      "Epoch: 564, Loss: 0.9410, Train: 0.9409, Val: 0.9703, Test: 0.9546\n",
      "Epoch: 565, Loss: 0.9409, Train: 0.9407, Val: 0.9701, Test: 0.9544\n",
      "Epoch: 566, Loss: 0.9407, Train: 0.9405, Val: 0.9700, Test: 0.9543\n",
      "Epoch: 567, Loss: 0.9405, Train: 0.9404, Val: 0.9699, Test: 0.9542\n",
      "Epoch: 568, Loss: 0.9404, Train: 0.9402, Val: 0.9698, Test: 0.9541\n",
      "Epoch: 569, Loss: 0.9402, Train: 0.9401, Val: 0.9696, Test: 0.9540\n",
      "Epoch: 570, Loss: 0.9401, Train: 0.9399, Val: 0.9695, Test: 0.9538\n",
      "Epoch: 571, Loss: 0.9399, Train: 0.9397, Val: 0.9694, Test: 0.9537\n",
      "Epoch: 572, Loss: 0.9397, Train: 0.9396, Val: 0.9693, Test: 0.9536\n",
      "Epoch: 573, Loss: 0.9396, Train: 0.9394, Val: 0.9691, Test: 0.9535\n",
      "Epoch: 574, Loss: 0.9394, Train: 0.9393, Val: 0.9690, Test: 0.9534\n",
      "Epoch: 575, Loss: 0.9393, Train: 0.9391, Val: 0.9689, Test: 0.9533\n",
      "Epoch: 576, Loss: 0.9391, Train: 0.9390, Val: 0.9688, Test: 0.9532\n",
      "Epoch: 577, Loss: 0.9390, Train: 0.9388, Val: 0.9687, Test: 0.9530\n",
      "Epoch: 578, Loss: 0.9388, Train: 0.9387, Val: 0.9685, Test: 0.9529\n",
      "Epoch: 579, Loss: 0.9387, Train: 0.9385, Val: 0.9684, Test: 0.9528\n",
      "Epoch: 580, Loss: 0.9385, Train: 0.9384, Val: 0.9683, Test: 0.9527\n",
      "Epoch: 581, Loss: 0.9384, Train: 0.9382, Val: 0.9682, Test: 0.9526\n",
      "Epoch: 582, Loss: 0.9382, Train: 0.9381, Val: 0.9681, Test: 0.9525\n",
      "Epoch: 583, Loss: 0.9381, Train: 0.9379, Val: 0.9680, Test: 0.9524\n",
      "Epoch: 584, Loss: 0.9379, Train: 0.9378, Val: 0.9678, Test: 0.9523\n",
      "Epoch: 585, Loss: 0.9378, Train: 0.9376, Val: 0.9677, Test: 0.9522\n",
      "Epoch: 586, Loss: 0.9376, Train: 0.9375, Val: 0.9676, Test: 0.9521\n",
      "Epoch: 587, Loss: 0.9375, Train: 0.9374, Val: 0.9675, Test: 0.9520\n",
      "Epoch: 588, Loss: 0.9374, Train: 0.9372, Val: 0.9674, Test: 0.9519\n",
      "Epoch: 589, Loss: 0.9372, Train: 0.9371, Val: 0.9673, Test: 0.9518\n",
      "Epoch: 590, Loss: 0.9371, Train: 0.9369, Val: 0.9672, Test: 0.9517\n",
      "Epoch: 591, Loss: 0.9369, Train: 0.9368, Val: 0.9671, Test: 0.9516\n",
      "Epoch: 592, Loss: 0.9368, Train: 0.9367, Val: 0.9670, Test: 0.9515\n",
      "Epoch: 593, Loss: 0.9367, Train: 0.9365, Val: 0.9669, Test: 0.9514\n",
      "Epoch: 594, Loss: 0.9365, Train: 0.9364, Val: 0.9668, Test: 0.9513\n",
      "Epoch: 595, Loss: 0.9364, Train: 0.9363, Val: 0.9667, Test: 0.9512\n",
      "Epoch: 596, Loss: 0.9363, Train: 0.9361, Val: 0.9666, Test: 0.9511\n",
      "Epoch: 597, Loss: 0.9361, Train: 0.9360, Val: 0.9665, Test: 0.9510\n",
      "Epoch: 598, Loss: 0.9360, Train: 0.9359, Val: 0.9664, Test: 0.9509\n",
      "Epoch: 599, Loss: 0.9359, Train: 0.9358, Val: 0.9663, Test: 0.9509\n",
      "Epoch: 600, Loss: 0.9358, Train: 0.9356, Val: 0.9662, Test: 0.9508\n",
      "Epoch: 601, Loss: 0.9356, Train: 0.9355, Val: 0.9661, Test: 0.9507\n",
      "Epoch: 602, Loss: 0.9355, Train: 0.9354, Val: 0.9660, Test: 0.9506\n",
      "Epoch: 603, Loss: 0.9354, Train: 0.9353, Val: 0.9659, Test: 0.9505\n",
      "Epoch: 604, Loss: 0.9353, Train: 0.9351, Val: 0.9658, Test: 0.9504\n",
      "Epoch: 605, Loss: 0.9351, Train: 0.9350, Val: 0.9657, Test: 0.9503\n",
      "Epoch: 606, Loss: 0.9350, Train: 0.9349, Val: 0.9656, Test: 0.9502\n",
      "Epoch: 607, Loss: 0.9349, Train: 0.9348, Val: 0.9656, Test: 0.9502\n",
      "Epoch: 608, Loss: 0.9348, Train: 0.9346, Val: 0.9655, Test: 0.9501\n",
      "Epoch: 609, Loss: 0.9346, Train: 0.9345, Val: 0.9654, Test: 0.9500\n",
      "Epoch: 610, Loss: 0.9345, Train: 0.9344, Val: 0.9653, Test: 0.9499\n",
      "Epoch: 611, Loss: 0.9344, Train: 0.9343, Val: 0.9652, Test: 0.9498\n",
      "Epoch: 612, Loss: 0.9343, Train: 0.9342, Val: 0.9651, Test: 0.9498\n",
      "Epoch: 613, Loss: 0.9342, Train: 0.9341, Val: 0.9650, Test: 0.9497\n",
      "Epoch: 614, Loss: 0.9341, Train: 0.9340, Val: 0.9650, Test: 0.9496\n",
      "Epoch: 615, Loss: 0.9340, Train: 0.9338, Val: 0.9649, Test: 0.9495\n",
      "Epoch: 616, Loss: 0.9338, Train: 0.9337, Val: 0.9648, Test: 0.9495\n",
      "Epoch: 617, Loss: 0.9337, Train: 0.9336, Val: 0.9647, Test: 0.9494\n",
      "Epoch: 618, Loss: 0.9336, Train: 0.9335, Val: 0.9646, Test: 0.9493\n",
      "Epoch: 619, Loss: 0.9335, Train: 0.9334, Val: 0.9645, Test: 0.9492\n",
      "Epoch: 620, Loss: 0.9334, Train: 0.9333, Val: 0.9645, Test: 0.9492\n",
      "Epoch: 621, Loss: 0.9333, Train: 0.9332, Val: 0.9644, Test: 0.9491\n",
      "Epoch: 622, Loss: 0.9332, Train: 0.9331, Val: 0.9643, Test: 0.9490\n",
      "Epoch: 623, Loss: 0.9331, Train: 0.9330, Val: 0.9642, Test: 0.9489\n",
      "Epoch: 624, Loss: 0.9330, Train: 0.9329, Val: 0.9642, Test: 0.9489\n",
      "Epoch: 625, Loss: 0.9329, Train: 0.9328, Val: 0.9641, Test: 0.9488\n",
      "Epoch: 626, Loss: 0.9328, Train: 0.9327, Val: 0.9640, Test: 0.9487\n",
      "Epoch: 627, Loss: 0.9327, Train: 0.9326, Val: 0.9639, Test: 0.9487\n",
      "Epoch: 628, Loss: 0.9326, Train: 0.9325, Val: 0.9639, Test: 0.9486\n",
      "Epoch: 629, Loss: 0.9325, Train: 0.9324, Val: 0.9638, Test: 0.9485\n",
      "Epoch: 630, Loss: 0.9324, Train: 0.9322, Val: 0.9637, Test: 0.9485\n",
      "Epoch: 631, Loss: 0.9323, Train: 0.9322, Val: 0.9636, Test: 0.9484\n",
      "Epoch: 632, Loss: 0.9322, Train: 0.9321, Val: 0.9636, Test: 0.9483\n",
      "Epoch: 633, Loss: 0.9321, Train: 0.9320, Val: 0.9635, Test: 0.9483\n",
      "Epoch: 634, Loss: 0.9320, Train: 0.9319, Val: 0.9634, Test: 0.9482\n",
      "Epoch: 635, Loss: 0.9319, Train: 0.9318, Val: 0.9634, Test: 0.9481\n",
      "Epoch: 636, Loss: 0.9318, Train: 0.9317, Val: 0.9633, Test: 0.9481\n",
      "Epoch: 637, Loss: 0.9317, Train: 0.9316, Val: 0.9632, Test: 0.9480\n",
      "Epoch: 638, Loss: 0.9316, Train: 0.9315, Val: 0.9632, Test: 0.9479\n",
      "Epoch: 639, Loss: 0.9315, Train: 0.9314, Val: 0.9631, Test: 0.9479\n",
      "Epoch: 640, Loss: 0.9314, Train: 0.9313, Val: 0.9630, Test: 0.9478\n",
      "Epoch: 641, Loss: 0.9313, Train: 0.9312, Val: 0.9630, Test: 0.9478\n",
      "Epoch: 642, Loss: 0.9312, Train: 0.9311, Val: 0.9629, Test: 0.9477\n",
      "Epoch: 643, Loss: 0.9311, Train: 0.9310, Val: 0.9628, Test: 0.9476\n",
      "Epoch: 644, Loss: 0.9310, Train: 0.9309, Val: 0.9628, Test: 0.9476\n",
      "Epoch: 645, Loss: 0.9309, Train: 0.9308, Val: 0.9627, Test: 0.9475\n",
      "Epoch: 646, Loss: 0.9308, Train: 0.9307, Val: 0.9626, Test: 0.9475\n",
      "Epoch: 647, Loss: 0.9307, Train: 0.9307, Val: 0.9626, Test: 0.9474\n",
      "Epoch: 648, Loss: 0.9307, Train: 0.9306, Val: 0.9625, Test: 0.9474\n",
      "Epoch: 649, Loss: 0.9306, Train: 0.9305, Val: 0.9625, Test: 0.9473\n",
      "Epoch: 650, Loss: 0.9305, Train: 0.9304, Val: 0.9624, Test: 0.9472\n",
      "Epoch: 651, Loss: 0.9304, Train: 0.9303, Val: 0.9623, Test: 0.9472\n",
      "Epoch: 652, Loss: 0.9303, Train: 0.9302, Val: 0.9623, Test: 0.9471\n",
      "Epoch: 653, Loss: 0.9302, Train: 0.9301, Val: 0.9622, Test: 0.9471\n",
      "Epoch: 654, Loss: 0.9301, Train: 0.9301, Val: 0.9622, Test: 0.9470\n",
      "Epoch: 655, Loss: 0.9301, Train: 0.9300, Val: 0.9621, Test: 0.9470\n",
      "Epoch: 656, Loss: 0.9300, Train: 0.9299, Val: 0.9620, Test: 0.9469\n",
      "Epoch: 657, Loss: 0.9299, Train: 0.9298, Val: 0.9620, Test: 0.9469\n",
      "Epoch: 658, Loss: 0.9298, Train: 0.9297, Val: 0.9619, Test: 0.9468\n",
      "Epoch: 659, Loss: 0.9297, Train: 0.9296, Val: 0.9619, Test: 0.9468\n",
      "Epoch: 660, Loss: 0.9296, Train: 0.9296, Val: 0.9618, Test: 0.9467\n",
      "Epoch: 661, Loss: 0.9296, Train: 0.9295, Val: 0.9618, Test: 0.9467\n",
      "Epoch: 662, Loss: 0.9295, Train: 0.9294, Val: 0.9617, Test: 0.9466\n",
      "Epoch: 663, Loss: 0.9294, Train: 0.9293, Val: 0.9617, Test: 0.9466\n",
      "Epoch: 664, Loss: 0.9293, Train: 0.9292, Val: 0.9616, Test: 0.9465\n",
      "Epoch: 665, Loss: 0.9292, Train: 0.9292, Val: 0.9616, Test: 0.9465\n",
      "Epoch: 666, Loss: 0.9292, Train: 0.9291, Val: 0.9615, Test: 0.9464\n",
      "Epoch: 667, Loss: 0.9291, Train: 0.9290, Val: 0.9614, Test: 0.9464\n",
      "Epoch: 668, Loss: 0.9290, Train: 0.9289, Val: 0.9614, Test: 0.9463\n",
      "Epoch: 669, Loss: 0.9289, Train: 0.9289, Val: 0.9613, Test: 0.9463\n",
      "Epoch: 670, Loss: 0.9289, Train: 0.9288, Val: 0.9613, Test: 0.9462\n",
      "Epoch: 671, Loss: 0.9288, Train: 0.9287, Val: 0.9612, Test: 0.9462\n",
      "Epoch: 672, Loss: 0.9287, Train: 0.9286, Val: 0.9612, Test: 0.9461\n",
      "Epoch: 673, Loss: 0.9286, Train: 0.9286, Val: 0.9611, Test: 0.9461\n",
      "Epoch: 674, Loss: 0.9286, Train: 0.9285, Val: 0.9611, Test: 0.9460\n",
      "Epoch: 675, Loss: 0.9285, Train: 0.9284, Val: 0.9610, Test: 0.9460\n",
      "Epoch: 676, Loss: 0.9284, Train: 0.9283, Val: 0.9610, Test: 0.9460\n",
      "Epoch: 677, Loss: 0.9283, Train: 0.9283, Val: 0.9609, Test: 0.9459\n",
      "Epoch: 678, Loss: 0.9283, Train: 0.9282, Val: 0.9609, Test: 0.9459\n",
      "Epoch: 679, Loss: 0.9282, Train: 0.9281, Val: 0.9608, Test: 0.9458\n",
      "Epoch: 680, Loss: 0.9281, Train: 0.9280, Val: 0.9608, Test: 0.9458\n",
      "Epoch: 681, Loss: 0.9280, Train: 0.9280, Val: 0.9608, Test: 0.9457\n",
      "Epoch: 682, Loss: 0.9280, Train: 0.9279, Val: 0.9607, Test: 0.9457\n",
      "Epoch: 683, Loss: 0.9279, Train: 0.9278, Val: 0.9607, Test: 0.9457\n",
      "Epoch: 684, Loss: 0.9278, Train: 0.9278, Val: 0.9606, Test: 0.9456\n",
      "Epoch: 685, Loss: 0.9278, Train: 0.9277, Val: 0.9606, Test: 0.9456\n",
      "Epoch: 686, Loss: 0.9277, Train: 0.9276, Val: 0.9605, Test: 0.9455\n",
      "Epoch: 687, Loss: 0.9276, Train: 0.9276, Val: 0.9605, Test: 0.9455\n",
      "Epoch: 688, Loss: 0.9276, Train: 0.9275, Val: 0.9604, Test: 0.9454\n",
      "Epoch: 689, Loss: 0.9275, Train: 0.9274, Val: 0.9604, Test: 0.9454\n",
      "Epoch: 690, Loss: 0.9274, Train: 0.9273, Val: 0.9603, Test: 0.9454\n",
      "Epoch: 691, Loss: 0.9274, Train: 0.9273, Val: 0.9603, Test: 0.9453\n",
      "Epoch: 692, Loss: 0.9273, Train: 0.9272, Val: 0.9603, Test: 0.9453\n",
      "Epoch: 693, Loss: 0.9272, Train: 0.9271, Val: 0.9602, Test: 0.9452\n",
      "Epoch: 694, Loss: 0.9272, Train: 0.9271, Val: 0.9602, Test: 0.9452\n",
      "Epoch: 695, Loss: 0.9271, Train: 0.9270, Val: 0.9601, Test: 0.9452\n",
      "Epoch: 696, Loss: 0.9270, Train: 0.9269, Val: 0.9601, Test: 0.9451\n",
      "Epoch: 697, Loss: 0.9270, Train: 0.9269, Val: 0.9600, Test: 0.9451\n",
      "Epoch: 698, Loss: 0.9269, Train: 0.9268, Val: 0.9600, Test: 0.9451\n",
      "Epoch: 699, Loss: 0.9268, Train: 0.9268, Val: 0.9600, Test: 0.9450\n",
      "Epoch: 700, Loss: 0.9268, Train: 0.9267, Val: 0.9599, Test: 0.9450\n",
      "Epoch: 701, Loss: 0.9267, Train: 0.9266, Val: 0.9599, Test: 0.9449\n",
      "Epoch: 702, Loss: 0.9266, Train: 0.9266, Val: 0.9598, Test: 0.9449\n",
      "Epoch: 703, Loss: 0.9266, Train: 0.9265, Val: 0.9598, Test: 0.9449\n",
      "Epoch: 704, Loss: 0.9265, Train: 0.9264, Val: 0.9598, Test: 0.9448\n",
      "Epoch: 705, Loss: 0.9264, Train: 0.9264, Val: 0.9597, Test: 0.9448\n",
      "Epoch: 706, Loss: 0.9264, Train: 0.9263, Val: 0.9597, Test: 0.9448\n",
      "Epoch: 707, Loss: 0.9263, Train: 0.9262, Val: 0.9596, Test: 0.9447\n",
      "Epoch: 708, Loss: 0.9263, Train: 0.9262, Val: 0.9596, Test: 0.9447\n",
      "Epoch: 709, Loss: 0.9262, Train: 0.9261, Val: 0.9596, Test: 0.9447\n",
      "Epoch: 710, Loss: 0.9261, Train: 0.9261, Val: 0.9595, Test: 0.9446\n",
      "Epoch: 711, Loss: 0.9261, Train: 0.9260, Val: 0.9595, Test: 0.9446\n",
      "Epoch: 712, Loss: 0.9260, Train: 0.9259, Val: 0.9594, Test: 0.9446\n",
      "Epoch: 713, Loss: 0.9260, Train: 0.9259, Val: 0.9594, Test: 0.9445\n",
      "Epoch: 714, Loss: 0.9259, Train: 0.9258, Val: 0.9594, Test: 0.9445\n",
      "Epoch: 715, Loss: 0.9258, Train: 0.9258, Val: 0.9593, Test: 0.9445\n",
      "Epoch: 716, Loss: 0.9258, Train: 0.9257, Val: 0.9593, Test: 0.9444\n",
      "Epoch: 717, Loss: 0.9257, Train: 0.9256, Val: 0.9593, Test: 0.9444\n",
      "Epoch: 718, Loss: 0.9257, Train: 0.9256, Val: 0.9592, Test: 0.9444\n",
      "Epoch: 719, Loss: 0.9256, Train: 0.9255, Val: 0.9592, Test: 0.9443\n",
      "Epoch: 720, Loss: 0.9255, Train: 0.9255, Val: 0.9591, Test: 0.9443\n",
      "Epoch: 721, Loss: 0.9255, Train: 0.9254, Val: 0.9591, Test: 0.9443\n",
      "Epoch: 722, Loss: 0.9254, Train: 0.9254, Val: 0.9591, Test: 0.9442\n",
      "Epoch: 723, Loss: 0.9254, Train: 0.9253, Val: 0.9590, Test: 0.9442\n",
      "Epoch: 724, Loss: 0.9253, Train: 0.9252, Val: 0.9590, Test: 0.9442\n",
      "Epoch: 725, Loss: 0.9252, Train: 0.9252, Val: 0.9590, Test: 0.9441\n",
      "Epoch: 726, Loss: 0.9252, Train: 0.9251, Val: 0.9589, Test: 0.9441\n",
      "Epoch: 727, Loss: 0.9251, Train: 0.9251, Val: 0.9589, Test: 0.9441\n",
      "Epoch: 728, Loss: 0.9251, Train: 0.9250, Val: 0.9589, Test: 0.9440\n",
      "Epoch: 729, Loss: 0.9250, Train: 0.9250, Val: 0.9588, Test: 0.9440\n",
      "Epoch: 730, Loss: 0.9250, Train: 0.9249, Val: 0.9588, Test: 0.9440\n",
      "Epoch: 731, Loss: 0.9249, Train: 0.9248, Val: 0.9588, Test: 0.9439\n",
      "Epoch: 732, Loss: 0.9249, Train: 0.9248, Val: 0.9587, Test: 0.9439\n",
      "Epoch: 733, Loss: 0.9248, Train: 0.9247, Val: 0.9587, Test: 0.9439\n",
      "Epoch: 734, Loss: 0.9247, Train: 0.9247, Val: 0.9587, Test: 0.9438\n",
      "Epoch: 735, Loss: 0.9247, Train: 0.9246, Val: 0.9586, Test: 0.9438\n",
      "Epoch: 736, Loss: 0.9246, Train: 0.9246, Val: 0.9586, Test: 0.9438\n",
      "Epoch: 737, Loss: 0.9246, Train: 0.9245, Val: 0.9586, Test: 0.9438\n",
      "Epoch: 738, Loss: 0.9245, Train: 0.9245, Val: 0.9585, Test: 0.9437\n",
      "Epoch: 739, Loss: 0.9245, Train: 0.9244, Val: 0.9585, Test: 0.9437\n",
      "Epoch: 740, Loss: 0.9244, Train: 0.9244, Val: 0.9585, Test: 0.9437\n",
      "Epoch: 741, Loss: 0.9244, Train: 0.9243, Val: 0.9584, Test: 0.9436\n",
      "Epoch: 742, Loss: 0.9243, Train: 0.9243, Val: 0.9584, Test: 0.9436\n",
      "Epoch: 743, Loss: 0.9243, Train: 0.9242, Val: 0.9584, Test: 0.9436\n",
      "Epoch: 744, Loss: 0.9242, Train: 0.9241, Val: 0.9583, Test: 0.9436\n",
      "Epoch: 745, Loss: 0.9242, Train: 0.9241, Val: 0.9583, Test: 0.9435\n",
      "Epoch: 746, Loss: 0.9241, Train: 0.9240, Val: 0.9583, Test: 0.9435\n",
      "Epoch: 747, Loss: 0.9241, Train: 0.9240, Val: 0.9583, Test: 0.9435\n",
      "Epoch: 748, Loss: 0.9240, Train: 0.9239, Val: 0.9582, Test: 0.9434\n",
      "Epoch: 749, Loss: 0.9240, Train: 0.9239, Val: 0.9582, Test: 0.9434\n",
      "Epoch: 750, Loss: 0.9239, Train: 0.9238, Val: 0.9582, Test: 0.9434\n",
      "Epoch: 751, Loss: 0.9238, Train: 0.9238, Val: 0.9581, Test: 0.9434\n",
      "Epoch: 752, Loss: 0.9238, Train: 0.9237, Val: 0.9581, Test: 0.9433\n",
      "Epoch: 753, Loss: 0.9237, Train: 0.9237, Val: 0.9581, Test: 0.9433\n",
      "Epoch: 754, Loss: 0.9237, Train: 0.9236, Val: 0.9580, Test: 0.9433\n",
      "Epoch: 755, Loss: 0.9236, Train: 0.9236, Val: 0.9580, Test: 0.9433\n",
      "Epoch: 756, Loss: 0.9236, Train: 0.9235, Val: 0.9580, Test: 0.9432\n",
      "Epoch: 757, Loss: 0.9235, Train: 0.9235, Val: 0.9579, Test: 0.9432\n",
      "Epoch: 758, Loss: 0.9235, Train: 0.9234, Val: 0.9579, Test: 0.9432\n",
      "Epoch: 759, Loss: 0.9234, Train: 0.9234, Val: 0.9579, Test: 0.9432\n",
      "Epoch: 760, Loss: 0.9234, Train: 0.9233, Val: 0.9579, Test: 0.9431\n",
      "Epoch: 761, Loss: 0.9233, Train: 0.9233, Val: 0.9578, Test: 0.9431\n",
      "Epoch: 762, Loss: 0.9233, Train: 0.9232, Val: 0.9578, Test: 0.9431\n",
      "Epoch: 763, Loss: 0.9232, Train: 0.9232, Val: 0.9578, Test: 0.9431\n",
      "Epoch: 764, Loss: 0.9232, Train: 0.9231, Val: 0.9577, Test: 0.9430\n",
      "Epoch: 765, Loss: 0.9232, Train: 0.9231, Val: 0.9577, Test: 0.9430\n",
      "Epoch: 766, Loss: 0.9231, Train: 0.9230, Val: 0.9577, Test: 0.9430\n",
      "Epoch: 767, Loss: 0.9231, Train: 0.9230, Val: 0.9577, Test: 0.9430\n",
      "Epoch: 768, Loss: 0.9230, Train: 0.9229, Val: 0.9576, Test: 0.9429\n",
      "Epoch: 769, Loss: 0.9230, Train: 0.9229, Val: 0.9576, Test: 0.9429\n",
      "Epoch: 770, Loss: 0.9229, Train: 0.9228, Val: 0.9576, Test: 0.9429\n",
      "Epoch: 771, Loss: 0.9229, Train: 0.9228, Val: 0.9575, Test: 0.9429\n",
      "Epoch: 772, Loss: 0.9228, Train: 0.9228, Val: 0.9575, Test: 0.9428\n",
      "Epoch: 773, Loss: 0.9228, Train: 0.9227, Val: 0.9575, Test: 0.9428\n",
      "Epoch: 774, Loss: 0.9227, Train: 0.9227, Val: 0.9575, Test: 0.9428\n",
      "Epoch: 775, Loss: 0.9227, Train: 0.9226, Val: 0.9574, Test: 0.9428\n",
      "Epoch: 776, Loss: 0.9226, Train: 0.9226, Val: 0.9574, Test: 0.9427\n",
      "Epoch: 777, Loss: 0.9226, Train: 0.9225, Val: 0.9574, Test: 0.9427\n",
      "Epoch: 778, Loss: 0.9225, Train: 0.9225, Val: 0.9574, Test: 0.9427\n",
      "Epoch: 779, Loss: 0.9225, Train: 0.9224, Val: 0.9573, Test: 0.9427\n",
      "Epoch: 780, Loss: 0.9224, Train: 0.9224, Val: 0.9573, Test: 0.9426\n",
      "Epoch: 781, Loss: 0.9224, Train: 0.9223, Val: 0.9573, Test: 0.9426\n",
      "Epoch: 782, Loss: 0.9223, Train: 0.9223, Val: 0.9573, Test: 0.9426\n",
      "Epoch: 783, Loss: 0.9223, Train: 0.9222, Val: 0.9572, Test: 0.9426\n",
      "Epoch: 784, Loss: 0.9223, Train: 0.9222, Val: 0.9572, Test: 0.9425\n",
      "Epoch: 785, Loss: 0.9222, Train: 0.9221, Val: 0.9572, Test: 0.9425\n",
      "Epoch: 786, Loss: 0.9222, Train: 0.9221, Val: 0.9572, Test: 0.9425\n",
      "Epoch: 787, Loss: 0.9221, Train: 0.9221, Val: 0.9571, Test: 0.9425\n",
      "Epoch: 788, Loss: 0.9221, Train: 0.9220, Val: 0.9571, Test: 0.9425\n",
      "Epoch: 789, Loss: 0.9220, Train: 0.9220, Val: 0.9571, Test: 0.9424\n",
      "Epoch: 790, Loss: 0.9220, Train: 0.9219, Val: 0.9570, Test: 0.9424\n",
      "Epoch: 791, Loss: 0.9219, Train: 0.9219, Val: 0.9570, Test: 0.9424\n",
      "Epoch: 792, Loss: 0.9219, Train: 0.9218, Val: 0.9570, Test: 0.9424\n",
      "Epoch: 793, Loss: 0.9219, Train: 0.9218, Val: 0.9570, Test: 0.9423\n",
      "Epoch: 794, Loss: 0.9218, Train: 0.9217, Val: 0.9569, Test: 0.9423\n",
      "Epoch: 795, Loss: 0.9218, Train: 0.9217, Val: 0.9569, Test: 0.9423\n",
      "Epoch: 796, Loss: 0.9217, Train: 0.9217, Val: 0.9569, Test: 0.9423\n",
      "Epoch: 797, Loss: 0.9217, Train: 0.9216, Val: 0.9569, Test: 0.9423\n",
      "Epoch: 798, Loss: 0.9216, Train: 0.9216, Val: 0.9568, Test: 0.9422\n",
      "Epoch: 799, Loss: 0.9216, Train: 0.9215, Val: 0.9568, Test: 0.9422\n",
      "Epoch: 800, Loss: 0.9215, Train: 0.9215, Val: 0.9568, Test: 0.9422\n",
      "Epoch: 801, Loss: 0.9215, Train: 0.9214, Val: 0.9568, Test: 0.9422\n",
      "Epoch: 802, Loss: 0.9215, Train: 0.9214, Val: 0.9567, Test: 0.9421\n",
      "Epoch: 803, Loss: 0.9214, Train: 0.9214, Val: 0.9567, Test: 0.9421\n",
      "Epoch: 804, Loss: 0.9214, Train: 0.9213, Val: 0.9567, Test: 0.9421\n",
      "Epoch: 805, Loss: 0.9213, Train: 0.9213, Val: 0.9567, Test: 0.9421\n",
      "Epoch: 806, Loss: 0.9213, Train: 0.9212, Val: 0.9567, Test: 0.9421\n",
      "Epoch: 807, Loss: 0.9212, Train: 0.9212, Val: 0.9566, Test: 0.9420\n",
      "Epoch: 808, Loss: 0.9212, Train: 0.9211, Val: 0.9566, Test: 0.9420\n",
      "Epoch: 809, Loss: 0.9212, Train: 0.9211, Val: 0.9566, Test: 0.9420\n",
      "Epoch: 810, Loss: 0.9211, Train: 0.9211, Val: 0.9566, Test: 0.9420\n",
      "Epoch: 811, Loss: 0.9211, Train: 0.9210, Val: 0.9565, Test: 0.9420\n",
      "Epoch: 812, Loss: 0.9210, Train: 0.9210, Val: 0.9565, Test: 0.9419\n",
      "Epoch: 813, Loss: 0.9210, Train: 0.9209, Val: 0.9565, Test: 0.9419\n",
      "Epoch: 814, Loss: 0.9209, Train: 0.9209, Val: 0.9565, Test: 0.9419\n",
      "Epoch: 815, Loss: 0.9209, Train: 0.9208, Val: 0.9564, Test: 0.9419\n",
      "Epoch: 816, Loss: 0.9209, Train: 0.9208, Val: 0.9564, Test: 0.9419\n",
      "Epoch: 817, Loss: 0.9208, Train: 0.9208, Val: 0.9564, Test: 0.9418\n",
      "Epoch: 818, Loss: 0.9208, Train: 0.9207, Val: 0.9564, Test: 0.9418\n",
      "Epoch: 819, Loss: 0.9207, Train: 0.9207, Val: 0.9563, Test: 0.9418\n",
      "Epoch: 820, Loss: 0.9207, Train: 0.9206, Val: 0.9563, Test: 0.9418\n",
      "Epoch: 821, Loss: 0.9207, Train: 0.9206, Val: 0.9563, Test: 0.9417\n",
      "Epoch: 822, Loss: 0.9206, Train: 0.9206, Val: 0.9563, Test: 0.9417\n",
      "Epoch: 823, Loss: 0.9206, Train: 0.9205, Val: 0.9563, Test: 0.9417\n",
      "Epoch: 824, Loss: 0.9205, Train: 0.9205, Val: 0.9562, Test: 0.9417\n",
      "Epoch: 825, Loss: 0.9205, Train: 0.9204, Val: 0.9562, Test: 0.9417\n",
      "Epoch: 826, Loss: 0.9205, Train: 0.9204, Val: 0.9562, Test: 0.9417\n",
      "Epoch: 827, Loss: 0.9204, Train: 0.9204, Val: 0.9562, Test: 0.9416\n",
      "Epoch: 828, Loss: 0.9204, Train: 0.9203, Val: 0.9561, Test: 0.9416\n",
      "Epoch: 829, Loss: 0.9203, Train: 0.9203, Val: 0.9561, Test: 0.9416\n",
      "Epoch: 830, Loss: 0.9203, Train: 0.9202, Val: 0.9561, Test: 0.9416\n",
      "Epoch: 831, Loss: 0.9203, Train: 0.9202, Val: 0.9561, Test: 0.9416\n",
      "Epoch: 832, Loss: 0.9202, Train: 0.9202, Val: 0.9561, Test: 0.9415\n",
      "Epoch: 833, Loss: 0.9202, Train: 0.9201, Val: 0.9560, Test: 0.9415\n",
      "Epoch: 834, Loss: 0.9201, Train: 0.9201, Val: 0.9560, Test: 0.9415\n",
      "Epoch: 835, Loss: 0.9201, Train: 0.9200, Val: 0.9560, Test: 0.9415\n",
      "Epoch: 836, Loss: 0.9201, Train: 0.9200, Val: 0.9560, Test: 0.9415\n",
      "Epoch: 837, Loss: 0.9200, Train: 0.9200, Val: 0.9559, Test: 0.9414\n",
      "Epoch: 838, Loss: 0.9200, Train: 0.9199, Val: 0.9559, Test: 0.9414\n",
      "Epoch: 839, Loss: 0.9199, Train: 0.9199, Val: 0.9559, Test: 0.9414\n",
      "Epoch: 840, Loss: 0.9199, Train: 0.9198, Val: 0.9559, Test: 0.9414\n",
      "Epoch: 841, Loss: 0.9199, Train: 0.9198, Val: 0.9559, Test: 0.9414\n",
      "Epoch: 842, Loss: 0.9198, Train: 0.9198, Val: 0.9558, Test: 0.9413\n",
      "Epoch: 843, Loss: 0.9198, Train: 0.9197, Val: 0.9558, Test: 0.9413\n",
      "Epoch: 844, Loss: 0.9197, Train: 0.9197, Val: 0.9558, Test: 0.9413\n",
      "Epoch: 845, Loss: 0.9197, Train: 0.9196, Val: 0.9558, Test: 0.9413\n",
      "Epoch: 846, Loss: 0.9197, Train: 0.9196, Val: 0.9558, Test: 0.9413\n",
      "Epoch: 847, Loss: 0.9196, Train: 0.9196, Val: 0.9557, Test: 0.9413\n",
      "Epoch: 848, Loss: 0.9196, Train: 0.9195, Val: 0.9557, Test: 0.9412\n",
      "Epoch: 849, Loss: 0.9195, Train: 0.9195, Val: 0.9557, Test: 0.9412\n",
      "Epoch: 850, Loss: 0.9195, Train: 0.9195, Val: 0.9557, Test: 0.9412\n",
      "Epoch: 851, Loss: 0.9195, Train: 0.9194, Val: 0.9557, Test: 0.9412\n",
      "Epoch: 852, Loss: 0.9194, Train: 0.9194, Val: 0.9556, Test: 0.9412\n",
      "Epoch: 853, Loss: 0.9194, Train: 0.9193, Val: 0.9556, Test: 0.9412\n",
      "Epoch: 854, Loss: 0.9194, Train: 0.9193, Val: 0.9556, Test: 0.9411\n",
      "Epoch: 855, Loss: 0.9193, Train: 0.9193, Val: 0.9556, Test: 0.9411\n",
      "Epoch: 856, Loss: 0.9193, Train: 0.9192, Val: 0.9555, Test: 0.9411\n",
      "Epoch: 857, Loss: 0.9192, Train: 0.9192, Val: 0.9555, Test: 0.9411\n",
      "Epoch: 858, Loss: 0.9192, Train: 0.9192, Val: 0.9555, Test: 0.9411\n",
      "Epoch: 859, Loss: 0.9192, Train: 0.9191, Val: 0.9555, Test: 0.9410\n",
      "Epoch: 860, Loss: 0.9191, Train: 0.9191, Val: 0.9555, Test: 0.9410\n",
      "Epoch: 861, Loss: 0.9191, Train: 0.9190, Val: 0.9554, Test: 0.9410\n",
      "Epoch: 862, Loss: 0.9191, Train: 0.9190, Val: 0.9554, Test: 0.9410\n",
      "Epoch: 863, Loss: 0.9190, Train: 0.9190, Val: 0.9554, Test: 0.9410\n",
      "Epoch: 864, Loss: 0.9190, Train: 0.9189, Val: 0.9554, Test: 0.9410\n",
      "Epoch: 865, Loss: 0.9189, Train: 0.9189, Val: 0.9554, Test: 0.9409\n",
      "Epoch: 866, Loss: 0.9189, Train: 0.9189, Val: 0.9553, Test: 0.9409\n",
      "Epoch: 867, Loss: 0.9189, Train: 0.9188, Val: 0.9553, Test: 0.9409\n",
      "Epoch: 868, Loss: 0.9188, Train: 0.9188, Val: 0.9553, Test: 0.9409\n",
      "Epoch: 869, Loss: 0.9188, Train: 0.9187, Val: 0.9553, Test: 0.9409\n",
      "Epoch: 870, Loss: 0.9188, Train: 0.9187, Val: 0.9553, Test: 0.9409\n",
      "Epoch: 871, Loss: 0.9187, Train: 0.9187, Val: 0.9552, Test: 0.9408\n",
      "Epoch: 872, Loss: 0.9187, Train: 0.9186, Val: 0.9552, Test: 0.9408\n",
      "Epoch: 873, Loss: 0.9187, Train: 0.9186, Val: 0.9552, Test: 0.9408\n",
      "Epoch: 874, Loss: 0.9186, Train: 0.9186, Val: 0.9552, Test: 0.9408\n",
      "Epoch: 875, Loss: 0.9186, Train: 0.9185, Val: 0.9552, Test: 0.9408\n",
      "Epoch: 876, Loss: 0.9186, Train: 0.9185, Val: 0.9552, Test: 0.9408\n",
      "Epoch: 877, Loss: 0.9185, Train: 0.9185, Val: 0.9551, Test: 0.9407\n",
      "Epoch: 878, Loss: 0.9185, Train: 0.9184, Val: 0.9551, Test: 0.9407\n",
      "Epoch: 879, Loss: 0.9184, Train: 0.9184, Val: 0.9551, Test: 0.9407\n",
      "Epoch: 880, Loss: 0.9184, Train: 0.9183, Val: 0.9551, Test: 0.9407\n",
      "Epoch: 881, Loss: 0.9184, Train: 0.9183, Val: 0.9551, Test: 0.9407\n",
      "Epoch: 882, Loss: 0.9183, Train: 0.9183, Val: 0.9550, Test: 0.9407\n",
      "Epoch: 883, Loss: 0.9183, Train: 0.9182, Val: 0.9550, Test: 0.9407\n",
      "Epoch: 884, Loss: 0.9183, Train: 0.9182, Val: 0.9550, Test: 0.9406\n",
      "Epoch: 885, Loss: 0.9182, Train: 0.9182, Val: 0.9550, Test: 0.9406\n",
      "Epoch: 886, Loss: 0.9182, Train: 0.9181, Val: 0.9550, Test: 0.9406\n",
      "Epoch: 887, Loss: 0.9182, Train: 0.9181, Val: 0.9549, Test: 0.9406\n",
      "Epoch: 888, Loss: 0.9181, Train: 0.9181, Val: 0.9549, Test: 0.9406\n",
      "Epoch: 889, Loss: 0.9181, Train: 0.9180, Val: 0.9549, Test: 0.9406\n",
      "Epoch: 890, Loss: 0.9181, Train: 0.9180, Val: 0.9549, Test: 0.9405\n",
      "Epoch: 891, Loss: 0.9180, Train: 0.9180, Val: 0.9549, Test: 0.9405\n",
      "Epoch: 892, Loss: 0.9180, Train: 0.9179, Val: 0.9548, Test: 0.9405\n",
      "Epoch: 893, Loss: 0.9180, Train: 0.9179, Val: 0.9548, Test: 0.9405\n",
      "Epoch: 894, Loss: 0.9179, Train: 0.9179, Val: 0.9548, Test: 0.9405\n",
      "Epoch: 895, Loss: 0.9179, Train: 0.9178, Val: 0.9548, Test: 0.9405\n",
      "Epoch: 896, Loss: 0.9179, Train: 0.9178, Val: 0.9548, Test: 0.9405\n",
      "Epoch: 897, Loss: 0.9178, Train: 0.9178, Val: 0.9548, Test: 0.9404\n",
      "Epoch: 898, Loss: 0.9178, Train: 0.9177, Val: 0.9547, Test: 0.9404\n",
      "Epoch: 899, Loss: 0.9177, Train: 0.9177, Val: 0.9547, Test: 0.9404\n",
      "Epoch: 900, Loss: 0.9177, Train: 0.9177, Val: 0.9547, Test: 0.9404\n",
      "Epoch: 901, Loss: 0.9177, Train: 0.9176, Val: 0.9547, Test: 0.9404\n",
      "Epoch: 902, Loss: 0.9176, Train: 0.9176, Val: 0.9547, Test: 0.9404\n",
      "Epoch: 903, Loss: 0.9176, Train: 0.9176, Val: 0.9546, Test: 0.9403\n",
      "Epoch: 904, Loss: 0.9176, Train: 0.9175, Val: 0.9546, Test: 0.9403\n",
      "Epoch: 905, Loss: 0.9175, Train: 0.9175, Val: 0.9546, Test: 0.9403\n",
      "Epoch: 906, Loss: 0.9175, Train: 0.9175, Val: 0.9546, Test: 0.9403\n",
      "Epoch: 907, Loss: 0.9175, Train: 0.9174, Val: 0.9546, Test: 0.9403\n",
      "Epoch: 908, Loss: 0.9174, Train: 0.9174, Val: 0.9546, Test: 0.9403\n",
      "Epoch: 909, Loss: 0.9174, Train: 0.9174, Val: 0.9545, Test: 0.9403\n",
      "Epoch: 910, Loss: 0.9174, Train: 0.9173, Val: 0.9545, Test: 0.9402\n",
      "Epoch: 911, Loss: 0.9173, Train: 0.9173, Val: 0.9545, Test: 0.9402\n",
      "Epoch: 912, Loss: 0.9173, Train: 0.9173, Val: 0.9545, Test: 0.9402\n",
      "Epoch: 913, Loss: 0.9173, Train: 0.9172, Val: 0.9545, Test: 0.9402\n",
      "Epoch: 914, Loss: 0.9172, Train: 0.9172, Val: 0.9544, Test: 0.9402\n",
      "Epoch: 915, Loss: 0.9172, Train: 0.9172, Val: 0.9544, Test: 0.9402\n",
      "Epoch: 916, Loss: 0.9172, Train: 0.9171, Val: 0.9544, Test: 0.9402\n",
      "Epoch: 917, Loss: 0.9171, Train: 0.9171, Val: 0.9544, Test: 0.9401\n",
      "Epoch: 918, Loss: 0.9171, Train: 0.9171, Val: 0.9544, Test: 0.9401\n",
      "Epoch: 919, Loss: 0.9171, Train: 0.9170, Val: 0.9544, Test: 0.9401\n",
      "Epoch: 920, Loss: 0.9170, Train: 0.9170, Val: 0.9543, Test: 0.9401\n",
      "Epoch: 921, Loss: 0.9170, Train: 0.9170, Val: 0.9543, Test: 0.9401\n",
      "Epoch: 922, Loss: 0.9170, Train: 0.9169, Val: 0.9543, Test: 0.9401\n",
      "Epoch: 923, Loss: 0.9170, Train: 0.9169, Val: 0.9543, Test: 0.9401\n",
      "Epoch: 924, Loss: 0.9169, Train: 0.9169, Val: 0.9543, Test: 0.9400\n",
      "Epoch: 925, Loss: 0.9169, Train: 0.9168, Val: 0.9542, Test: 0.9400\n",
      "Epoch: 926, Loss: 0.9169, Train: 0.9168, Val: 0.9542, Test: 0.9400\n",
      "Epoch: 927, Loss: 0.9168, Train: 0.9168, Val: 0.9542, Test: 0.9400\n",
      "Epoch: 928, Loss: 0.9168, Train: 0.9167, Val: 0.9542, Test: 0.9400\n",
      "Epoch: 929, Loss: 0.9168, Train: 0.9167, Val: 0.9542, Test: 0.9400\n",
      "Epoch: 930, Loss: 0.9167, Train: 0.9167, Val: 0.9542, Test: 0.9400\n",
      "Epoch: 931, Loss: 0.9167, Train: 0.9166, Val: 0.9541, Test: 0.9399\n",
      "Epoch: 932, Loss: 0.9167, Train: 0.9166, Val: 0.9541, Test: 0.9399\n",
      "Epoch: 933, Loss: 0.9166, Train: 0.9166, Val: 0.9541, Test: 0.9399\n",
      "Epoch: 934, Loss: 0.9166, Train: 0.9165, Val: 0.9541, Test: 0.9399\n",
      "Epoch: 935, Loss: 0.9166, Train: 0.9165, Val: 0.9541, Test: 0.9399\n",
      "Epoch: 936, Loss: 0.9165, Train: 0.9165, Val: 0.9541, Test: 0.9399\n",
      "Epoch: 937, Loss: 0.9165, Train: 0.9164, Val: 0.9540, Test: 0.9399\n",
      "Epoch: 938, Loss: 0.9165, Train: 0.9164, Val: 0.9540, Test: 0.9399\n",
      "Epoch: 939, Loss: 0.9164, Train: 0.9164, Val: 0.9540, Test: 0.9398\n",
      "Epoch: 940, Loss: 0.9164, Train: 0.9164, Val: 0.9540, Test: 0.9398\n",
      "Epoch: 941, Loss: 0.9164, Train: 0.9163, Val: 0.9540, Test: 0.9398\n",
      "Epoch: 942, Loss: 0.9163, Train: 0.9163, Val: 0.9540, Test: 0.9398\n",
      "Epoch: 943, Loss: 0.9163, Train: 0.9163, Val: 0.9539, Test: 0.9398\n",
      "Epoch: 944, Loss: 0.9163, Train: 0.9162, Val: 0.9539, Test: 0.9398\n",
      "Epoch: 945, Loss: 0.9163, Train: 0.9162, Val: 0.9539, Test: 0.9398\n",
      "Epoch: 946, Loss: 0.9162, Train: 0.9162, Val: 0.9539, Test: 0.9397\n",
      "Epoch: 947, Loss: 0.9162, Train: 0.9161, Val: 0.9539, Test: 0.9397\n",
      "Epoch: 948, Loss: 0.9162, Train: 0.9161, Val: 0.9539, Test: 0.9397\n",
      "Epoch: 949, Loss: 0.9161, Train: 0.9161, Val: 0.9538, Test: 0.9397\n",
      "Epoch: 950, Loss: 0.9161, Train: 0.9160, Val: 0.9538, Test: 0.9397\n",
      "Epoch: 951, Loss: 0.9161, Train: 0.9160, Val: 0.9538, Test: 0.9397\n",
      "Epoch: 952, Loss: 0.9160, Train: 0.9160, Val: 0.9538, Test: 0.9397\n",
      "Epoch: 953, Loss: 0.9160, Train: 0.9159, Val: 0.9538, Test: 0.9397\n",
      "Epoch: 954, Loss: 0.9160, Train: 0.9159, Val: 0.9538, Test: 0.9396\n",
      "Epoch: 955, Loss: 0.9159, Train: 0.9159, Val: 0.9537, Test: 0.9396\n",
      "Epoch: 956, Loss: 0.9159, Train: 0.9159, Val: 0.9537, Test: 0.9396\n",
      "Epoch: 957, Loss: 0.9159, Train: 0.9158, Val: 0.9537, Test: 0.9396\n",
      "Epoch: 958, Loss: 0.9159, Train: 0.9158, Val: 0.9537, Test: 0.9396\n",
      "Epoch: 959, Loss: 0.9158, Train: 0.9158, Val: 0.9537, Test: 0.9396\n",
      "Epoch: 960, Loss: 0.9158, Train: 0.9157, Val: 0.9537, Test: 0.9396\n",
      "Epoch: 961, Loss: 0.9158, Train: 0.9157, Val: 0.9536, Test: 0.9396\n",
      "Epoch: 962, Loss: 0.9157, Train: 0.9157, Val: 0.9536, Test: 0.9395\n",
      "Epoch: 963, Loss: 0.9157, Train: 0.9156, Val: 0.9536, Test: 0.9395\n",
      "Epoch: 964, Loss: 0.9157, Train: 0.9156, Val: 0.9536, Test: 0.9395\n",
      "Epoch: 965, Loss: 0.9156, Train: 0.9156, Val: 0.9536, Test: 0.9395\n",
      "Epoch: 966, Loss: 0.9156, Train: 0.9156, Val: 0.9536, Test: 0.9395\n",
      "Epoch: 967, Loss: 0.9156, Train: 0.9155, Val: 0.9535, Test: 0.9395\n",
      "Epoch: 968, Loss: 0.9156, Train: 0.9155, Val: 0.9535, Test: 0.9395\n",
      "Epoch: 969, Loss: 0.9155, Train: 0.9155, Val: 0.9535, Test: 0.9395\n",
      "Epoch: 970, Loss: 0.9155, Train: 0.9154, Val: 0.9535, Test: 0.9394\n",
      "Epoch: 971, Loss: 0.9155, Train: 0.9154, Val: 0.9535, Test: 0.9394\n",
      "Epoch: 972, Loss: 0.9154, Train: 0.9154, Val: 0.9535, Test: 0.9394\n",
      "Epoch: 973, Loss: 0.9154, Train: 0.9153, Val: 0.9535, Test: 0.9394\n",
      "Epoch: 974, Loss: 0.9154, Train: 0.9153, Val: 0.9534, Test: 0.9394\n",
      "Epoch: 975, Loss: 0.9154, Train: 0.9153, Val: 0.9534, Test: 0.9394\n",
      "Epoch: 976, Loss: 0.9153, Train: 0.9153, Val: 0.9534, Test: 0.9394\n",
      "Epoch: 977, Loss: 0.9153, Train: 0.9152, Val: 0.9534, Test: 0.9394\n",
      "Epoch: 978, Loss: 0.9153, Train: 0.9152, Val: 0.9534, Test: 0.9393\n",
      "Epoch: 979, Loss: 0.9152, Train: 0.9152, Val: 0.9534, Test: 0.9393\n",
      "Epoch: 980, Loss: 0.9152, Train: 0.9151, Val: 0.9533, Test: 0.9393\n",
      "Epoch: 981, Loss: 0.9152, Train: 0.9151, Val: 0.9533, Test: 0.9393\n",
      "Epoch: 982, Loss: 0.9151, Train: 0.9151, Val: 0.9533, Test: 0.9393\n",
      "Epoch: 983, Loss: 0.9151, Train: 0.9151, Val: 0.9533, Test: 0.9393\n",
      "Epoch: 984, Loss: 0.9151, Train: 0.9150, Val: 0.9533, Test: 0.9393\n",
      "Epoch: 985, Loss: 0.9151, Train: 0.9150, Val: 0.9533, Test: 0.9393\n",
      "Epoch: 986, Loss: 0.9150, Train: 0.9150, Val: 0.9533, Test: 0.9392\n",
      "Epoch: 987, Loss: 0.9150, Train: 0.9149, Val: 0.9532, Test: 0.9392\n",
      "Epoch: 988, Loss: 0.9150, Train: 0.9149, Val: 0.9532, Test: 0.9392\n",
      "Epoch: 989, Loss: 0.9149, Train: 0.9149, Val: 0.9532, Test: 0.9392\n",
      "Epoch: 990, Loss: 0.9149, Train: 0.9149, Val: 0.9532, Test: 0.9392\n",
      "Epoch: 991, Loss: 0.9149, Train: 0.9148, Val: 0.9532, Test: 0.9392\n",
      "Epoch: 992, Loss: 0.9149, Train: 0.9148, Val: 0.9532, Test: 0.9392\n",
      "Epoch: 993, Loss: 0.9148, Train: 0.9148, Val: 0.9531, Test: 0.9392\n",
      "Epoch: 994, Loss: 0.9148, Train: 0.9147, Val: 0.9531, Test: 0.9391\n",
      "Epoch: 995, Loss: 0.9148, Train: 0.9147, Val: 0.9531, Test: 0.9391\n",
      "Epoch: 996, Loss: 0.9147, Train: 0.9147, Val: 0.9531, Test: 0.9391\n",
      "Epoch: 997, Loss: 0.9147, Train: 0.9147, Val: 0.9531, Test: 0.9391\n",
      "Epoch: 998, Loss: 0.9147, Train: 0.9146, Val: 0.9531, Test: 0.9391\n",
      "Epoch: 999, Loss: 0.9147, Train: 0.9146, Val: 0.9531, Test: 0.9391\n",
      "Epoch: 1000, Loss: 0.9146, Train: 0.9146, Val: 0.9530, Test: 0.9391\n"
     ]
    }
   ],
   "source": [
    "model = Model(layer_name=\"SAGE\", hidden_channels=16, data=data, encoder_num_layers=2,\n",
    "              decoder_num_layers=10, encoder_dropout=0.0, decoder_dropout=0.0, encoder_skip_connections=1)\n",
    "train_test(model, train_data=train_data, test_data=test_data,\n",
    "           val_data=val_data, logging_step=1, epochs=1000, use_weighted_loss=False, lr=0.012)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without rounding with 0.014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGE\n",
      "Aggregation: None\n",
      "Epoch: 001, Loss: 3.5937, Train: 3.5454, Val: 3.5307, Test: 3.5439\n",
      "Epoch: 002, Loss: 3.5454, Train: 3.5076, Val: 3.4929, Test: 3.5061\n",
      "Epoch: 003, Loss: 3.5076, Train: 3.4839, Val: 3.4692, Test: 3.4824\n",
      "Epoch: 004, Loss: 3.4839, Train: 3.4610, Val: 3.4463, Test: 3.4594\n",
      "Epoch: 005, Loss: 3.4610, Train: 3.4387, Val: 3.4241, Test: 3.4372\n",
      "Epoch: 006, Loss: 3.4387, Train: 3.4162, Val: 3.4016, Test: 3.4146\n",
      "Epoch: 007, Loss: 3.4162, Train: 3.3931, Val: 3.3786, Test: 3.3915\n",
      "Epoch: 008, Loss: 3.3931, Train: 3.3693, Val: 3.3547, Test: 3.3677\n",
      "Epoch: 009, Loss: 3.3693, Train: 3.3445, Val: 3.3300, Test: 3.3429\n",
      "Epoch: 010, Loss: 3.3445, Train: 3.3187, Val: 3.3042, Test: 3.3171\n",
      "Epoch: 011, Loss: 3.3187, Train: 3.2915, Val: 3.2771, Test: 3.2899\n",
      "Epoch: 012, Loss: 3.2915, Train: 3.2627, Val: 3.2483, Test: 3.2610\n",
      "Epoch: 013, Loss: 3.2627, Train: 3.2318, Val: 3.2175, Test: 3.2301\n",
      "Epoch: 014, Loss: 3.2318, Train: 3.1985, Val: 3.1842, Test: 3.1968\n",
      "Epoch: 015, Loss: 3.1985, Train: 3.1676, Val: 3.1534, Test: 3.1658\n",
      "Epoch: 016, Loss: 3.1676, Train: 3.1311, Val: 3.1169, Test: 3.1293\n",
      "Epoch: 017, Loss: 3.1311, Train: 3.0863, Val: 3.0722, Test: 3.0845\n",
      "Epoch: 018, Loss: 3.0863, Train: 3.0283, Val: 3.0143, Test: 3.0264\n",
      "Epoch: 019, Loss: 3.0283, Train: 2.9502, Val: 2.9366, Test: 2.9485\n",
      "Epoch: 020, Loss: 2.9502, Train: 2.8370, Val: 2.8240, Test: 2.8355\n",
      "Epoch: 021, Loss: 2.8370, Train: 2.6716, Val: 2.6596, Test: 2.6704\n",
      "Epoch: 022, Loss: 2.6716, Train: 2.4062, Val: 2.3962, Test: 2.4057\n",
      "Epoch: 023, Loss: 2.4062, Train: 1.9619, Val: 1.9565, Test: 1.9628\n",
      "Epoch: 024, Loss: 1.9619, Train: 1.3263, Val: 1.3302, Test: 1.3261\n",
      "Epoch: 025, Loss: 1.3263, Train: 1.4258, Val: 1.4306, Test: 1.4057\n",
      "Epoch: 026, Loss: 1.4258, Train: 1.4836, Val: 1.4875, Test: 1.4626\n",
      "Epoch: 027, Loss: 1.4836, Train: 1.1648, Val: 1.1733, Test: 1.1516\n",
      "Epoch: 028, Loss: 1.1648, Train: 1.1558, Val: 1.1627, Test: 1.1514\n",
      "Epoch: 029, Loss: 1.1558, Train: 1.2831, Val: 1.2867, Test: 1.2815\n",
      "Epoch: 030, Loss: 1.2831, Train: 1.3285, Val: 1.3310, Test: 1.3273\n",
      "Epoch: 031, Loss: 1.3285, Train: 1.2859, Val: 1.2891, Test: 1.2841\n",
      "Epoch: 032, Loss: 1.2859, Train: 1.1902, Val: 1.1955, Test: 1.1866\n",
      "Epoch: 033, Loss: 1.1902, Train: 1.1150, Val: 1.1227, Test: 1.1074\n",
      "Epoch: 034, Loss: 1.1150, Train: 1.1498, Val: 1.1580, Test: 1.1371\n",
      "Epoch: 035, Loss: 1.1498, Train: 1.2243, Val: 1.2316, Test: 1.2088\n",
      "Epoch: 036, Loss: 1.2243, Train: 1.2005, Val: 1.2081, Test: 1.1857\n",
      "Epoch: 037, Loss: 1.2005, Train: 1.1294, Val: 1.1377, Test: 1.1179\n",
      "Epoch: 038, Loss: 1.1294, Train: 1.1109, Val: 1.1186, Test: 1.1030\n",
      "Epoch: 039, Loss: 1.1109, Train: 1.1399, Val: 1.1462, Test: 1.1344\n",
      "Epoch: 040, Loss: 1.1399, Train: 1.1662, Val: 1.1717, Test: 1.1617\n",
      "Epoch: 041, Loss: 1.1662, Train: 1.1665, Val: 1.1720, Test: 1.1621\n",
      "Epoch: 042, Loss: 1.1665, Train: 1.1438, Val: 1.1499, Test: 1.1385\n",
      "Epoch: 043, Loss: 1.1438, Train: 1.1161, Val: 1.1232, Test: 1.1092\n",
      "Epoch: 044, Loss: 1.1161, Train: 1.1062, Val: 1.1142, Test: 1.0972\n",
      "Epoch: 045, Loss: 1.1062, Train: 1.1207, Val: 1.1290, Test: 1.1096\n",
      "Epoch: 046, Loss: 1.1207, Train: 1.1368, Val: 1.1451, Test: 1.1247\n",
      "Epoch: 047, Loss: 1.1368, Train: 1.1323, Val: 1.1406, Test: 1.1204\n",
      "Epoch: 048, Loss: 1.1323, Train: 1.1144, Val: 1.1227, Test: 1.1038\n",
      "Epoch: 049, Loss: 1.1144, Train: 1.1037, Val: 1.1117, Test: 1.0948\n",
      "Epoch: 050, Loss: 1.1037, Train: 1.1069, Val: 1.1144, Test: 1.0995\n",
      "Epoch: 051, Loss: 1.1069, Train: 1.1154, Val: 1.1225, Test: 1.1089\n",
      "Epoch: 052, Loss: 1.1154, Train: 1.1194, Val: 1.1263, Test: 1.1133\n",
      "Epoch: 053, Loss: 1.1194, Train: 1.1157, Val: 1.1228, Test: 1.1093\n",
      "Epoch: 054, Loss: 1.1157, Train: 1.1075, Val: 1.1150, Test: 1.1005\n",
      "Epoch: 055, Loss: 1.1075, Train: 1.1014, Val: 1.1093, Test: 1.0934\n",
      "Epoch: 056, Loss: 1.1014, Train: 1.1014, Val: 1.1097, Test: 1.0924\n",
      "Epoch: 057, Loss: 1.1014, Train: 1.1058, Val: 1.1142, Test: 1.0959\n",
      "Epoch: 058, Loss: 1.1058, Train: 1.1085, Val: 1.1170, Test: 1.0983\n",
      "Epoch: 059, Loss: 1.1085, Train: 1.1062, Val: 1.1147, Test: 1.0962\n",
      "Epoch: 060, Loss: 1.1062, Train: 1.1013, Val: 1.1098, Test: 1.0920\n",
      "Epoch: 061, Loss: 1.1013, Train: 1.0983, Val: 1.1066, Test: 1.0898\n",
      "Epoch: 062, Loss: 1.0983, Train: 1.0987, Val: 1.1068, Test: 1.0910\n",
      "Epoch: 063, Loss: 1.0987, Train: 1.1007, Val: 1.1086, Test: 1.0935\n",
      "Epoch: 064, Loss: 1.1007, Train: 1.1017, Val: 1.1095, Test: 1.0947\n",
      "Epoch: 065, Loss: 1.1017, Train: 1.1006, Val: 1.1084, Test: 1.0935\n",
      "Epoch: 066, Loss: 1.1006, Train: 1.0981, Val: 1.1062, Test: 1.0907\n",
      "Epoch: 067, Loss: 1.0981, Train: 1.0961, Val: 1.1044, Test: 1.0882\n",
      "Epoch: 068, Loss: 1.0961, Train: 1.0956, Val: 1.1041, Test: 1.0873\n",
      "Epoch: 069, Loss: 1.0956, Train: 1.0964, Val: 1.1050, Test: 1.0876\n",
      "Epoch: 070, Loss: 1.0964, Train: 1.0970, Val: 1.1057, Test: 1.0880\n",
      "Epoch: 071, Loss: 1.0970, Train: 1.0965, Val: 1.1052, Test: 1.0876\n",
      "Epoch: 072, Loss: 1.0965, Train: 1.0950, Val: 1.1037, Test: 1.0864\n",
      "Epoch: 073, Loss: 1.0950, Train: 1.0937, Val: 1.1023, Test: 1.0855\n",
      "Epoch: 074, Loss: 1.0937, Train: 1.0932, Val: 1.1017, Test: 1.0854\n",
      "Epoch: 075, Loss: 1.0932, Train: 1.0934, Val: 1.1018, Test: 1.0859\n",
      "Epoch: 076, Loss: 1.0934, Train: 1.0936, Val: 1.1020, Test: 1.0863\n",
      "Epoch: 077, Loss: 1.0936, Train: 1.0932, Val: 1.1017, Test: 1.0860\n",
      "Epoch: 078, Loss: 1.0932, Train: 1.0924, Val: 1.1009, Test: 1.0851\n",
      "Epoch: 079, Loss: 1.0924, Train: 1.0915, Val: 1.1001, Test: 1.0840\n",
      "Epoch: 080, Loss: 1.0915, Train: 1.0910, Val: 1.0997, Test: 1.0831\n",
      "Epoch: 081, Loss: 1.0910, Train: 1.0908, Val: 1.0996, Test: 1.0827\n",
      "Epoch: 082, Loss: 1.0908, Train: 1.0907, Val: 1.0996, Test: 1.0826\n",
      "Epoch: 083, Loss: 1.0907, Train: 1.0905, Val: 1.0994, Test: 1.0823\n",
      "Epoch: 084, Loss: 1.0905, Train: 1.0899, Val: 1.0989, Test: 1.0819\n",
      "Epoch: 085, Loss: 1.0899, Train: 1.0893, Val: 1.0982, Test: 1.0814\n",
      "Epoch: 086, Loss: 1.0893, Train: 1.0888, Val: 1.0977, Test: 1.0811\n",
      "Epoch: 087, Loss: 1.0888, Train: 1.0884, Val: 1.0973, Test: 1.0810\n",
      "Epoch: 088, Loss: 1.0884, Train: 1.0882, Val: 1.0971, Test: 1.0809\n",
      "Epoch: 089, Loss: 1.0882, Train: 1.0880, Val: 1.0968, Test: 1.0808\n",
      "Epoch: 090, Loss: 1.0880, Train: 1.0876, Val: 1.0965, Test: 1.0804\n",
      "Epoch: 091, Loss: 1.0876, Train: 1.0871, Val: 1.0960, Test: 1.0798\n",
      "Epoch: 092, Loss: 1.0871, Train: 1.0866, Val: 1.0956, Test: 1.0792\n",
      "Epoch: 093, Loss: 1.0866, Train: 1.0862, Val: 1.0953, Test: 1.0787\n",
      "Epoch: 094, Loss: 1.0862, Train: 1.0859, Val: 1.0951, Test: 1.0784\n",
      "Epoch: 095, Loss: 1.0859, Train: 1.0856, Val: 1.0948, Test: 1.0780\n",
      "Epoch: 096, Loss: 1.0856, Train: 1.0852, Val: 1.0945, Test: 1.0777\n",
      "Epoch: 097, Loss: 1.0852, Train: 1.0848, Val: 1.0941, Test: 1.0773\n",
      "Epoch: 098, Loss: 1.0848, Train: 1.0844, Val: 1.0936, Test: 1.0770\n",
      "Epoch: 099, Loss: 1.0844, Train: 1.0840, Val: 1.0932, Test: 1.0767\n",
      "Epoch: 100, Loss: 1.0840, Train: 1.0836, Val: 1.0929, Test: 1.0765\n",
      "Epoch: 101, Loss: 1.0836, Train: 1.0833, Val: 1.0926, Test: 1.0763\n",
      "Epoch: 102, Loss: 1.0833, Train: 1.0830, Val: 1.0923, Test: 1.0760\n",
      "Epoch: 103, Loss: 1.0830, Train: 1.0826, Val: 1.0919, Test: 1.0756\n",
      "Epoch: 104, Loss: 1.0826, Train: 1.0822, Val: 1.0916, Test: 1.0752\n",
      "Epoch: 105, Loss: 1.0822, Train: 1.0818, Val: 1.0913, Test: 1.0748\n",
      "Epoch: 106, Loss: 1.0818, Train: 1.0814, Val: 1.0909, Test: 1.0744\n",
      "Epoch: 107, Loss: 1.0814, Train: 1.0811, Val: 1.0907, Test: 1.0740\n",
      "Epoch: 108, Loss: 1.0811, Train: 1.0807, Val: 1.0903, Test: 1.0737\n",
      "Epoch: 109, Loss: 1.0807, Train: 1.0804, Val: 1.0900, Test: 1.0733\n",
      "Epoch: 110, Loss: 1.0804, Train: 1.0800, Val: 1.0897, Test: 1.0730\n",
      "Epoch: 111, Loss: 1.0800, Train: 1.0796, Val: 1.0893, Test: 1.0727\n",
      "Epoch: 112, Loss: 1.0796, Train: 1.0793, Val: 1.0889, Test: 1.0724\n",
      "Epoch: 113, Loss: 1.0793, Train: 1.0789, Val: 1.0886, Test: 1.0721\n",
      "Epoch: 114, Loss: 1.0789, Train: 1.0785, Val: 1.0883, Test: 1.0718\n",
      "Epoch: 115, Loss: 1.0785, Train: 1.0782, Val: 1.0879, Test: 1.0715\n",
      "Epoch: 116, Loss: 1.0782, Train: 1.0778, Val: 1.0876, Test: 1.0712\n",
      "Epoch: 117, Loss: 1.0778, Train: 1.0774, Val: 1.0873, Test: 1.0708\n",
      "Epoch: 118, Loss: 1.0774, Train: 1.0771, Val: 1.0870, Test: 1.0704\n",
      "Epoch: 119, Loss: 1.0771, Train: 1.0767, Val: 1.0866, Test: 1.0701\n",
      "Epoch: 120, Loss: 1.0767, Train: 1.0763, Val: 1.0863, Test: 1.0697\n",
      "Epoch: 121, Loss: 1.0763, Train: 1.0760, Val: 1.0860, Test: 1.0694\n",
      "Epoch: 122, Loss: 1.0760, Train: 1.0756, Val: 1.0856, Test: 1.0690\n",
      "Epoch: 123, Loss: 1.0756, Train: 1.0752, Val: 1.0853, Test: 1.0687\n",
      "Epoch: 124, Loss: 1.0752, Train: 1.0748, Val: 1.0850, Test: 1.0684\n",
      "Epoch: 125, Loss: 1.0748, Train: 1.0745, Val: 1.0846, Test: 1.0681\n",
      "Epoch: 126, Loss: 1.0745, Train: 1.0741, Val: 1.0843, Test: 1.0678\n",
      "Epoch: 127, Loss: 1.0741, Train: 1.0737, Val: 1.0839, Test: 1.0674\n",
      "Epoch: 128, Loss: 1.0737, Train: 1.0734, Val: 1.0836, Test: 1.0671\n",
      "Epoch: 129, Loss: 1.0734, Train: 1.0730, Val: 1.0833, Test: 1.0668\n",
      "Epoch: 130, Loss: 1.0730, Train: 1.0726, Val: 1.0829, Test: 1.0664\n",
      "Epoch: 131, Loss: 1.0726, Train: 1.0722, Val: 1.0826, Test: 1.0661\n",
      "Epoch: 132, Loss: 1.0722, Train: 1.0719, Val: 1.0823, Test: 1.0657\n",
      "Epoch: 133, Loss: 1.0719, Train: 1.0715, Val: 1.0819, Test: 1.0654\n",
      "Epoch: 134, Loss: 1.0715, Train: 1.0711, Val: 1.0816, Test: 1.0650\n",
      "Epoch: 135, Loss: 1.0711, Train: 1.0707, Val: 1.0813, Test: 1.0647\n",
      "Epoch: 136, Loss: 1.0707, Train: 1.0704, Val: 1.0809, Test: 1.0643\n",
      "Epoch: 137, Loss: 1.0704, Train: 1.0700, Val: 1.0806, Test: 1.0640\n",
      "Epoch: 138, Loss: 1.0700, Train: 1.0696, Val: 1.0802, Test: 1.0637\n",
      "Epoch: 139, Loss: 1.0696, Train: 1.0692, Val: 1.0799, Test: 1.0633\n",
      "Epoch: 140, Loss: 1.0692, Train: 1.0688, Val: 1.0795, Test: 1.0630\n",
      "Epoch: 141, Loss: 1.0688, Train: 1.0685, Val: 1.0792, Test: 1.0626\n",
      "Epoch: 142, Loss: 1.0685, Train: 1.0681, Val: 1.0789, Test: 1.0623\n",
      "Epoch: 143, Loss: 1.0681, Train: 1.0677, Val: 1.0785, Test: 1.0620\n",
      "Epoch: 144, Loss: 1.0677, Train: 1.0673, Val: 1.0782, Test: 1.0616\n",
      "Epoch: 145, Loss: 1.0673, Train: 1.0669, Val: 1.0778, Test: 1.0613\n",
      "Epoch: 146, Loss: 1.0669, Train: 1.0666, Val: 1.0775, Test: 1.0609\n",
      "Epoch: 147, Loss: 1.0666, Train: 1.0662, Val: 1.0772, Test: 1.0606\n",
      "Epoch: 148, Loss: 1.0662, Train: 1.0658, Val: 1.0768, Test: 1.0602\n",
      "Epoch: 149, Loss: 1.0658, Train: 1.0654, Val: 1.0765, Test: 1.0599\n",
      "Epoch: 150, Loss: 1.0654, Train: 1.0650, Val: 1.0761, Test: 1.0595\n",
      "Epoch: 151, Loss: 1.0650, Train: 1.0646, Val: 1.0758, Test: 1.0592\n",
      "Epoch: 152, Loss: 1.0646, Train: 1.0643, Val: 1.0755, Test: 1.0589\n",
      "Epoch: 153, Loss: 1.0643, Train: 1.0639, Val: 1.0751, Test: 1.0585\n",
      "Epoch: 154, Loss: 1.0639, Train: 1.0635, Val: 1.0748, Test: 1.0582\n",
      "Epoch: 155, Loss: 1.0635, Train: 1.0631, Val: 1.0744, Test: 1.0578\n",
      "Epoch: 156, Loss: 1.0631, Train: 1.0627, Val: 1.0741, Test: 1.0575\n",
      "Epoch: 157, Loss: 1.0627, Train: 1.0624, Val: 1.0738, Test: 1.0571\n",
      "Epoch: 158, Loss: 1.0624, Train: 1.0620, Val: 1.0734, Test: 1.0568\n",
      "Epoch: 159, Loss: 1.0620, Train: 1.0616, Val: 1.0731, Test: 1.0564\n",
      "Epoch: 160, Loss: 1.0616, Train: 1.0612, Val: 1.0728, Test: 1.0561\n",
      "Epoch: 161, Loss: 1.0612, Train: 1.0608, Val: 1.0724, Test: 1.0558\n",
      "Epoch: 162, Loss: 1.0608, Train: 1.0605, Val: 1.0721, Test: 1.0554\n",
      "Epoch: 163, Loss: 1.0605, Train: 1.0601, Val: 1.0717, Test: 1.0551\n",
      "Epoch: 164, Loss: 1.0601, Train: 1.0597, Val: 1.0714, Test: 1.0547\n",
      "Epoch: 165, Loss: 1.0597, Train: 1.0593, Val: 1.0711, Test: 1.0544\n",
      "Epoch: 166, Loss: 1.0593, Train: 1.0589, Val: 1.0707, Test: 1.0541\n",
      "Epoch: 167, Loss: 1.0589, Train: 1.0586, Val: 1.0704, Test: 1.0537\n",
      "Epoch: 168, Loss: 1.0586, Train: 1.0582, Val: 1.0701, Test: 1.0534\n",
      "Epoch: 169, Loss: 1.0582, Train: 1.0578, Val: 1.0697, Test: 1.0530\n",
      "Epoch: 170, Loss: 1.0578, Train: 1.0574, Val: 1.0694, Test: 1.0527\n",
      "Epoch: 171, Loss: 1.0574, Train: 1.0570, Val: 1.0691, Test: 1.0524\n",
      "Epoch: 172, Loss: 1.0570, Train: 1.0567, Val: 1.0688, Test: 1.0520\n",
      "Epoch: 173, Loss: 1.0567, Train: 1.0563, Val: 1.0684, Test: 1.0517\n",
      "Epoch: 174, Loss: 1.0563, Train: 1.0559, Val: 1.0681, Test: 1.0514\n",
      "Epoch: 175, Loss: 1.0559, Train: 1.0555, Val: 1.0678, Test: 1.0510\n",
      "Epoch: 176, Loss: 1.0555, Train: 1.0552, Val: 1.0675, Test: 1.0507\n",
      "Epoch: 177, Loss: 1.0552, Train: 1.0548, Val: 1.0671, Test: 1.0504\n",
      "Epoch: 178, Loss: 1.0548, Train: 1.0544, Val: 1.0668, Test: 1.0500\n",
      "Epoch: 179, Loss: 1.0544, Train: 1.0541, Val: 1.0665, Test: 1.0497\n",
      "Epoch: 180, Loss: 1.0541, Train: 1.0537, Val: 1.0662, Test: 1.0494\n",
      "Epoch: 181, Loss: 1.0537, Train: 1.0533, Val: 1.0659, Test: 1.0491\n",
      "Epoch: 182, Loss: 1.0533, Train: 1.0530, Val: 1.0656, Test: 1.0487\n",
      "Epoch: 183, Loss: 1.0530, Train: 1.0526, Val: 1.0652, Test: 1.0484\n",
      "Epoch: 184, Loss: 1.0526, Train: 1.0522, Val: 1.0649, Test: 1.0481\n",
      "Epoch: 185, Loss: 1.0522, Train: 1.0519, Val: 1.0646, Test: 1.0478\n",
      "Epoch: 186, Loss: 1.0519, Train: 1.0515, Val: 1.0643, Test: 1.0475\n",
      "Epoch: 187, Loss: 1.0515, Train: 1.0511, Val: 1.0640, Test: 1.0471\n",
      "Epoch: 188, Loss: 1.0511, Train: 1.0508, Val: 1.0637, Test: 1.0468\n",
      "Epoch: 189, Loss: 1.0508, Train: 1.0504, Val: 1.0634, Test: 1.0465\n",
      "Epoch: 190, Loss: 1.0504, Train: 1.0501, Val: 1.0631, Test: 1.0462\n",
      "Epoch: 191, Loss: 1.0501, Train: 1.0497, Val: 1.0628, Test: 1.0459\n",
      "Epoch: 192, Loss: 1.0497, Train: 1.0494, Val: 1.0625, Test: 1.0456\n",
      "Epoch: 193, Loss: 1.0494, Train: 1.0490, Val: 1.0622, Test: 1.0453\n",
      "Epoch: 194, Loss: 1.0490, Train: 1.0487, Val: 1.0619, Test: 1.0450\n",
      "Epoch: 195, Loss: 1.0487, Train: 1.0483, Val: 1.0616, Test: 1.0447\n",
      "Epoch: 196, Loss: 1.0483, Train: 1.0480, Val: 1.0613, Test: 1.0444\n",
      "Epoch: 197, Loss: 1.0480, Train: 1.0477, Val: 1.0611, Test: 1.0441\n",
      "Epoch: 198, Loss: 1.0477, Train: 1.0473, Val: 1.0608, Test: 1.0438\n",
      "Epoch: 199, Loss: 1.0473, Train: 1.0470, Val: 1.0605, Test: 1.0435\n",
      "Epoch: 200, Loss: 1.0470, Train: 1.0467, Val: 1.0602, Test: 1.0432\n",
      "Epoch: 201, Loss: 1.0467, Train: 1.0463, Val: 1.0599, Test: 1.0429\n",
      "Epoch: 202, Loss: 1.0463, Train: 1.0460, Val: 1.0597, Test: 1.0426\n",
      "Epoch: 203, Loss: 1.0460, Train: 1.0457, Val: 1.0594, Test: 1.0423\n",
      "Epoch: 204, Loss: 1.0457, Train: 1.0453, Val: 1.0591, Test: 1.0420\n",
      "Epoch: 205, Loss: 1.0453, Train: 1.0450, Val: 1.0588, Test: 1.0418\n",
      "Epoch: 206, Loss: 1.0450, Train: 1.0447, Val: 1.0586, Test: 1.0415\n",
      "Epoch: 207, Loss: 1.0447, Train: 1.0444, Val: 1.0583, Test: 1.0412\n",
      "Epoch: 208, Loss: 1.0444, Train: 1.0440, Val: 1.0580, Test: 1.0409\n",
      "Epoch: 209, Loss: 1.0440, Train: 1.0437, Val: 1.0578, Test: 1.0407\n",
      "Epoch: 210, Loss: 1.0437, Train: 1.0434, Val: 1.0575, Test: 1.0404\n",
      "Epoch: 211, Loss: 1.0434, Train: 1.0431, Val: 1.0573, Test: 1.0401\n",
      "Epoch: 212, Loss: 1.0431, Train: 1.0428, Val: 1.0570, Test: 1.0398\n",
      "Epoch: 213, Loss: 1.0428, Train: 1.0425, Val: 1.0568, Test: 1.0396\n",
      "Epoch: 214, Loss: 1.0425, Train: 1.0422, Val: 1.0565, Test: 1.0393\n",
      "Epoch: 215, Loss: 1.0422, Train: 1.0419, Val: 1.0562, Test: 1.0390\n",
      "Epoch: 216, Loss: 1.0419, Train: 1.0416, Val: 1.0560, Test: 1.0388\n",
      "Epoch: 217, Loss: 1.0416, Train: 1.0413, Val: 1.0558, Test: 1.0385\n",
      "Epoch: 218, Loss: 1.0413, Train: 1.0410, Val: 1.0555, Test: 1.0383\n",
      "Epoch: 219, Loss: 1.0410, Train: 1.0407, Val: 1.0553, Test: 1.0380\n",
      "Epoch: 220, Loss: 1.0407, Train: 1.0404, Val: 1.0550, Test: 1.0378\n",
      "Epoch: 221, Loss: 1.0404, Train: 1.0401, Val: 1.0548, Test: 1.0375\n",
      "Epoch: 222, Loss: 1.0401, Train: 1.0398, Val: 1.0546, Test: 1.0372\n",
      "Epoch: 223, Loss: 1.0398, Train: 1.0395, Val: 1.0543, Test: 1.0370\n",
      "Epoch: 224, Loss: 1.0395, Train: 1.0392, Val: 1.0541, Test: 1.0368\n",
      "Epoch: 225, Loss: 1.0392, Train: 1.0389, Val: 1.0539, Test: 1.0365\n",
      "Epoch: 226, Loss: 1.0389, Train: 1.0386, Val: 1.0536, Test: 1.0363\n",
      "Epoch: 227, Loss: 1.0386, Train: 1.0383, Val: 1.0534, Test: 1.0360\n",
      "Epoch: 228, Loss: 1.0383, Train: 1.0381, Val: 1.0532, Test: 1.0358\n",
      "Epoch: 229, Loss: 1.0381, Train: 1.0378, Val: 1.0529, Test: 1.0356\n",
      "Epoch: 230, Loss: 1.0378, Train: 1.0375, Val: 1.0527, Test: 1.0353\n",
      "Epoch: 231, Loss: 1.0375, Train: 1.0372, Val: 1.0525, Test: 1.0351\n",
      "Epoch: 232, Loss: 1.0372, Train: 1.0369, Val: 1.0523, Test: 1.0348\n",
      "Epoch: 233, Loss: 1.0369, Train: 1.0367, Val: 1.0521, Test: 1.0346\n",
      "Epoch: 234, Loss: 1.0367, Train: 1.0364, Val: 1.0518, Test: 1.0344\n",
      "Epoch: 235, Loss: 1.0364, Train: 1.0361, Val: 1.0516, Test: 1.0342\n",
      "Epoch: 236, Loss: 1.0361, Train: 1.0358, Val: 1.0514, Test: 1.0339\n",
      "Epoch: 237, Loss: 1.0358, Train: 1.0356, Val: 1.0512, Test: 1.0337\n",
      "Epoch: 238, Loss: 1.0356, Train: 1.0353, Val: 1.0510, Test: 1.0335\n",
      "Epoch: 239, Loss: 1.0353, Train: 1.0350, Val: 1.0507, Test: 1.0333\n",
      "Epoch: 240, Loss: 1.0350, Train: 1.0347, Val: 1.0505, Test: 1.0330\n",
      "Epoch: 241, Loss: 1.0347, Train: 1.0345, Val: 1.0503, Test: 1.0328\n",
      "Epoch: 242, Loss: 1.0345, Train: 1.0342, Val: 1.0501, Test: 1.0326\n",
      "Epoch: 243, Loss: 1.0342, Train: 1.0339, Val: 1.0499, Test: 1.0324\n",
      "Epoch: 244, Loss: 1.0339, Train: 1.0337, Val: 1.0497, Test: 1.0321\n",
      "Epoch: 245, Loss: 1.0337, Train: 1.0334, Val: 1.0495, Test: 1.0319\n",
      "Epoch: 246, Loss: 1.0334, Train: 1.0331, Val: 1.0493, Test: 1.0317\n",
      "Epoch: 247, Loss: 1.0331, Train: 1.0329, Val: 1.0490, Test: 1.0315\n",
      "Epoch: 248, Loss: 1.0329, Train: 1.0326, Val: 1.0488, Test: 1.0313\n",
      "Epoch: 249, Loss: 1.0326, Train: 1.0323, Val: 1.0486, Test: 1.0311\n",
      "Epoch: 250, Loss: 1.0323, Train: 1.0321, Val: 1.0484, Test: 1.0309\n",
      "Epoch: 251, Loss: 1.0321, Train: 1.0318, Val: 1.0482, Test: 1.0306\n",
      "Epoch: 252, Loss: 1.0318, Train: 1.0315, Val: 1.0480, Test: 1.0304\n",
      "Epoch: 253, Loss: 1.0315, Train: 1.0313, Val: 1.0478, Test: 1.0302\n",
      "Epoch: 254, Loss: 1.0313, Train: 1.0310, Val: 1.0476, Test: 1.0300\n",
      "Epoch: 255, Loss: 1.0310, Train: 1.0308, Val: 1.0474, Test: 1.0298\n",
      "Epoch: 256, Loss: 1.0308, Train: 1.0305, Val: 1.0472, Test: 1.0296\n",
      "Epoch: 257, Loss: 1.0305, Train: 1.0302, Val: 1.0470, Test: 1.0294\n",
      "Epoch: 258, Loss: 1.0302, Train: 1.0300, Val: 1.0468, Test: 1.0292\n",
      "Epoch: 259, Loss: 1.0300, Train: 1.0297, Val: 1.0466, Test: 1.0290\n",
      "Epoch: 260, Loss: 1.0297, Train: 1.0295, Val: 1.0463, Test: 1.0288\n",
      "Epoch: 261, Loss: 1.0295, Train: 1.0292, Val: 1.0461, Test: 1.0286\n",
      "Epoch: 262, Loss: 1.0292, Train: 1.0289, Val: 1.0459, Test: 1.0284\n",
      "Epoch: 263, Loss: 1.0289, Train: 1.0287, Val: 1.0457, Test: 1.0281\n",
      "Epoch: 264, Loss: 1.0287, Train: 1.0284, Val: 1.0455, Test: 1.0279\n",
      "Epoch: 265, Loss: 1.0284, Train: 1.0282, Val: 1.0453, Test: 1.0277\n",
      "Epoch: 266, Loss: 1.0282, Train: 1.0279, Val: 1.0451, Test: 1.0275\n",
      "Epoch: 267, Loss: 1.0279, Train: 1.0276, Val: 1.0449, Test: 1.0273\n",
      "Epoch: 268, Loss: 1.0276, Train: 1.0274, Val: 1.0447, Test: 1.0271\n",
      "Epoch: 269, Loss: 1.0274, Train: 1.0271, Val: 1.0445, Test: 1.0269\n",
      "Epoch: 270, Loss: 1.0271, Train: 1.0269, Val: 1.0443, Test: 1.0267\n",
      "Epoch: 271, Loss: 1.0269, Train: 1.0266, Val: 1.0441, Test: 1.0265\n",
      "Epoch: 272, Loss: 1.0266, Train: 1.0263, Val: 1.0439, Test: 1.0263\n",
      "Epoch: 273, Loss: 1.0263, Train: 1.0261, Val: 1.0437, Test: 1.0261\n",
      "Epoch: 274, Loss: 1.0261, Train: 1.0258, Val: 1.0435, Test: 1.0259\n",
      "Epoch: 275, Loss: 1.0258, Train: 1.0256, Val: 1.0433, Test: 1.0257\n",
      "Epoch: 276, Loss: 1.0256, Train: 1.0253, Val: 1.0431, Test: 1.0255\n",
      "Epoch: 277, Loss: 1.0253, Train: 1.0250, Val: 1.0429, Test: 1.0253\n",
      "Epoch: 278, Loss: 1.0250, Train: 1.0248, Val: 1.0427, Test: 1.0251\n",
      "Epoch: 279, Loss: 1.0248, Train: 1.0245, Val: 1.0424, Test: 1.0249\n",
      "Epoch: 280, Loss: 1.0245, Train: 1.0242, Val: 1.0422, Test: 1.0247\n",
      "Epoch: 281, Loss: 1.0242, Train: 1.0240, Val: 1.0420, Test: 1.0245\n",
      "Epoch: 282, Loss: 1.0240, Train: 1.0237, Val: 1.0418, Test: 1.0243\n",
      "Epoch: 283, Loss: 1.0237, Train: 1.0235, Val: 1.0416, Test: 1.0241\n",
      "Epoch: 284, Loss: 1.0235, Train: 1.0232, Val: 1.0414, Test: 1.0239\n",
      "Epoch: 285, Loss: 1.0232, Train: 1.0229, Val: 1.0412, Test: 1.0237\n",
      "Epoch: 286, Loss: 1.0229, Train: 1.0227, Val: 1.0410, Test: 1.0235\n",
      "Epoch: 287, Loss: 1.0227, Train: 1.0224, Val: 1.0408, Test: 1.0233\n",
      "Epoch: 288, Loss: 1.0224, Train: 1.0221, Val: 1.0406, Test: 1.0231\n",
      "Epoch: 289, Loss: 1.0221, Train: 1.0219, Val: 1.0403, Test: 1.0229\n",
      "Epoch: 290, Loss: 1.0219, Train: 1.0216, Val: 1.0401, Test: 1.0227\n",
      "Epoch: 291, Loss: 1.0216, Train: 1.0213, Val: 1.0399, Test: 1.0225\n",
      "Epoch: 292, Loss: 1.0213, Train: 1.0211, Val: 1.0397, Test: 1.0222\n",
      "Epoch: 293, Loss: 1.0211, Train: 1.0208, Val: 1.0395, Test: 1.0220\n",
      "Epoch: 294, Loss: 1.0208, Train: 1.0205, Val: 1.0393, Test: 1.0218\n",
      "Epoch: 295, Loss: 1.0205, Train: 1.0202, Val: 1.0391, Test: 1.0216\n",
      "Epoch: 296, Loss: 1.0202, Train: 1.0200, Val: 1.0388, Test: 1.0214\n",
      "Epoch: 297, Loss: 1.0200, Train: 1.0197, Val: 1.0386, Test: 1.0212\n",
      "Epoch: 298, Loss: 1.0197, Train: 1.0194, Val: 1.0384, Test: 1.0210\n",
      "Epoch: 299, Loss: 1.0194, Train: 1.0191, Val: 1.0382, Test: 1.0208\n",
      "Epoch: 300, Loss: 1.0191, Train: 1.0189, Val: 1.0380, Test: 1.0206\n",
      "Epoch: 301, Loss: 1.0189, Train: 1.0186, Val: 1.0377, Test: 1.0204\n",
      "Epoch: 302, Loss: 1.0186, Train: 1.0183, Val: 1.0375, Test: 1.0202\n",
      "Epoch: 303, Loss: 1.0183, Train: 1.0180, Val: 1.0373, Test: 1.0199\n",
      "Epoch: 304, Loss: 1.0180, Train: 1.0177, Val: 1.0371, Test: 1.0197\n",
      "Epoch: 305, Loss: 1.0177, Train: 1.0175, Val: 1.0369, Test: 1.0195\n",
      "Epoch: 306, Loss: 1.0175, Train: 1.0172, Val: 1.0366, Test: 1.0193\n",
      "Epoch: 307, Loss: 1.0172, Train: 1.0169, Val: 1.0364, Test: 1.0191\n",
      "Epoch: 308, Loss: 1.0169, Train: 1.0166, Val: 1.0362, Test: 1.0189\n",
      "Epoch: 309, Loss: 1.0166, Train: 1.0163, Val: 1.0359, Test: 1.0186\n",
      "Epoch: 310, Loss: 1.0163, Train: 1.0160, Val: 1.0357, Test: 1.0184\n",
      "Epoch: 311, Loss: 1.0160, Train: 1.0158, Val: 1.0355, Test: 1.0182\n",
      "Epoch: 312, Loss: 1.0158, Train: 1.0155, Val: 1.0353, Test: 1.0180\n",
      "Epoch: 313, Loss: 1.0155, Train: 1.0152, Val: 1.0350, Test: 1.0178\n",
      "Epoch: 314, Loss: 1.0152, Train: 1.0149, Val: 1.0348, Test: 1.0175\n",
      "Epoch: 315, Loss: 1.0149, Train: 1.0146, Val: 1.0346, Test: 1.0173\n",
      "Epoch: 316, Loss: 1.0146, Train: 1.0143, Val: 1.0343, Test: 1.0171\n",
      "Epoch: 317, Loss: 1.0143, Train: 1.0140, Val: 1.0341, Test: 1.0169\n",
      "Epoch: 318, Loss: 1.0140, Train: 1.0137, Val: 1.0338, Test: 1.0167\n",
      "Epoch: 319, Loss: 1.0137, Train: 1.0134, Val: 1.0336, Test: 1.0164\n",
      "Epoch: 320, Loss: 1.0134, Train: 1.0131, Val: 1.0334, Test: 1.0162\n",
      "Epoch: 321, Loss: 1.0131, Train: 1.0128, Val: 1.0331, Test: 1.0160\n",
      "Epoch: 322, Loss: 1.0128, Train: 1.0125, Val: 1.0329, Test: 1.0157\n",
      "Epoch: 323, Loss: 1.0125, Train: 1.0122, Val: 1.0326, Test: 1.0155\n",
      "Epoch: 324, Loss: 1.0122, Train: 1.0119, Val: 1.0324, Test: 1.0153\n",
      "Epoch: 325, Loss: 1.0119, Train: 1.0116, Val: 1.0322, Test: 1.0151\n",
      "Epoch: 326, Loss: 1.0116, Train: 1.0113, Val: 1.0319, Test: 1.0148\n",
      "Epoch: 327, Loss: 1.0113, Train: 1.0110, Val: 1.0317, Test: 1.0146\n",
      "Epoch: 328, Loss: 1.0110, Train: 1.0107, Val: 1.0314, Test: 1.0143\n",
      "Epoch: 329, Loss: 1.0107, Train: 1.0104, Val: 1.0312, Test: 1.0141\n",
      "Epoch: 330, Loss: 1.0104, Train: 1.0101, Val: 1.0309, Test: 1.0139\n",
      "Epoch: 331, Loss: 1.0101, Train: 1.0098, Val: 1.0307, Test: 1.0136\n",
      "Epoch: 332, Loss: 1.0098, Train: 1.0095, Val: 1.0304, Test: 1.0134\n",
      "Epoch: 333, Loss: 1.0095, Train: 1.0091, Val: 1.0302, Test: 1.0132\n",
      "Epoch: 334, Loss: 1.0091, Train: 1.0088, Val: 1.0299, Test: 1.0129\n",
      "Epoch: 335, Loss: 1.0088, Train: 1.0085, Val: 1.0297, Test: 1.0127\n",
      "Epoch: 336, Loss: 1.0085, Train: 1.0082, Val: 1.0294, Test: 1.0124\n",
      "Epoch: 337, Loss: 1.0082, Train: 1.0079, Val: 1.0292, Test: 1.0122\n",
      "Epoch: 338, Loss: 1.0079, Train: 1.0076, Val: 1.0289, Test: 1.0119\n",
      "Epoch: 339, Loss: 1.0076, Train: 1.0073, Val: 1.0287, Test: 1.0117\n",
      "Epoch: 340, Loss: 1.0073, Train: 1.0069, Val: 1.0284, Test: 1.0114\n",
      "Epoch: 341, Loss: 1.0069, Train: 1.0066, Val: 1.0281, Test: 1.0112\n",
      "Epoch: 342, Loss: 1.0066, Train: 1.0062, Val: 1.0278, Test: 1.0109\n",
      "Epoch: 343, Loss: 1.0062, Train: 1.0058, Val: 1.0275, Test: 1.0106\n",
      "Epoch: 344, Loss: 1.0058, Train: 1.0055, Val: 1.0272, Test: 1.0103\n",
      "Epoch: 345, Loss: 1.0055, Train: 1.0052, Val: 1.0270, Test: 1.0100\n",
      "Epoch: 346, Loss: 1.0052, Train: 1.0048, Val: 1.0267, Test: 1.0097\n",
      "Epoch: 347, Loss: 1.0048, Train: 1.0045, Val: 1.0264, Test: 1.0094\n",
      "Epoch: 348, Loss: 1.0045, Train: 1.0041, Val: 1.0261, Test: 1.0092\n",
      "Epoch: 349, Loss: 1.0041, Train: 1.0037, Val: 1.0258, Test: 1.0089\n",
      "Epoch: 350, Loss: 1.0037, Train: 1.0034, Val: 1.0255, Test: 1.0086\n",
      "Epoch: 351, Loss: 1.0034, Train: 1.0030, Val: 1.0251, Test: 1.0083\n",
      "Epoch: 352, Loss: 1.0030, Train: 1.0026, Val: 1.0248, Test: 1.0080\n",
      "Epoch: 353, Loss: 1.0026, Train: 1.0022, Val: 1.0245, Test: 1.0077\n",
      "Epoch: 354, Loss: 1.0022, Train: 1.0018, Val: 1.0242, Test: 1.0073\n",
      "Epoch: 355, Loss: 1.0018, Train: 1.0014, Val: 1.0238, Test: 1.0070\n",
      "Epoch: 356, Loss: 1.0014, Train: 1.0010, Val: 1.0235, Test: 1.0067\n",
      "Epoch: 357, Loss: 1.0010, Train: 1.0006, Val: 1.0231, Test: 1.0063\n",
      "Epoch: 358, Loss: 1.0006, Train: 1.0002, Val: 1.0228, Test: 1.0060\n",
      "Epoch: 359, Loss: 1.0002, Train: 0.9998, Val: 1.0224, Test: 1.0056\n",
      "Epoch: 360, Loss: 0.9998, Train: 0.9993, Val: 1.0220, Test: 1.0053\n",
      "Epoch: 361, Loss: 0.9993, Train: 0.9989, Val: 1.0216, Test: 1.0049\n",
      "Epoch: 362, Loss: 0.9989, Train: 0.9985, Val: 1.0212, Test: 1.0046\n",
      "Epoch: 363, Loss: 0.9985, Train: 0.9980, Val: 1.0209, Test: 1.0042\n",
      "Epoch: 364, Loss: 0.9980, Train: 0.9976, Val: 1.0205, Test: 1.0038\n",
      "Epoch: 365, Loss: 0.9976, Train: 0.9971, Val: 1.0201, Test: 1.0034\n",
      "Epoch: 366, Loss: 0.9971, Train: 0.9967, Val: 1.0197, Test: 1.0030\n",
      "Epoch: 367, Loss: 0.9967, Train: 0.9962, Val: 1.0192, Test: 1.0026\n",
      "Epoch: 368, Loss: 0.9962, Train: 0.9957, Val: 1.0188, Test: 1.0022\n",
      "Epoch: 369, Loss: 0.9957, Train: 0.9953, Val: 1.0184, Test: 1.0018\n",
      "Epoch: 370, Loss: 0.9953, Train: 0.9948, Val: 1.0180, Test: 1.0014\n",
      "Epoch: 371, Loss: 0.9948, Train: 0.9943, Val: 1.0175, Test: 1.0010\n",
      "Epoch: 372, Loss: 0.9943, Train: 0.9938, Val: 1.0171, Test: 1.0006\n",
      "Epoch: 373, Loss: 0.9938, Train: 0.9933, Val: 1.0166, Test: 1.0002\n",
      "Epoch: 374, Loss: 0.9933, Train: 0.9928, Val: 1.0162, Test: 0.9997\n",
      "Epoch: 375, Loss: 0.9928, Train: 0.9923, Val: 1.0157, Test: 0.9993\n",
      "Epoch: 376, Loss: 0.9923, Train: 0.9918, Val: 1.0153, Test: 0.9989\n",
      "Epoch: 377, Loss: 0.9918, Train: 0.9913, Val: 1.0148, Test: 0.9984\n",
      "Epoch: 378, Loss: 0.9913, Train: 0.9907, Val: 1.0143, Test: 0.9980\n",
      "Epoch: 379, Loss: 0.9907, Train: 0.9902, Val: 1.0139, Test: 0.9975\n",
      "Epoch: 380, Loss: 0.9902, Train: 0.9897, Val: 1.0134, Test: 0.9971\n",
      "Epoch: 381, Loss: 0.9897, Train: 0.9892, Val: 1.0129, Test: 0.9966\n",
      "Epoch: 382, Loss: 0.9892, Train: 0.9886, Val: 1.0124, Test: 0.9961\n",
      "Epoch: 383, Loss: 0.9886, Train: 0.9881, Val: 1.0119, Test: 0.9957\n",
      "Epoch: 384, Loss: 0.9881, Train: 0.9875, Val: 1.0114, Test: 0.9952\n",
      "Epoch: 385, Loss: 0.9875, Train: 0.9870, Val: 1.0109, Test: 0.9947\n",
      "Epoch: 386, Loss: 0.9870, Train: 0.9864, Val: 1.0104, Test: 0.9943\n",
      "Epoch: 387, Loss: 0.9864, Train: 0.9859, Val: 1.0099, Test: 0.9938\n",
      "Epoch: 388, Loss: 0.9859, Train: 0.9853, Val: 1.0094, Test: 0.9933\n",
      "Epoch: 389, Loss: 0.9853, Train: 0.9847, Val: 1.0089, Test: 0.9928\n",
      "Epoch: 390, Loss: 0.9847, Train: 0.9841, Val: 1.0084, Test: 0.9923\n",
      "Epoch: 391, Loss: 0.9841, Train: 0.9836, Val: 1.0079, Test: 0.9918\n",
      "Epoch: 392, Loss: 0.9836, Train: 0.9830, Val: 1.0074, Test: 0.9913\n",
      "Epoch: 393, Loss: 0.9830, Train: 0.9824, Val: 1.0068, Test: 0.9908\n",
      "Epoch: 394, Loss: 0.9824, Train: 0.9818, Val: 1.0063, Test: 0.9903\n",
      "Epoch: 395, Loss: 0.9818, Train: 0.9812, Val: 1.0058, Test: 0.9898\n",
      "Epoch: 396, Loss: 0.9812, Train: 0.9806, Val: 1.0053, Test: 0.9893\n",
      "Epoch: 397, Loss: 0.9806, Train: 0.9801, Val: 1.0048, Test: 0.9888\n",
      "Epoch: 398, Loss: 0.9801, Train: 0.9795, Val: 1.0043, Test: 0.9883\n",
      "Epoch: 399, Loss: 0.9795, Train: 0.9789, Val: 1.0037, Test: 0.9878\n",
      "Epoch: 400, Loss: 0.9789, Train: 0.9783, Val: 1.0032, Test: 0.9873\n",
      "Epoch: 401, Loss: 0.9783, Train: 0.9777, Val: 1.0027, Test: 0.9868\n",
      "Epoch: 402, Loss: 0.9777, Train: 0.9771, Val: 1.0022, Test: 0.9863\n",
      "Epoch: 403, Loss: 0.9771, Train: 0.9765, Val: 1.0017, Test: 0.9858\n",
      "Epoch: 404, Loss: 0.9765, Train: 0.9759, Val: 1.0011, Test: 0.9853\n",
      "Epoch: 405, Loss: 0.9759, Train: 0.9753, Val: 1.0006, Test: 0.9848\n",
      "Epoch: 406, Loss: 0.9753, Train: 0.9747, Val: 1.0001, Test: 0.9843\n",
      "Epoch: 407, Loss: 0.9747, Train: 0.9741, Val: 0.9995, Test: 0.9837\n",
      "Epoch: 408, Loss: 0.9741, Train: 0.9735, Val: 0.9990, Test: 0.9832\n",
      "Epoch: 409, Loss: 0.9735, Train: 0.9728, Val: 0.9985, Test: 0.9827\n",
      "Epoch: 410, Loss: 0.9728, Train: 0.9722, Val: 0.9980, Test: 0.9822\n",
      "Epoch: 411, Loss: 0.9722, Train: 0.9716, Val: 0.9974, Test: 0.9816\n",
      "Epoch: 412, Loss: 0.9716, Train: 0.9710, Val: 0.9969, Test: 0.9811\n",
      "Epoch: 413, Loss: 0.9710, Train: 0.9704, Val: 0.9964, Test: 0.9806\n",
      "Epoch: 414, Loss: 0.9704, Train: 0.9697, Val: 0.9958, Test: 0.9801\n",
      "Epoch: 415, Loss: 0.9697, Train: 0.9691, Val: 0.9953, Test: 0.9795\n",
      "Epoch: 416, Loss: 0.9691, Train: 0.9685, Val: 0.9948, Test: 0.9790\n",
      "Epoch: 417, Loss: 0.9685, Train: 0.9679, Val: 0.9942, Test: 0.9785\n",
      "Epoch: 418, Loss: 0.9679, Train: 0.9673, Val: 0.9937, Test: 0.9780\n",
      "Epoch: 419, Loss: 0.9673, Train: 0.9666, Val: 0.9932, Test: 0.9775\n",
      "Epoch: 420, Loss: 0.9666, Train: 0.9660, Val: 0.9927, Test: 0.9770\n",
      "Epoch: 421, Loss: 0.9660, Train: 0.9654, Val: 0.9921, Test: 0.9764\n",
      "Epoch: 422, Loss: 0.9654, Train: 0.9648, Val: 0.9916, Test: 0.9759\n",
      "Epoch: 423, Loss: 0.9648, Train: 0.9641, Val: 0.9911, Test: 0.9754\n",
      "Epoch: 424, Loss: 0.9641, Train: 0.9635, Val: 0.9906, Test: 0.9749\n",
      "Epoch: 425, Loss: 0.9635, Train: 0.9629, Val: 0.9901, Test: 0.9744\n",
      "Epoch: 426, Loss: 0.9629, Train: 0.9623, Val: 0.9896, Test: 0.9739\n",
      "Epoch: 427, Loss: 0.9623, Train: 0.9617, Val: 0.9891, Test: 0.9734\n",
      "Epoch: 428, Loss: 0.9617, Train: 0.9611, Val: 0.9886, Test: 0.9729\n",
      "Epoch: 429, Loss: 0.9611, Train: 0.9604, Val: 0.9881, Test: 0.9724\n",
      "Epoch: 430, Loss: 0.9604, Train: 0.9598, Val: 0.9876, Test: 0.9719\n",
      "Epoch: 431, Loss: 0.9598, Train: 0.9592, Val: 0.9871, Test: 0.9714\n",
      "Epoch: 432, Loss: 0.9592, Train: 0.9586, Val: 0.9866, Test: 0.9709\n",
      "Epoch: 433, Loss: 0.9586, Train: 0.9580, Val: 0.9862, Test: 0.9704\n",
      "Epoch: 434, Loss: 0.9580, Train: 0.9574, Val: 0.9857, Test: 0.9699\n",
      "Epoch: 435, Loss: 0.9574, Train: 0.9569, Val: 0.9852, Test: 0.9694\n",
      "Epoch: 436, Loss: 0.9569, Train: 0.9563, Val: 0.9847, Test: 0.9689\n",
      "Epoch: 437, Loss: 0.9563, Train: 0.9557, Val: 0.9843, Test: 0.9685\n",
      "Epoch: 438, Loss: 0.9557, Train: 0.9551, Val: 0.9838, Test: 0.9680\n",
      "Epoch: 439, Loss: 0.9551, Train: 0.9545, Val: 0.9834, Test: 0.9675\n",
      "Epoch: 440, Loss: 0.9545, Train: 0.9540, Val: 0.9829, Test: 0.9671\n",
      "Epoch: 441, Loss: 0.9540, Train: 0.9534, Val: 0.9825, Test: 0.9666\n",
      "Epoch: 442, Loss: 0.9534, Train: 0.9528, Val: 0.9820, Test: 0.9662\n",
      "Epoch: 443, Loss: 0.9528, Train: 0.9523, Val: 0.9816, Test: 0.9657\n",
      "Epoch: 444, Loss: 0.9523, Train: 0.9517, Val: 0.9811, Test: 0.9653\n",
      "Epoch: 445, Loss: 0.9517, Train: 0.9512, Val: 0.9807, Test: 0.9648\n",
      "Epoch: 446, Loss: 0.9512, Train: 0.9506, Val: 0.9803, Test: 0.9644\n",
      "Epoch: 447, Loss: 0.9506, Train: 0.9501, Val: 0.9799, Test: 0.9640\n",
      "Epoch: 448, Loss: 0.9501, Train: 0.9495, Val: 0.9794, Test: 0.9635\n",
      "Epoch: 449, Loss: 0.9495, Train: 0.9490, Val: 0.9790, Test: 0.9631\n",
      "Epoch: 450, Loss: 0.9490, Train: 0.9485, Val: 0.9786, Test: 0.9627\n",
      "Epoch: 451, Loss: 0.9485, Train: 0.9480, Val: 0.9782, Test: 0.9623\n",
      "Epoch: 452, Loss: 0.9480, Train: 0.9474, Val: 0.9778, Test: 0.9619\n",
      "Epoch: 453, Loss: 0.9474, Train: 0.9469, Val: 0.9775, Test: 0.9615\n",
      "Epoch: 454, Loss: 0.9469, Train: 0.9464, Val: 0.9771, Test: 0.9611\n",
      "Epoch: 455, Loss: 0.9464, Train: 0.9459, Val: 0.9767, Test: 0.9607\n",
      "Epoch: 456, Loss: 0.9459, Train: 0.9454, Val: 0.9763, Test: 0.9603\n",
      "Epoch: 457, Loss: 0.9454, Train: 0.9450, Val: 0.9760, Test: 0.9600\n",
      "Epoch: 458, Loss: 0.9450, Train: 0.9445, Val: 0.9756, Test: 0.9596\n",
      "Epoch: 459, Loss: 0.9445, Train: 0.9440, Val: 0.9753, Test: 0.9593\n",
      "Epoch: 460, Loss: 0.9440, Train: 0.9436, Val: 0.9750, Test: 0.9589\n",
      "Epoch: 461, Loss: 0.9436, Train: 0.9431, Val: 0.9746, Test: 0.9586\n",
      "Epoch: 462, Loss: 0.9431, Train: 0.9426, Val: 0.9743, Test: 0.9582\n",
      "Epoch: 463, Loss: 0.9426, Train: 0.9422, Val: 0.9740, Test: 0.9579\n",
      "Epoch: 464, Loss: 0.9422, Train: 0.9418, Val: 0.9737, Test: 0.9576\n",
      "Epoch: 465, Loss: 0.9418, Train: 0.9413, Val: 0.9734, Test: 0.9573\n",
      "Epoch: 466, Loss: 0.9413, Train: 0.9409, Val: 0.9731, Test: 0.9570\n",
      "Epoch: 467, Loss: 0.9409, Train: 0.9405, Val: 0.9728, Test: 0.9567\n",
      "Epoch: 468, Loss: 0.9405, Train: 0.9401, Val: 0.9725, Test: 0.9564\n",
      "Epoch: 469, Loss: 0.9401, Train: 0.9397, Val: 0.9722, Test: 0.9561\n",
      "Epoch: 470, Loss: 0.9397, Train: 0.9393, Val: 0.9719, Test: 0.9558\n",
      "Epoch: 471, Loss: 0.9393, Train: 0.9389, Val: 0.9717, Test: 0.9555\n",
      "Epoch: 472, Loss: 0.9389, Train: 0.9385, Val: 0.9714, Test: 0.9553\n",
      "Epoch: 473, Loss: 0.9385, Train: 0.9381, Val: 0.9711, Test: 0.9550\n",
      "Epoch: 474, Loss: 0.9381, Train: 0.9377, Val: 0.9708, Test: 0.9547\n",
      "Epoch: 475, Loss: 0.9377, Train: 0.9374, Val: 0.9706, Test: 0.9545\n",
      "Epoch: 476, Loss: 0.9374, Train: 0.9370, Val: 0.9703, Test: 0.9542\n",
      "Epoch: 477, Loss: 0.9370, Train: 0.9367, Val: 0.9701, Test: 0.9540\n",
      "Epoch: 478, Loss: 0.9367, Train: 0.9363, Val: 0.9698, Test: 0.9537\n",
      "Epoch: 479, Loss: 0.9363, Train: 0.9360, Val: 0.9696, Test: 0.9535\n",
      "Epoch: 480, Loss: 0.9360, Train: 0.9356, Val: 0.9693, Test: 0.9532\n",
      "Epoch: 481, Loss: 0.9356, Train: 0.9353, Val: 0.9691, Test: 0.9530\n",
      "Epoch: 482, Loss: 0.9353, Train: 0.9350, Val: 0.9688, Test: 0.9528\n",
      "Epoch: 483, Loss: 0.9350, Train: 0.9346, Val: 0.9686, Test: 0.9525\n",
      "Epoch: 484, Loss: 0.9346, Train: 0.9343, Val: 0.9684, Test: 0.9523\n",
      "Epoch: 485, Loss: 0.9343, Train: 0.9340, Val: 0.9681, Test: 0.9521\n",
      "Epoch: 486, Loss: 0.9340, Train: 0.9337, Val: 0.9679, Test: 0.9518\n",
      "Epoch: 487, Loss: 0.9337, Train: 0.9334, Val: 0.9677, Test: 0.9516\n",
      "Epoch: 488, Loss: 0.9334, Train: 0.9331, Val: 0.9675, Test: 0.9514\n",
      "Epoch: 489, Loss: 0.9331, Train: 0.9328, Val: 0.9672, Test: 0.9512\n",
      "Epoch: 490, Loss: 0.9328, Train: 0.9325, Val: 0.9670, Test: 0.9510\n",
      "Epoch: 491, Loss: 0.9325, Train: 0.9322, Val: 0.9668, Test: 0.9508\n",
      "Epoch: 492, Loss: 0.9322, Train: 0.9319, Val: 0.9666, Test: 0.9506\n",
      "Epoch: 493, Loss: 0.9319, Train: 0.9317, Val: 0.9664, Test: 0.9504\n",
      "Epoch: 494, Loss: 0.9317, Train: 0.9314, Val: 0.9662, Test: 0.9502\n",
      "Epoch: 495, Loss: 0.9314, Train: 0.9311, Val: 0.9660, Test: 0.9500\n",
      "Epoch: 496, Loss: 0.9311, Train: 0.9309, Val: 0.9658, Test: 0.9498\n",
      "Epoch: 497, Loss: 0.9309, Train: 0.9306, Val: 0.9656, Test: 0.9496\n",
      "Epoch: 498, Loss: 0.9306, Train: 0.9303, Val: 0.9654, Test: 0.9494\n",
      "Epoch: 499, Loss: 0.9303, Train: 0.9301, Val: 0.9652, Test: 0.9492\n",
      "Epoch: 500, Loss: 0.9301, Train: 0.9298, Val: 0.9650, Test: 0.9491\n",
      "Epoch: 501, Loss: 0.9298, Train: 0.9296, Val: 0.9649, Test: 0.9489\n",
      "Epoch: 502, Loss: 0.9296, Train: 0.9293, Val: 0.9647, Test: 0.9487\n",
      "Epoch: 503, Loss: 0.9294, Train: 0.9291, Val: 0.9645, Test: 0.9485\n",
      "Epoch: 504, Loss: 0.9291, Train: 0.9289, Val: 0.9644, Test: 0.9484\n",
      "Epoch: 505, Loss: 0.9289, Train: 0.9286, Val: 0.9642, Test: 0.9482\n",
      "Epoch: 506, Loss: 0.9286, Train: 0.9284, Val: 0.9640, Test: 0.9481\n",
      "Epoch: 507, Loss: 0.9284, Train: 0.9282, Val: 0.9639, Test: 0.9479\n",
      "Epoch: 508, Loss: 0.9282, Train: 0.9280, Val: 0.9637, Test: 0.9478\n",
      "Epoch: 509, Loss: 0.9280, Train: 0.9278, Val: 0.9636, Test: 0.9476\n",
      "Epoch: 510, Loss: 0.9278, Train: 0.9275, Val: 0.9634, Test: 0.9475\n",
      "Epoch: 511, Loss: 0.9275, Train: 0.9273, Val: 0.9633, Test: 0.9473\n",
      "Epoch: 512, Loss: 0.9273, Train: 0.9271, Val: 0.9631, Test: 0.9472\n",
      "Epoch: 513, Loss: 0.9271, Train: 0.9269, Val: 0.9630, Test: 0.9471\n",
      "Epoch: 514, Loss: 0.9269, Train: 0.9267, Val: 0.9629, Test: 0.9469\n",
      "Epoch: 515, Loss: 0.9267, Train: 0.9265, Val: 0.9627, Test: 0.9468\n",
      "Epoch: 516, Loss: 0.9265, Train: 0.9263, Val: 0.9626, Test: 0.9467\n",
      "Epoch: 517, Loss: 0.9263, Train: 0.9261, Val: 0.9625, Test: 0.9466\n",
      "Epoch: 518, Loss: 0.9261, Train: 0.9259, Val: 0.9624, Test: 0.9464\n",
      "Epoch: 519, Loss: 0.9259, Train: 0.9257, Val: 0.9622, Test: 0.9463\n",
      "Epoch: 520, Loss: 0.9257, Train: 0.9255, Val: 0.9621, Test: 0.9462\n",
      "Epoch: 521, Loss: 0.9255, Train: 0.9253, Val: 0.9620, Test: 0.9461\n",
      "Epoch: 522, Loss: 0.9253, Train: 0.9252, Val: 0.9619, Test: 0.9460\n",
      "Epoch: 523, Loss: 0.9252, Train: 0.9250, Val: 0.9618, Test: 0.9458\n",
      "Epoch: 524, Loss: 0.9250, Train: 0.9248, Val: 0.9616, Test: 0.9457\n",
      "Epoch: 525, Loss: 0.9248, Train: 0.9246, Val: 0.9615, Test: 0.9456\n",
      "Epoch: 526, Loss: 0.9246, Train: 0.9244, Val: 0.9614, Test: 0.9455\n",
      "Epoch: 527, Loss: 0.9244, Train: 0.9242, Val: 0.9613, Test: 0.9454\n",
      "Epoch: 528, Loss: 0.9242, Train: 0.9241, Val: 0.9612, Test: 0.9453\n",
      "Epoch: 529, Loss: 0.9241, Train: 0.9239, Val: 0.9611, Test: 0.9452\n",
      "Epoch: 530, Loss: 0.9239, Train: 0.9237, Val: 0.9610, Test: 0.9451\n",
      "Epoch: 531, Loss: 0.9237, Train: 0.9235, Val: 0.9609, Test: 0.9450\n",
      "Epoch: 532, Loss: 0.9235, Train: 0.9234, Val: 0.9608, Test: 0.9449\n",
      "Epoch: 533, Loss: 0.9234, Train: 0.9232, Val: 0.9607, Test: 0.9448\n",
      "Epoch: 534, Loss: 0.9232, Train: 0.9230, Val: 0.9606, Test: 0.9447\n",
      "Epoch: 535, Loss: 0.9230, Train: 0.9229, Val: 0.9605, Test: 0.9446\n",
      "Epoch: 536, Loss: 0.9229, Train: 0.9227, Val: 0.9604, Test: 0.9445\n",
      "Epoch: 537, Loss: 0.9227, Train: 0.9225, Val: 0.9603, Test: 0.9444\n",
      "Epoch: 538, Loss: 0.9225, Train: 0.9224, Val: 0.9602, Test: 0.9443\n",
      "Epoch: 539, Loss: 0.9224, Train: 0.9222, Val: 0.9601, Test: 0.9443\n",
      "Epoch: 540, Loss: 0.9222, Train: 0.9220, Val: 0.9600, Test: 0.9442\n",
      "Epoch: 541, Loss: 0.9220, Train: 0.9219, Val: 0.9599, Test: 0.9441\n",
      "Epoch: 542, Loss: 0.9219, Train: 0.9217, Val: 0.9599, Test: 0.9440\n",
      "Epoch: 543, Loss: 0.9217, Train: 0.9216, Val: 0.9598, Test: 0.9439\n",
      "Epoch: 544, Loss: 0.9216, Train: 0.9214, Val: 0.9597, Test: 0.9438\n",
      "Epoch: 545, Loss: 0.9214, Train: 0.9212, Val: 0.9596, Test: 0.9438\n",
      "Epoch: 546, Loss: 0.9213, Train: 0.9211, Val: 0.9595, Test: 0.9437\n",
      "Epoch: 547, Loss: 0.9211, Train: 0.9209, Val: 0.9594, Test: 0.9436\n",
      "Epoch: 548, Loss: 0.9210, Train: 0.9208, Val: 0.9593, Test: 0.9435\n",
      "Epoch: 549, Loss: 0.9208, Train: 0.9206, Val: 0.9593, Test: 0.9434\n",
      "Epoch: 550, Loss: 0.9206, Train: 0.9205, Val: 0.9592, Test: 0.9434\n",
      "Epoch: 551, Loss: 0.9205, Train: 0.9203, Val: 0.9591, Test: 0.9433\n",
      "Epoch: 552, Loss: 0.9203, Train: 0.9202, Val: 0.9590, Test: 0.9432\n",
      "Epoch: 553, Loss: 0.9202, Train: 0.9200, Val: 0.9589, Test: 0.9431\n",
      "Epoch: 554, Loss: 0.9201, Train: 0.9199, Val: 0.9588, Test: 0.9431\n",
      "Epoch: 555, Loss: 0.9199, Train: 0.9197, Val: 0.9588, Test: 0.9430\n",
      "Epoch: 556, Loss: 0.9198, Train: 0.9196, Val: 0.9587, Test: 0.9429\n",
      "Epoch: 557, Loss: 0.9196, Train: 0.9195, Val: 0.9586, Test: 0.9429\n",
      "Epoch: 558, Loss: 0.9195, Train: 0.9193, Val: 0.9585, Test: 0.9428\n",
      "Epoch: 559, Loss: 0.9193, Train: 0.9192, Val: 0.9585, Test: 0.9427\n",
      "Epoch: 560, Loss: 0.9192, Train: 0.9190, Val: 0.9584, Test: 0.9427\n",
      "Epoch: 561, Loss: 0.9191, Train: 0.9189, Val: 0.9583, Test: 0.9426\n",
      "Epoch: 562, Loss: 0.9189, Train: 0.9188, Val: 0.9582, Test: 0.9425\n",
      "Epoch: 563, Loss: 0.9188, Train: 0.9186, Val: 0.9582, Test: 0.9425\n",
      "Epoch: 564, Loss: 0.9186, Train: 0.9185, Val: 0.9581, Test: 0.9424\n",
      "Epoch: 565, Loss: 0.9185, Train: 0.9183, Val: 0.9580, Test: 0.9423\n",
      "Epoch: 566, Loss: 0.9184, Train: 0.9182, Val: 0.9580, Test: 0.9423\n",
      "Epoch: 567, Loss: 0.9182, Train: 0.9181, Val: 0.9579, Test: 0.9422\n",
      "Epoch: 568, Loss: 0.9181, Train: 0.9179, Val: 0.9578, Test: 0.9422\n",
      "Epoch: 569, Loss: 0.9180, Train: 0.9178, Val: 0.9578, Test: 0.9421\n",
      "Epoch: 570, Loss: 0.9178, Train: 0.9177, Val: 0.9577, Test: 0.9420\n",
      "Epoch: 571, Loss: 0.9177, Train: 0.9175, Val: 0.9576, Test: 0.9420\n",
      "Epoch: 572, Loss: 0.9176, Train: 0.9174, Val: 0.9576, Test: 0.9419\n",
      "Epoch: 573, Loss: 0.9174, Train: 0.9173, Val: 0.9575, Test: 0.9419\n",
      "Epoch: 574, Loss: 0.9173, Train: 0.9171, Val: 0.9574, Test: 0.9418\n",
      "Epoch: 575, Loss: 0.9172, Train: 0.9170, Val: 0.9574, Test: 0.9418\n",
      "Epoch: 576, Loss: 0.9170, Train: 0.9169, Val: 0.9573, Test: 0.9417\n",
      "Epoch: 577, Loss: 0.9169, Train: 0.9168, Val: 0.9573, Test: 0.9417\n",
      "Epoch: 578, Loss: 0.9168, Train: 0.9166, Val: 0.9572, Test: 0.9416\n",
      "Epoch: 579, Loss: 0.9166, Train: 0.9165, Val: 0.9571, Test: 0.9415\n",
      "Epoch: 580, Loss: 0.9165, Train: 0.9164, Val: 0.9571, Test: 0.9415\n",
      "Epoch: 581, Loss: 0.9164, Train: 0.9162, Val: 0.9570, Test: 0.9414\n",
      "Epoch: 582, Loss: 0.9163, Train: 0.9161, Val: 0.9570, Test: 0.9414\n",
      "Epoch: 583, Loss: 0.9161, Train: 0.9160, Val: 0.9569, Test: 0.9413\n",
      "Epoch: 584, Loss: 0.9160, Train: 0.9159, Val: 0.9568, Test: 0.9413\n",
      "Epoch: 585, Loss: 0.9159, Train: 0.9157, Val: 0.9568, Test: 0.9412\n",
      "Epoch: 586, Loss: 0.9158, Train: 0.9156, Val: 0.9567, Test: 0.9412\n",
      "Epoch: 587, Loss: 0.9157, Train: 0.9155, Val: 0.9567, Test: 0.9411\n",
      "Epoch: 588, Loss: 0.9155, Train: 0.9154, Val: 0.9566, Test: 0.9411\n",
      "Epoch: 589, Loss: 0.9154, Train: 0.9153, Val: 0.9566, Test: 0.9410\n",
      "Epoch: 590, Loss: 0.9153, Train: 0.9151, Val: 0.9565, Test: 0.9410\n",
      "Epoch: 591, Loss: 0.9152, Train: 0.9150, Val: 0.9564, Test: 0.9409\n",
      "Epoch: 592, Loss: 0.9151, Train: 0.9149, Val: 0.9564, Test: 0.9409\n",
      "Epoch: 593, Loss: 0.9149, Train: 0.9148, Val: 0.9563, Test: 0.9408\n",
      "Epoch: 594, Loss: 0.9148, Train: 0.9147, Val: 0.9563, Test: 0.9408\n",
      "Epoch: 595, Loss: 0.9147, Train: 0.9145, Val: 0.9562, Test: 0.9407\n",
      "Epoch: 596, Loss: 0.9146, Train: 0.9144, Val: 0.9562, Test: 0.9407\n",
      "Epoch: 597, Loss: 0.9145, Train: 0.9143, Val: 0.9561, Test: 0.9406\n",
      "Epoch: 598, Loss: 0.9143, Train: 0.9142, Val: 0.9561, Test: 0.9406\n",
      "Epoch: 599, Loss: 0.9142, Train: 0.9141, Val: 0.9560, Test: 0.9405\n",
      "Epoch: 600, Loss: 0.9141, Train: 0.9139, Val: 0.9560, Test: 0.9405\n",
      "Epoch: 601, Loss: 0.9140, Train: 0.9138, Val: 0.9560, Test: 0.9405\n",
      "Epoch: 602, Loss: 0.9139, Train: 0.9137, Val: 0.9559, Test: 0.9404\n",
      "Epoch: 603, Loss: 0.9137, Train: 0.9136, Val: 0.9558, Test: 0.9404\n",
      "Epoch: 604, Loss: 0.9136, Train: 0.9135, Val: 0.9558, Test: 0.9403\n",
      "Epoch: 605, Loss: 0.9135, Train: 0.9134, Val: 0.9557, Test: 0.9402\n",
      "Epoch: 606, Loss: 0.9134, Train: 0.9133, Val: 0.9556, Test: 0.9402\n",
      "Epoch: 607, Loss: 0.9133, Train: 0.9131, Val: 0.9556, Test: 0.9401\n",
      "Epoch: 608, Loss: 0.9132, Train: 0.9130, Val: 0.9556, Test: 0.9401\n",
      "Epoch: 609, Loss: 0.9131, Train: 0.9129, Val: 0.9555, Test: 0.9401\n",
      "Epoch: 610, Loss: 0.9129, Train: 0.9128, Val: 0.9555, Test: 0.9401\n",
      "Epoch: 611, Loss: 0.9128, Train: 0.9127, Val: 0.9555, Test: 0.9400\n",
      "Epoch: 612, Loss: 0.9127, Train: 0.9126, Val: 0.9554, Test: 0.9400\n",
      "Epoch: 613, Loss: 0.9126, Train: 0.9125, Val: 0.9553, Test: 0.9399\n",
      "Epoch: 614, Loss: 0.9125, Train: 0.9123, Val: 0.9553, Test: 0.9398\n",
      "Epoch: 615, Loss: 0.9124, Train: 0.9122, Val: 0.9552, Test: 0.9398\n",
      "Epoch: 616, Loss: 0.9123, Train: 0.9121, Val: 0.9552, Test: 0.9398\n",
      "Epoch: 617, Loss: 0.9122, Train: 0.9120, Val: 0.9552, Test: 0.9397\n",
      "Epoch: 618, Loss: 0.9120, Train: 0.9119, Val: 0.9551, Test: 0.9397\n",
      "Epoch: 619, Loss: 0.9119, Train: 0.9118, Val: 0.9551, Test: 0.9397\n",
      "Epoch: 620, Loss: 0.9118, Train: 0.9117, Val: 0.9550, Test: 0.9396\n",
      "Epoch: 621, Loss: 0.9117, Train: 0.9116, Val: 0.9550, Test: 0.9396\n",
      "Epoch: 622, Loss: 0.9116, Train: 0.9115, Val: 0.9549, Test: 0.9395\n",
      "Epoch: 623, Loss: 0.9115, Train: 0.9114, Val: 0.9549, Test: 0.9395\n",
      "Epoch: 624, Loss: 0.9114, Train: 0.9113, Val: 0.9548, Test: 0.9394\n",
      "Epoch: 625, Loss: 0.9113, Train: 0.9111, Val: 0.9548, Test: 0.9394\n",
      "Epoch: 626, Loss: 0.9112, Train: 0.9110, Val: 0.9548, Test: 0.9394\n",
      "Epoch: 627, Loss: 0.9111, Train: 0.9109, Val: 0.9548, Test: 0.9394\n",
      "Epoch: 628, Loss: 0.9110, Train: 0.9108, Val: 0.9547, Test: 0.9393\n",
      "Epoch: 629, Loss: 0.9109, Train: 0.9107, Val: 0.9547, Test: 0.9393\n",
      "Epoch: 630, Loss: 0.9108, Train: 0.9106, Val: 0.9546, Test: 0.9392\n",
      "Epoch: 631, Loss: 0.9107, Train: 0.9105, Val: 0.9546, Test: 0.9392\n",
      "Epoch: 632, Loss: 0.9106, Train: 0.9104, Val: 0.9545, Test: 0.9392\n",
      "Epoch: 633, Loss: 0.9105, Train: 0.9103, Val: 0.9545, Test: 0.9391\n",
      "Epoch: 634, Loss: 0.9104, Train: 0.9102, Val: 0.9545, Test: 0.9391\n",
      "Epoch: 635, Loss: 0.9103, Train: 0.9101, Val: 0.9544, Test: 0.9391\n",
      "Epoch: 636, Loss: 0.9102, Train: 0.9100, Val: 0.9544, Test: 0.9390\n",
      "Epoch: 637, Loss: 0.9100, Train: 0.9099, Val: 0.9544, Test: 0.9390\n",
      "Epoch: 638, Loss: 0.9099, Train: 0.9098, Val: 0.9543, Test: 0.9389\n",
      "Epoch: 639, Loss: 0.9098, Train: 0.9097, Val: 0.9543, Test: 0.9389\n",
      "Epoch: 640, Loss: 0.9097, Train: 0.9096, Val: 0.9542, Test: 0.9389\n",
      "Epoch: 641, Loss: 0.9096, Train: 0.9095, Val: 0.9542, Test: 0.9389\n",
      "Epoch: 642, Loss: 0.9095, Train: 0.9094, Val: 0.9542, Test: 0.9388\n",
      "Epoch: 643, Loss: 0.9094, Train: 0.9093, Val: 0.9541, Test: 0.9388\n",
      "Epoch: 644, Loss: 0.9093, Train: 0.9092, Val: 0.9541, Test: 0.9388\n",
      "Epoch: 645, Loss: 0.9092, Train: 0.9091, Val: 0.9541, Test: 0.9387\n",
      "Epoch: 646, Loss: 0.9092, Train: 0.9090, Val: 0.9540, Test: 0.9387\n",
      "Epoch: 647, Loss: 0.9091, Train: 0.9089, Val: 0.9540, Test: 0.9387\n",
      "Epoch: 648, Loss: 0.9090, Train: 0.9088, Val: 0.9539, Test: 0.9386\n",
      "Epoch: 649, Loss: 0.9089, Train: 0.9087, Val: 0.9539, Test: 0.9386\n",
      "Epoch: 650, Loss: 0.9088, Train: 0.9086, Val: 0.9539, Test: 0.9386\n",
      "Epoch: 651, Loss: 0.9087, Train: 0.9085, Val: 0.9538, Test: 0.9386\n",
      "Epoch: 652, Loss: 0.9086, Train: 0.9084, Val: 0.9538, Test: 0.9385\n",
      "Epoch: 653, Loss: 0.9085, Train: 0.9083, Val: 0.9538, Test: 0.9385\n",
      "Epoch: 654, Loss: 0.9084, Train: 0.9082, Val: 0.9537, Test: 0.9385\n",
      "Epoch: 655, Loss: 0.9083, Train: 0.9081, Val: 0.9537, Test: 0.9384\n",
      "Epoch: 656, Loss: 0.9082, Train: 0.9080, Val: 0.9536, Test: 0.9384\n",
      "Epoch: 657, Loss: 0.9081, Train: 0.9079, Val: 0.9536, Test: 0.9384\n",
      "Epoch: 658, Loss: 0.9080, Train: 0.9079, Val: 0.9536, Test: 0.9384\n",
      "Epoch: 659, Loss: 0.9079, Train: 0.9078, Val: 0.9535, Test: 0.9383\n",
      "Epoch: 660, Loss: 0.9078, Train: 0.9077, Val: 0.9535, Test: 0.9383\n",
      "Epoch: 661, Loss: 0.9077, Train: 0.9076, Val: 0.9535, Test: 0.9383\n",
      "Epoch: 662, Loss: 0.9076, Train: 0.9075, Val: 0.9534, Test: 0.9383\n",
      "Epoch: 663, Loss: 0.9075, Train: 0.9074, Val: 0.9534, Test: 0.9382\n",
      "Epoch: 664, Loss: 0.9074, Train: 0.9073, Val: 0.9534, Test: 0.9382\n",
      "Epoch: 665, Loss: 0.9073, Train: 0.9072, Val: 0.9533, Test: 0.9382\n",
      "Epoch: 666, Loss: 0.9072, Train: 0.9071, Val: 0.9533, Test: 0.9381\n",
      "Epoch: 667, Loss: 0.9072, Train: 0.9070, Val: 0.9533, Test: 0.9381\n",
      "Epoch: 668, Loss: 0.9071, Train: 0.9069, Val: 0.9532, Test: 0.9381\n",
      "Epoch: 669, Loss: 0.9070, Train: 0.9068, Val: 0.9532, Test: 0.9381\n",
      "Epoch: 670, Loss: 0.9069, Train: 0.9067, Val: 0.9532, Test: 0.9380\n",
      "Epoch: 671, Loss: 0.9068, Train: 0.9066, Val: 0.9531, Test: 0.9380\n",
      "Epoch: 672, Loss: 0.9067, Train: 0.9066, Val: 0.9531, Test: 0.9380\n",
      "Epoch: 673, Loss: 0.9066, Train: 0.9065, Val: 0.9530, Test: 0.9379\n",
      "Epoch: 674, Loss: 0.9065, Train: 0.9064, Val: 0.9530, Test: 0.9379\n",
      "Epoch: 675, Loss: 0.9064, Train: 0.9063, Val: 0.9530, Test: 0.9379\n",
      "Epoch: 676, Loss: 0.9063, Train: 0.9062, Val: 0.9529, Test: 0.9379\n",
      "Epoch: 677, Loss: 0.9062, Train: 0.9061, Val: 0.9529, Test: 0.9378\n",
      "Epoch: 678, Loss: 0.9062, Train: 0.9060, Val: 0.9529, Test: 0.9378\n",
      "Epoch: 679, Loss: 0.9061, Train: 0.9059, Val: 0.9528, Test: 0.9378\n",
      "Epoch: 680, Loss: 0.9060, Train: 0.9058, Val: 0.9528, Test: 0.9377\n",
      "Epoch: 681, Loss: 0.9059, Train: 0.9057, Val: 0.9528, Test: 0.9377\n",
      "Epoch: 682, Loss: 0.9058, Train: 0.9057, Val: 0.9528, Test: 0.9377\n",
      "Epoch: 683, Loss: 0.9057, Train: 0.9056, Val: 0.9527, Test: 0.9377\n",
      "Epoch: 684, Loss: 0.9056, Train: 0.9055, Val: 0.9527, Test: 0.9376\n",
      "Epoch: 685, Loss: 0.9055, Train: 0.9054, Val: 0.9527, Test: 0.9376\n",
      "Epoch: 686, Loss: 0.9054, Train: 0.9053, Val: 0.9526, Test: 0.9376\n",
      "Epoch: 687, Loss: 0.9054, Train: 0.9052, Val: 0.9526, Test: 0.9376\n",
      "Epoch: 688, Loss: 0.9053, Train: 0.9051, Val: 0.9526, Test: 0.9375\n",
      "Epoch: 689, Loss: 0.9052, Train: 0.9050, Val: 0.9525, Test: 0.9375\n",
      "Epoch: 690, Loss: 0.9051, Train: 0.9050, Val: 0.9525, Test: 0.9375\n",
      "Epoch: 691, Loss: 0.9050, Train: 0.9049, Val: 0.9525, Test: 0.9375\n",
      "Epoch: 692, Loss: 0.9049, Train: 0.9048, Val: 0.9524, Test: 0.9374\n",
      "Epoch: 693, Loss: 0.9048, Train: 0.9047, Val: 0.9524, Test: 0.9374\n",
      "Epoch: 694, Loss: 0.9047, Train: 0.9046, Val: 0.9524, Test: 0.9374\n",
      "Epoch: 695, Loss: 0.9047, Train: 0.9045, Val: 0.9524, Test: 0.9374\n",
      "Epoch: 696, Loss: 0.9046, Train: 0.9044, Val: 0.9523, Test: 0.9373\n",
      "Epoch: 697, Loss: 0.9045, Train: 0.9044, Val: 0.9523, Test: 0.9373\n",
      "Epoch: 698, Loss: 0.9044, Train: 0.9043, Val: 0.9523, Test: 0.9373\n",
      "Epoch: 699, Loss: 0.9043, Train: 0.9042, Val: 0.9522, Test: 0.9373\n",
      "Epoch: 700, Loss: 0.9042, Train: 0.9041, Val: 0.9522, Test: 0.9372\n",
      "Epoch: 701, Loss: 0.9042, Train: 0.9040, Val: 0.9522, Test: 0.9372\n",
      "Epoch: 702, Loss: 0.9041, Train: 0.9039, Val: 0.9521, Test: 0.9372\n",
      "Epoch: 703, Loss: 0.9040, Train: 0.9039, Val: 0.9521, Test: 0.9372\n",
      "Epoch: 704, Loss: 0.9039, Train: 0.9038, Val: 0.9521, Test: 0.9372\n",
      "Epoch: 705, Loss: 0.9038, Train: 0.9037, Val: 0.9521, Test: 0.9371\n",
      "Epoch: 706, Loss: 0.9037, Train: 0.9036, Val: 0.9520, Test: 0.9371\n",
      "Epoch: 707, Loss: 0.9037, Train: 0.9035, Val: 0.9520, Test: 0.9371\n",
      "Epoch: 708, Loss: 0.9036, Train: 0.9034, Val: 0.9520, Test: 0.9371\n",
      "Epoch: 709, Loss: 0.9035, Train: 0.9034, Val: 0.9519, Test: 0.9371\n",
      "Epoch: 710, Loss: 0.9034, Train: 0.9033, Val: 0.9519, Test: 0.9370\n",
      "Epoch: 711, Loss: 0.9033, Train: 0.9032, Val: 0.9519, Test: 0.9370\n",
      "Epoch: 712, Loss: 0.9033, Train: 0.9031, Val: 0.9519, Test: 0.9370\n",
      "Epoch: 713, Loss: 0.9032, Train: 0.9030, Val: 0.9519, Test: 0.9370\n",
      "Epoch: 714, Loss: 0.9031, Train: 0.9030, Val: 0.9518, Test: 0.9370\n",
      "Epoch: 715, Loss: 0.9030, Train: 0.9029, Val: 0.9518, Test: 0.9369\n",
      "Epoch: 716, Loss: 0.9029, Train: 0.9028, Val: 0.9518, Test: 0.9369\n",
      "Epoch: 717, Loss: 0.9029, Train: 0.9027, Val: 0.9517, Test: 0.9369\n",
      "Epoch: 718, Loss: 0.9028, Train: 0.9026, Val: 0.9517, Test: 0.9369\n",
      "Epoch: 719, Loss: 0.9027, Train: 0.9025, Val: 0.9517, Test: 0.9369\n",
      "Epoch: 720, Loss: 0.9026, Train: 0.9025, Val: 0.9517, Test: 0.9369\n",
      "Epoch: 721, Loss: 0.9025, Train: 0.9024, Val: 0.9516, Test: 0.9368\n",
      "Epoch: 722, Loss: 0.9025, Train: 0.9023, Val: 0.9516, Test: 0.9368\n",
      "Epoch: 723, Loss: 0.9024, Train: 0.9022, Val: 0.9516, Test: 0.9368\n",
      "Epoch: 724, Loss: 0.9023, Train: 0.9022, Val: 0.9516, Test: 0.9368\n",
      "Epoch: 725, Loss: 0.9022, Train: 0.9021, Val: 0.9515, Test: 0.9368\n",
      "Epoch: 726, Loss: 0.9021, Train: 0.9020, Val: 0.9515, Test: 0.9367\n",
      "Epoch: 727, Loss: 0.9021, Train: 0.9019, Val: 0.9515, Test: 0.9367\n",
      "Epoch: 728, Loss: 0.9020, Train: 0.9018, Val: 0.9515, Test: 0.9367\n",
      "Epoch: 729, Loss: 0.9019, Train: 0.9018, Val: 0.9515, Test: 0.9367\n",
      "Epoch: 730, Loss: 0.9018, Train: 0.9017, Val: 0.9514, Test: 0.9367\n",
      "Epoch: 731, Loss: 0.9017, Train: 0.9016, Val: 0.9514, Test: 0.9367\n",
      "Epoch: 732, Loss: 0.9017, Train: 0.9015, Val: 0.9514, Test: 0.9366\n",
      "Epoch: 733, Loss: 0.9016, Train: 0.9014, Val: 0.9513, Test: 0.9366\n",
      "Epoch: 734, Loss: 0.9015, Train: 0.9014, Val: 0.9513, Test: 0.9366\n",
      "Epoch: 735, Loss: 0.9014, Train: 0.9013, Val: 0.9513, Test: 0.9366\n",
      "Epoch: 736, Loss: 0.9013, Train: 0.9012, Val: 0.9513, Test: 0.9366\n",
      "Epoch: 737, Loss: 0.9013, Train: 0.9011, Val: 0.9513, Test: 0.9366\n",
      "Epoch: 738, Loss: 0.9012, Train: 0.9010, Val: 0.9512, Test: 0.9366\n",
      "Epoch: 739, Loss: 0.9011, Train: 0.9009, Val: 0.9512, Test: 0.9366\n",
      "Epoch: 740, Loss: 0.9010, Train: 0.9008, Val: 0.9512, Test: 0.9365\n",
      "Epoch: 741, Loss: 0.9009, Train: 0.9007, Val: 0.9513, Test: 0.9365\n",
      "Epoch: 742, Loss: 0.9008, Train: 0.9006, Val: 0.9513, Test: 0.9365\n",
      "Epoch: 743, Loss: 0.9007, Train: 0.9005, Val: 0.9514, Test: 0.9365\n",
      "Epoch: 744, Loss: 0.9006, Train: 0.9004, Val: 0.9514, Test: 0.9365\n",
      "Epoch: 745, Loss: 0.9005, Train: 0.9004, Val: 0.9513, Test: 0.9365\n",
      "Epoch: 746, Loss: 0.9004, Train: 0.9003, Val: 0.9513, Test: 0.9365\n",
      "Epoch: 747, Loss: 0.9004, Train: 0.9002, Val: 0.9513, Test: 0.9365\n",
      "Epoch: 748, Loss: 0.9003, Train: 0.9001, Val: 0.9512, Test: 0.9364\n",
      "Epoch: 749, Loss: 0.9002, Train: 0.9000, Val: 0.9512, Test: 0.9364\n",
      "Epoch: 750, Loss: 0.9001, Train: 0.8999, Val: 0.9512, Test: 0.9364\n",
      "Epoch: 751, Loss: 0.9000, Train: 0.8999, Val: 0.9511, Test: 0.9364\n",
      "Epoch: 752, Loss: 0.8999, Train: 0.8998, Val: 0.9511, Test: 0.9364\n",
      "Epoch: 753, Loss: 0.8999, Train: 0.8997, Val: 0.9511, Test: 0.9363\n",
      "Epoch: 754, Loss: 0.8998, Train: 0.8996, Val: 0.9510, Test: 0.9363\n",
      "Epoch: 755, Loss: 0.8997, Train: 0.8996, Val: 0.9510, Test: 0.9363\n",
      "Epoch: 756, Loss: 0.8996, Train: 0.8995, Val: 0.9510, Test: 0.9363\n",
      "Epoch: 757, Loss: 0.8995, Train: 0.8994, Val: 0.9510, Test: 0.9363\n",
      "Epoch: 758, Loss: 0.8995, Train: 0.8993, Val: 0.9510, Test: 0.9363\n",
      "Epoch: 759, Loss: 0.8994, Train: 0.8992, Val: 0.9510, Test: 0.9363\n",
      "Epoch: 760, Loss: 0.8993, Train: 0.8991, Val: 0.9510, Test: 0.9363\n",
      "Epoch: 761, Loss: 0.8992, Train: 0.8991, Val: 0.9510, Test: 0.9363\n",
      "Epoch: 762, Loss: 0.8991, Train: 0.8990, Val: 0.9509, Test: 0.9363\n",
      "Epoch: 763, Loss: 0.8991, Train: 0.8989, Val: 0.9509, Test: 0.9363\n",
      "Epoch: 764, Loss: 0.8990, Train: 0.8988, Val: 0.9509, Test: 0.9363\n",
      "Epoch: 765, Loss: 0.8989, Train: 0.8987, Val: 0.9509, Test: 0.9362\n",
      "Epoch: 766, Loss: 0.8988, Train: 0.8987, Val: 0.9509, Test: 0.9362\n",
      "Epoch: 767, Loss: 0.8987, Train: 0.8986, Val: 0.9508, Test: 0.9362\n",
      "Epoch: 768, Loss: 0.8987, Train: 0.8985, Val: 0.9508, Test: 0.9362\n",
      "Epoch: 769, Loss: 0.8986, Train: 0.8984, Val: 0.9508, Test: 0.9362\n",
      "Epoch: 770, Loss: 0.8985, Train: 0.8984, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 771, Loss: 0.8984, Train: 0.8983, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 772, Loss: 0.8983, Train: 0.8982, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 773, Loss: 0.8983, Train: 0.8981, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 774, Loss: 0.8982, Train: 0.8980, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 775, Loss: 0.8981, Train: 0.8980, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 776, Loss: 0.8980, Train: 0.8979, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 777, Loss: 0.8979, Train: 0.8978, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 778, Loss: 0.8979, Train: 0.8977, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 779, Loss: 0.8978, Train: 0.8977, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 780, Loss: 0.8977, Train: 0.8976, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 781, Loss: 0.8976, Train: 0.8975, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 782, Loss: 0.8976, Train: 0.8974, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 783, Loss: 0.8975, Train: 0.8973, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 784, Loss: 0.8974, Train: 0.8973, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 785, Loss: 0.8973, Train: 0.8972, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 786, Loss: 0.8973, Train: 0.8971, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 787, Loss: 0.8972, Train: 0.8970, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 788, Loss: 0.8971, Train: 0.8970, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 789, Loss: 0.8970, Train: 0.8969, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 790, Loss: 0.8969, Train: 0.8968, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 791, Loss: 0.8969, Train: 0.8967, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 792, Loss: 0.8968, Train: 0.8966, Val: 0.9504, Test: 0.9359\n",
      "Epoch: 793, Loss: 0.8967, Train: 0.8966, Val: 0.9504, Test: 0.9360\n",
      "Epoch: 794, Loss: 0.8966, Train: 0.8965, Val: 0.9504, Test: 0.9360\n",
      "Epoch: 795, Loss: 0.8966, Train: 0.8964, Val: 0.9504, Test: 0.9360\n",
      "Epoch: 796, Loss: 0.8965, Train: 0.8963, Val: 0.9504, Test: 0.9360\n",
      "Epoch: 797, Loss: 0.8964, Train: 0.8962, Val: 0.9504, Test: 0.9360\n",
      "Epoch: 798, Loss: 0.8963, Train: 0.8961, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 799, Loss: 0.8962, Train: 0.8960, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 800, Loss: 0.8961, Train: 0.8960, Val: 0.9508, Test: 0.9363\n",
      "Epoch: 801, Loss: 0.8960, Train: 0.8959, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 802, Loss: 0.8960, Train: 0.8958, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 803, Loss: 0.8958, Train: 0.8957, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 804, Loss: 0.8958, Train: 0.8956, Val: 0.9505, Test: 0.9361\n",
      "Epoch: 805, Loss: 0.8957, Train: 0.8955, Val: 0.9504, Test: 0.9360\n",
      "Epoch: 806, Loss: 0.8956, Train: 0.8954, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 807, Loss: 0.8955, Train: 0.8954, Val: 0.9506, Test: 0.9362\n",
      "Epoch: 808, Loss: 0.8954, Train: 0.8953, Val: 0.9507, Test: 0.9362\n",
      "Epoch: 809, Loss: 0.8953, Train: 0.8952, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 810, Loss: 0.8952, Train: 0.8951, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 811, Loss: 0.8951, Train: 0.8950, Val: 0.9506, Test: 0.9361\n",
      "Epoch: 812, Loss: 0.8951, Train: 0.8949, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 813, Loss: 0.8950, Train: 0.8948, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 814, Loss: 0.8949, Train: 0.8947, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 815, Loss: 0.8948, Train: 0.8946, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 816, Loss: 0.8947, Train: 0.8946, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 817, Loss: 0.8946, Train: 0.8945, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 818, Loss: 0.8945, Train: 0.8944, Val: 0.9507, Test: 0.9361\n",
      "Epoch: 819, Loss: 0.8944, Train: 0.8943, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 820, Loss: 0.8944, Train: 0.8942, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 821, Loss: 0.8943, Train: 0.8941, Val: 0.9506, Test: 0.9359\n",
      "Epoch: 822, Loss: 0.8942, Train: 0.8940, Val: 0.9506, Test: 0.9359\n",
      "Epoch: 823, Loss: 0.8941, Train: 0.8939, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 824, Loss: 0.8940, Train: 0.8939, Val: 0.9505, Test: 0.9358\n",
      "Epoch: 825, Loss: 0.8939, Train: 0.8938, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 826, Loss: 0.8938, Train: 0.8937, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 827, Loss: 0.8938, Train: 0.8936, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 828, Loss: 0.8937, Train: 0.8935, Val: 0.9506, Test: 0.9359\n",
      "Epoch: 829, Loss: 0.8936, Train: 0.8934, Val: 0.9506, Test: 0.9359\n",
      "Epoch: 830, Loss: 0.8935, Train: 0.8933, Val: 0.9506, Test: 0.9359\n",
      "Epoch: 831, Loss: 0.8934, Train: 0.8933, Val: 0.9506, Test: 0.9359\n",
      "Epoch: 832, Loss: 0.8933, Train: 0.8932, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 833, Loss: 0.8932, Train: 0.8931, Val: 0.9506, Test: 0.9359\n",
      "Epoch: 834, Loss: 0.8931, Train: 0.8930, Val: 0.9507, Test: 0.9360\n",
      "Epoch: 835, Loss: 0.8931, Train: 0.8929, Val: 0.9507, Test: 0.9360\n",
      "Epoch: 836, Loss: 0.8930, Train: 0.8928, Val: 0.9507, Test: 0.9360\n",
      "Epoch: 837, Loss: 0.8929, Train: 0.8927, Val: 0.9507, Test: 0.9360\n",
      "Epoch: 838, Loss: 0.8928, Train: 0.8927, Val: 0.9507, Test: 0.9360\n",
      "Epoch: 839, Loss: 0.8927, Train: 0.8926, Val: 0.9507, Test: 0.9360\n",
      "Epoch: 840, Loss: 0.8926, Train: 0.8925, Val: 0.9506, Test: 0.9359\n",
      "Epoch: 841, Loss: 0.8925, Train: 0.8924, Val: 0.9507, Test: 0.9360\n",
      "Epoch: 842, Loss: 0.8925, Train: 0.8923, Val: 0.9507, Test: 0.9360\n",
      "Epoch: 843, Loss: 0.8924, Train: 0.8922, Val: 0.9506, Test: 0.9359\n",
      "Epoch: 844, Loss: 0.8923, Train: 0.8921, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 845, Loss: 0.8922, Train: 0.8921, Val: 0.9506, Test: 0.9359\n",
      "Epoch: 846, Loss: 0.8921, Train: 0.8920, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 847, Loss: 0.8920, Train: 0.8919, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 848, Loss: 0.8919, Train: 0.8918, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 849, Loss: 0.8919, Train: 0.8917, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 850, Loss: 0.8918, Train: 0.8916, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 851, Loss: 0.8917, Train: 0.8915, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 852, Loss: 0.8916, Train: 0.8915, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 853, Loss: 0.8915, Train: 0.8914, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 854, Loss: 0.8914, Train: 0.8913, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 855, Loss: 0.8913, Train: 0.8912, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 856, Loss: 0.8913, Train: 0.8911, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 857, Loss: 0.8912, Train: 0.8910, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 858, Loss: 0.8911, Train: 0.8910, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 859, Loss: 0.8910, Train: 0.8909, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 860, Loss: 0.8909, Train: 0.8908, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 861, Loss: 0.8908, Train: 0.8907, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 862, Loss: 0.8907, Train: 0.8906, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 863, Loss: 0.8907, Train: 0.8905, Val: 0.9504, Test: 0.9359\n",
      "Epoch: 864, Loss: 0.8906, Train: 0.8904, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 865, Loss: 0.8905, Train: 0.8904, Val: 0.9504, Test: 0.9359\n",
      "Epoch: 866, Loss: 0.8904, Train: 0.8903, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 867, Loss: 0.8903, Train: 0.8902, Val: 0.9503, Test: 0.9358\n",
      "Epoch: 868, Loss: 0.8902, Train: 0.8901, Val: 0.9505, Test: 0.9360\n",
      "Epoch: 869, Loss: 0.8902, Train: 0.8900, Val: 0.9502, Test: 0.9357\n",
      "Epoch: 870, Loss: 0.8901, Train: 0.8900, Val: 0.9504, Test: 0.9360\n",
      "Epoch: 871, Loss: 0.8900, Train: 0.8899, Val: 0.9502, Test: 0.9358\n",
      "Epoch: 872, Loss: 0.8899, Train: 0.8898, Val: 0.9503, Test: 0.9358\n",
      "Epoch: 873, Loss: 0.8898, Train: 0.8897, Val: 0.9504, Test: 0.9360\n",
      "Epoch: 874, Loss: 0.8898, Train: 0.8896, Val: 0.9501, Test: 0.9357\n",
      "Epoch: 875, Loss: 0.8897, Train: 0.8895, Val: 0.9503, Test: 0.9359\n",
      "Epoch: 876, Loss: 0.8896, Train: 0.8895, Val: 0.9503, Test: 0.9358\n",
      "Epoch: 877, Loss: 0.8895, Train: 0.8894, Val: 0.9501, Test: 0.9357\n",
      "Epoch: 878, Loss: 0.8894, Train: 0.8893, Val: 0.9503, Test: 0.9359\n",
      "Epoch: 879, Loss: 0.8893, Train: 0.8892, Val: 0.9501, Test: 0.9356\n",
      "Epoch: 880, Loss: 0.8893, Train: 0.8891, Val: 0.9502, Test: 0.9358\n",
      "Epoch: 881, Loss: 0.8892, Train: 0.8890, Val: 0.9502, Test: 0.9357\n",
      "Epoch: 882, Loss: 0.8891, Train: 0.8890, Val: 0.9501, Test: 0.9356\n",
      "Epoch: 883, Loss: 0.8890, Train: 0.8889, Val: 0.9503, Test: 0.9359\n",
      "Epoch: 884, Loss: 0.8889, Train: 0.8888, Val: 0.9500, Test: 0.9356\n",
      "Epoch: 885, Loss: 0.8889, Train: 0.8887, Val: 0.9502, Test: 0.9358\n",
      "Epoch: 886, Loss: 0.8888, Train: 0.8886, Val: 0.9501, Test: 0.9357\n",
      "Epoch: 887, Loss: 0.8887, Train: 0.8886, Val: 0.9501, Test: 0.9356\n",
      "Epoch: 888, Loss: 0.8886, Train: 0.8885, Val: 0.9502, Test: 0.9358\n",
      "Epoch: 889, Loss: 0.8885, Train: 0.8884, Val: 0.9500, Test: 0.9355\n",
      "Epoch: 890, Loss: 0.8885, Train: 0.8883, Val: 0.9502, Test: 0.9358\n",
      "Epoch: 891, Loss: 0.8884, Train: 0.8882, Val: 0.9500, Test: 0.9356\n",
      "Epoch: 892, Loss: 0.8883, Train: 0.8882, Val: 0.9501, Test: 0.9356\n",
      "Epoch: 893, Loss: 0.8882, Train: 0.8881, Val: 0.9501, Test: 0.9357\n",
      "Epoch: 894, Loss: 0.8881, Train: 0.8880, Val: 0.9499, Test: 0.9355\n",
      "Epoch: 895, Loss: 0.8881, Train: 0.8879, Val: 0.9501, Test: 0.9357\n",
      "Epoch: 896, Loss: 0.8880, Train: 0.8879, Val: 0.9499, Test: 0.9355\n",
      "Epoch: 897, Loss: 0.8879, Train: 0.8878, Val: 0.9500, Test: 0.9356\n",
      "Epoch: 898, Loss: 0.8878, Train: 0.8877, Val: 0.9500, Test: 0.9356\n",
      "Epoch: 899, Loss: 0.8877, Train: 0.8876, Val: 0.9500, Test: 0.9355\n",
      "Epoch: 900, Loss: 0.8877, Train: 0.8875, Val: 0.9501, Test: 0.9356\n",
      "Epoch: 901, Loss: 0.8876, Train: 0.8875, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 902, Loss: 0.8875, Train: 0.8874, Val: 0.9501, Test: 0.9356\n",
      "Epoch: 903, Loss: 0.8874, Train: 0.8873, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 904, Loss: 0.8874, Train: 0.8872, Val: 0.9500, Test: 0.9355\n",
      "Epoch: 905, Loss: 0.8873, Train: 0.8872, Val: 0.9499, Test: 0.9355\n",
      "Epoch: 906, Loss: 0.8872, Train: 0.8871, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 907, Loss: 0.8871, Train: 0.8870, Val: 0.9500, Test: 0.9355\n",
      "Epoch: 908, Loss: 0.8870, Train: 0.8869, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 909, Loss: 0.8870, Train: 0.8869, Val: 0.9500, Test: 0.9356\n",
      "Epoch: 910, Loss: 0.8869, Train: 0.8868, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 911, Loss: 0.8868, Train: 0.8867, Val: 0.9500, Test: 0.9355\n",
      "Epoch: 912, Loss: 0.8867, Train: 0.8866, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 913, Loss: 0.8867, Train: 0.8866, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 914, Loss: 0.8866, Train: 0.8865, Val: 0.9500, Test: 0.9355\n",
      "Epoch: 915, Loss: 0.8865, Train: 0.8864, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 916, Loss: 0.8864, Train: 0.8863, Val: 0.9500, Test: 0.9355\n",
      "Epoch: 917, Loss: 0.8864, Train: 0.8863, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 918, Loss: 0.8863, Train: 0.8862, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 919, Loss: 0.8862, Train: 0.8861, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 920, Loss: 0.8861, Train: 0.8860, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 921, Loss: 0.8861, Train: 0.8860, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 922, Loss: 0.8860, Train: 0.8859, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 923, Loss: 0.8859, Train: 0.8858, Val: 0.9499, Test: 0.9353\n",
      "Epoch: 924, Loss: 0.8859, Train: 0.8857, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 925, Loss: 0.8858, Train: 0.8857, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 926, Loss: 0.8857, Train: 0.8856, Val: 0.9499, Test: 0.9353\n",
      "Epoch: 927, Loss: 0.8856, Train: 0.8855, Val: 0.9498, Test: 0.9352\n",
      "Epoch: 928, Loss: 0.8856, Train: 0.8855, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 929, Loss: 0.8855, Train: 0.8854, Val: 0.9499, Test: 0.9353\n",
      "Epoch: 930, Loss: 0.8854, Train: 0.8853, Val: 0.9497, Test: 0.9352\n",
      "Epoch: 931, Loss: 0.8853, Train: 0.8852, Val: 0.9499, Test: 0.9353\n",
      "Epoch: 932, Loss: 0.8853, Train: 0.8852, Val: 0.9496, Test: 0.9351\n",
      "Epoch: 933, Loss: 0.8852, Train: 0.8851, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 934, Loss: 0.8851, Train: 0.8850, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 935, Loss: 0.8851, Train: 0.8850, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 936, Loss: 0.8850, Train: 0.8849, Val: 0.9496, Test: 0.9351\n",
      "Epoch: 937, Loss: 0.8849, Train: 0.8848, Val: 0.9498, Test: 0.9352\n",
      "Epoch: 938, Loss: 0.8848, Train: 0.8847, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 939, Loss: 0.8848, Train: 0.8847, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 940, Loss: 0.8847, Train: 0.8846, Val: 0.9499, Test: 0.9353\n",
      "Epoch: 941, Loss: 0.8846, Train: 0.8845, Val: 0.9496, Test: 0.9351\n",
      "Epoch: 942, Loss: 0.8846, Train: 0.8845, Val: 0.9497, Test: 0.9352\n",
      "Epoch: 943, Loss: 0.8845, Train: 0.8844, Val: 0.9498, Test: 0.9352\n",
      "Epoch: 944, Loss: 0.8844, Train: 0.8843, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 945, Loss: 0.8844, Train: 0.8843, Val: 0.9498, Test: 0.9352\n",
      "Epoch: 946, Loss: 0.8843, Train: 0.8842, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 947, Loss: 0.8842, Train: 0.8841, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 948, Loss: 0.8841, Train: 0.8840, Val: 0.9497, Test: 0.9352\n",
      "Epoch: 949, Loss: 0.8841, Train: 0.8840, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 950, Loss: 0.8840, Train: 0.8839, Val: 0.9498, Test: 0.9352\n",
      "Epoch: 951, Loss: 0.8839, Train: 0.8838, Val: 0.9495, Test: 0.9350\n",
      "Epoch: 952, Loss: 0.8839, Train: 0.8838, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 953, Loss: 0.8838, Train: 0.8837, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 954, Loss: 0.8837, Train: 0.8836, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 955, Loss: 0.8837, Train: 0.8836, Val: 0.9497, Test: 0.9352\n",
      "Epoch: 956, Loss: 0.8836, Train: 0.8835, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 957, Loss: 0.8835, Train: 0.8834, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 958, Loss: 0.8835, Train: 0.8834, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 959, Loss: 0.8834, Train: 0.8833, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 960, Loss: 0.8833, Train: 0.8832, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 961, Loss: 0.8833, Train: 0.8832, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 962, Loss: 0.8832, Train: 0.8831, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 963, Loss: 0.8831, Train: 0.8830, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 964, Loss: 0.8831, Train: 0.8830, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 965, Loss: 0.8830, Train: 0.8829, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 966, Loss: 0.8829, Train: 0.8828, Val: 0.9498, Test: 0.9351\n",
      "Epoch: 967, Loss: 0.8829, Train: 0.8828, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 968, Loss: 0.8828, Train: 0.8827, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 969, Loss: 0.8827, Train: 0.8826, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 970, Loss: 0.8827, Train: 0.8826, Val: 0.9497, Test: 0.9350\n",
      "Epoch: 971, Loss: 0.8826, Train: 0.8825, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 972, Loss: 0.8825, Train: 0.8824, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 973, Loss: 0.8825, Train: 0.8824, Val: 0.9497, Test: 0.9350\n",
      "Epoch: 974, Loss: 0.8824, Train: 0.8823, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 975, Loss: 0.8824, Train: 0.8823, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 976, Loss: 0.8823, Train: 0.8822, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 977, Loss: 0.8822, Train: 0.8821, Val: 0.9498, Test: 0.9352\n",
      "Epoch: 978, Loss: 0.8822, Train: 0.8821, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 979, Loss: 0.8821, Train: 0.8820, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 980, Loss: 0.8820, Train: 0.8819, Val: 0.9495, Test: 0.9348\n",
      "Epoch: 981, Loss: 0.8820, Train: 0.8819, Val: 0.9496, Test: 0.9349\n",
      "Epoch: 982, Loss: 0.8819, Train: 0.8818, Val: 0.9497, Test: 0.9350\n",
      "Epoch: 983, Loss: 0.8818, Train: 0.8818, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 984, Loss: 0.8818, Train: 0.8817, Val: 0.9498, Test: 0.9351\n",
      "Epoch: 985, Loss: 0.8817, Train: 0.8816, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 986, Loss: 0.8817, Train: 0.8816, Val: 0.9497, Test: 0.9350\n",
      "Epoch: 987, Loss: 0.8816, Train: 0.8815, Val: 0.9495, Test: 0.9348\n",
      "Epoch: 988, Loss: 0.8815, Train: 0.8814, Val: 0.9496, Test: 0.9349\n",
      "Epoch: 989, Loss: 0.8815, Train: 0.8814, Val: 0.9497, Test: 0.9350\n",
      "Epoch: 990, Loss: 0.8814, Train: 0.8813, Val: 0.9494, Test: 0.9347\n",
      "Epoch: 991, Loss: 0.8813, Train: 0.8813, Val: 0.9498, Test: 0.9351\n",
      "Epoch: 992, Loss: 0.8813, Train: 0.8812, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 993, Loss: 0.8812, Train: 0.8811, Val: 0.9497, Test: 0.9350\n",
      "Epoch: 994, Loss: 0.8812, Train: 0.8811, Val: 0.9495, Test: 0.9348\n",
      "Epoch: 995, Loss: 0.8811, Train: 0.8810, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 996, Loss: 0.8810, Train: 0.8809, Val: 0.9497, Test: 0.9350\n",
      "Epoch: 997, Loss: 0.8810, Train: 0.8809, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 998, Loss: 0.8809, Train: 0.8808, Val: 0.9497, Test: 0.9350\n",
      "Epoch: 999, Loss: 0.8808, Train: 0.8808, Val: 0.9495, Test: 0.9348\n",
      "Epoch: 1000, Loss: 0.8808, Train: 0.8807, Val: 0.9497, Test: 0.9350\n",
      "Epoch: 1001, Loss: 0.8807, Train: 0.8806, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 1002, Loss: 0.8807, Train: 0.8806, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 1003, Loss: 0.8806, Train: 0.8805, Val: 0.9495, Test: 0.9348\n",
      "Epoch: 1004, Loss: 0.8805, Train: 0.8805, Val: 0.9496, Test: 0.9349\n",
      "Epoch: 1005, Loss: 0.8805, Train: 0.8804, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 1006, Loss: 0.8804, Train: 0.8803, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 1007, Loss: 0.8804, Train: 0.8803, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 1008, Loss: 0.8803, Train: 0.8802, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 1009, Loss: 0.8802, Train: 0.8802, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 1010, Loss: 0.8802, Train: 0.8801, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 1011, Loss: 0.8801, Train: 0.8800, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 1012, Loss: 0.8801, Train: 0.8800, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 1013, Loss: 0.8800, Train: 0.8799, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 1014, Loss: 0.8800, Train: 0.8799, Val: 0.9493, Test: 0.9347\n",
      "Epoch: 1015, Loss: 0.8799, Train: 0.8798, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 1016, Loss: 0.8798, Train: 0.8798, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 1017, Loss: 0.8798, Train: 0.8797, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 1018, Loss: 0.8797, Train: 0.8796, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 1019, Loss: 0.8797, Train: 0.8796, Val: 0.9495, Test: 0.9350\n",
      "Epoch: 1020, Loss: 0.8796, Train: 0.8795, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 1021, Loss: 0.8795, Train: 0.8795, Val: 0.9494, Test: 0.9349\n",
      "Epoch: 1022, Loss: 0.8795, Train: 0.8794, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 1023, Loss: 0.8794, Train: 0.8793, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 1024, Loss: 0.8794, Train: 0.8793, Val: 0.9497, Test: 0.9352\n",
      "Epoch: 1025, Loss: 0.8793, Train: 0.8792, Val: 0.9494, Test: 0.9348\n",
      "Epoch: 1026, Loss: 0.8793, Train: 0.8792, Val: 0.9496, Test: 0.9351\n",
      "Epoch: 1027, Loss: 0.8792, Train: 0.8791, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 1028, Loss: 0.8791, Train: 0.8790, Val: 0.9496, Test: 0.9350\n",
      "Epoch: 1029, Loss: 0.8791, Train: 0.8790, Val: 0.9497, Test: 0.9351\n",
      "Epoch: 1030, Loss: 0.8790, Train: 0.8789, Val: 0.9495, Test: 0.9349\n",
      "Epoch: 1031, Loss: 0.8790, Train: 0.8789, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 1032, Loss: 0.8789, Train: 0.8788, Val: 0.9495, Test: 0.9350\n",
      "Epoch: 1033, Loss: 0.8788, Train: 0.8788, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 1034, Loss: 0.8788, Train: 0.8787, Val: 0.9496, Test: 0.9351\n",
      "Epoch: 1035, Loss: 0.8787, Train: 0.8786, Val: 0.9499, Test: 0.9353\n",
      "Epoch: 1036, Loss: 0.8786, Train: 0.8786, Val: 0.9498, Test: 0.9352\n",
      "Epoch: 1037, Loss: 0.8786, Train: 0.8785, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 1038, Loss: 0.8785, Train: 0.8784, Val: 0.9500, Test: 0.9354\n",
      "Epoch: 1039, Loss: 0.8785, Train: 0.8784, Val: 0.9500, Test: 0.9353\n",
      "Epoch: 1040, Loss: 0.8784, Train: 0.8783, Val: 0.9502, Test: 0.9356\n",
      "Epoch: 1041, Loss: 0.8783, Train: 0.8783, Val: 0.9499, Test: 0.9353\n",
      "Epoch: 1042, Loss: 0.8783, Train: 0.8782, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 1043, Loss: 0.8783, Train: 0.8782, Val: 0.9499, Test: 0.9353\n",
      "Epoch: 1044, Loss: 0.8782, Train: 0.8781, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 1045, Loss: 0.8782, Train: 0.8781, Val: 0.9498, Test: 0.9352\n",
      "Epoch: 1046, Loss: 0.8781, Train: 0.8780, Val: 0.9504, Test: 0.9358\n",
      "Epoch: 1047, Loss: 0.8780, Train: 0.8779, Val: 0.9502, Test: 0.9356\n",
      "Epoch: 1048, Loss: 0.8779, Train: 0.8779, Val: 0.9500, Test: 0.9354\n",
      "Epoch: 1049, Loss: 0.8779, Train: 0.8778, Val: 0.9505, Test: 0.9359\n",
      "Epoch: 1050, Loss: 0.8778, Train: 0.8778, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 1051, Loss: 0.8778, Train: 0.8777, Val: 0.9506, Test: 0.9360\n",
      "Epoch: 1052, Loss: 0.8777, Train: 0.8777, Val: 0.9500, Test: 0.9354\n",
      "Epoch: 1053, Loss: 0.8777, Train: 0.8776, Val: 0.9502, Test: 0.9356\n",
      "Epoch: 1054, Loss: 0.8776, Train: 0.8775, Val: 0.9502, Test: 0.9357\n",
      "Epoch: 1055, Loss: 0.8775, Train: 0.8775, Val: 0.9498, Test: 0.9353\n",
      "Epoch: 1056, Loss: 0.8775, Train: 0.8774, Val: 0.9503, Test: 0.9358\n",
      "Epoch: 1057, Loss: 0.8774, Train: 0.8774, Val: 0.9499, Test: 0.9354\n",
      "Epoch: 1058, Loss: 0.8774, Train: 0.8773, Val: 0.9501, Test: 0.9356\n",
      "Epoch: 1059, Loss: 0.8773, Train: 0.8772, Val: 0.9501, Test: 0.9357\n",
      "Epoch: 1060, Loss: 0.8773, Train: 0.8772, Val: 0.9499, Test: 0.9355\n",
      "Epoch: 1061, Loss: 0.8772, Train: 0.8771, Val: 0.9503, Test: 0.9359\n",
      "Epoch: 1062, Loss: 0.8772, Train: 0.8771, Val: 0.9498, Test: 0.9354\n",
      "Epoch: 1063, Loss: 0.8771, Train: 0.8770, Val: 0.9503, Test: 0.9358\n",
      "Epoch: 1064, Loss: 0.8770, Train: 0.8770, Val: 0.9500, Test: 0.9355\n",
      "Epoch: 1065, Loss: 0.8770, Train: 0.8769, Val: 0.9501, Test: 0.9356\n",
      "Epoch: 1066, Loss: 0.8769, Train: 0.8769, Val: 0.9502, Test: 0.9357\n",
      "Epoch: 1067, Loss: 0.8769, Train: 0.8768, Val: 0.9500, Test: 0.9355\n",
      "Epoch: 1068, Loss: 0.8768, Train: 0.8768, Val: 0.9503, Test: 0.9359\n",
      "Epoch: 1069, Loss: 0.8768, Train: 0.8767, Val: 0.9501, Test: 0.9356\n",
      "Epoch: 1070, Loss: 0.8767, Train: 0.8766, Val: 0.9502, Test: 0.9357\n",
      "Epoch: 1071, Loss: 0.8767, Train: 0.8766, Val: 0.9502, Test: 0.9358\n",
      "Epoch: 1072, Loss: 0.8766, Train: 0.8765, Val: 0.9501, Test: 0.9357\n",
      "Epoch: 1073, Loss: 0.8765, Train: 0.8765, Val: 0.9503, Test: 0.9359\n",
      "Epoch: 1074, Loss: 0.8765, Train: 0.8764, Val: 0.9500, Test: 0.9357\n",
      "Epoch: 1075, Loss: 0.8764, Train: 0.8764, Val: 0.9504, Test: 0.9360\n",
      "Epoch: 1076, Loss: 0.8764, Train: 0.8763, Val: 0.9500, Test: 0.9356\n",
      "Epoch: 1077, Loss: 0.8763, Train: 0.8763, Val: 0.9504, Test: 0.9361\n",
      "Epoch: 1078, Loss: 0.8763, Train: 0.8762, Val: 0.9500, Test: 0.9356\n",
      "Epoch: 1079, Loss: 0.8763, Train: 0.8762, Val: 0.9506, Test: 0.9363\n",
      "Epoch: 1080, Loss: 0.8762, Train: 0.8762, Val: 0.9499, Test: 0.9355\n",
      "Epoch: 1081, Loss: 0.8762, Train: 0.8761, Val: 0.9506, Test: 0.9363\n",
      "Epoch: 1082, Loss: 0.8761, Train: 0.8760, Val: 0.9500, Test: 0.9357\n",
      "Epoch: 1083, Loss: 0.8760, Train: 0.8760, Val: 0.9504, Test: 0.9361\n",
      "Epoch: 1084, Loss: 0.8760, Train: 0.8759, Val: 0.9503, Test: 0.9360\n",
      "Epoch: 1085, Loss: 0.8759, Train: 0.8759, Val: 0.9501, Test: 0.9358\n",
      "Epoch: 1086, Loss: 0.8759, Train: 0.8758, Val: 0.9506, Test: 0.9363\n",
      "Epoch: 1087, Loss: 0.8758, Train: 0.8758, Val: 0.9500, Test: 0.9357\n",
      "Epoch: 1088, Loss: 0.8758, Train: 0.8757, Val: 0.9506, Test: 0.9364\n",
      "Epoch: 1089, Loss: 0.8757, Train: 0.8757, Val: 0.9501, Test: 0.9358\n",
      "Epoch: 1090, Loss: 0.8757, Train: 0.8756, Val: 0.9505, Test: 0.9362\n",
      "Epoch: 1091, Loss: 0.8756, Train: 0.8755, Val: 0.9504, Test: 0.9361\n",
      "Epoch: 1092, Loss: 0.8756, Train: 0.8755, Val: 0.9502, Test: 0.9360\n",
      "Epoch: 1093, Loss: 0.8755, Train: 0.8755, Val: 0.9505, Test: 0.9363\n",
      "Epoch: 1094, Loss: 0.8755, Train: 0.8754, Val: 0.9502, Test: 0.9359\n",
      "Epoch: 1095, Loss: 0.8754, Train: 0.8754, Val: 0.9506, Test: 0.9363\n",
      "Epoch: 1096, Loss: 0.8754, Train: 0.8753, Val: 0.9502, Test: 0.9359\n",
      "Epoch: 1097, Loss: 0.8753, Train: 0.8752, Val: 0.9505, Test: 0.9363\n",
      "Epoch: 1098, Loss: 0.8753, Train: 0.8752, Val: 0.9504, Test: 0.9362\n",
      "Epoch: 1099, Loss: 0.8752, Train: 0.8751, Val: 0.9503, Test: 0.9361\n",
      "Epoch: 1100, Loss: 0.8752, Train: 0.8751, Val: 0.9505, Test: 0.9363\n",
      "Epoch: 1101, Loss: 0.8751, Train: 0.8751, Val: 0.9503, Test: 0.9361\n",
      "Epoch: 1102, Loss: 0.8751, Train: 0.8750, Val: 0.9506, Test: 0.9364\n",
      "Epoch: 1103, Loss: 0.8750, Train: 0.8750, Val: 0.9503, Test: 0.9361\n",
      "Epoch: 1104, Loss: 0.8750, Train: 0.8749, Val: 0.9507, Test: 0.9364\n",
      "Epoch: 1105, Loss: 0.8749, Train: 0.8749, Val: 0.9503, Test: 0.9361\n",
      "Epoch: 1106, Loss: 0.8749, Train: 0.8748, Val: 0.9508, Test: 0.9365\n",
      "Epoch: 1107, Loss: 0.8748, Train: 0.8748, Val: 0.9503, Test: 0.9361\n",
      "Epoch: 1108, Loss: 0.8748, Train: 0.8747, Val: 0.9508, Test: 0.9366\n",
      "Epoch: 1109, Loss: 0.8747, Train: 0.8747, Val: 0.9503, Test: 0.9360\n",
      "Epoch: 1110, Loss: 0.8747, Train: 0.8746, Val: 0.9509, Test: 0.9366\n",
      "Epoch: 1111, Loss: 0.8746, Train: 0.8746, Val: 0.9504, Test: 0.9361\n",
      "Epoch: 1112, Loss: 0.8746, Train: 0.8745, Val: 0.9507, Test: 0.9365\n",
      "Epoch: 1113, Loss: 0.8745, Train: 0.8745, Val: 0.9505, Test: 0.9363\n",
      "Epoch: 1114, Loss: 0.8745, Train: 0.8744, Val: 0.9507, Test: 0.9364\n",
      "Epoch: 1115, Loss: 0.8744, Train: 0.8744, Val: 0.9507, Test: 0.9365\n",
      "Epoch: 1116, Loss: 0.8744, Train: 0.8743, Val: 0.9505, Test: 0.9363\n",
      "Epoch: 1117, Loss: 0.8743, Train: 0.8743, Val: 0.9508, Test: 0.9366\n",
      "Epoch: 1118, Loss: 0.8743, Train: 0.8742, Val: 0.9505, Test: 0.9363\n",
      "Epoch: 1119, Loss: 0.8742, Train: 0.8742, Val: 0.9510, Test: 0.9367\n",
      "Epoch: 1120, Loss: 0.8742, Train: 0.8741, Val: 0.9505, Test: 0.9363\n",
      "Epoch: 1121, Loss: 0.8742, Train: 0.8741, Val: 0.9510, Test: 0.9368\n",
      "Epoch: 1122, Loss: 0.8741, Train: 0.8740, Val: 0.9506, Test: 0.9363\n",
      "Epoch: 1123, Loss: 0.8741, Train: 0.8740, Val: 0.9510, Test: 0.9367\n",
      "Epoch: 1124, Loss: 0.8740, Train: 0.8739, Val: 0.9506, Test: 0.9364\n",
      "Epoch: 1125, Loss: 0.8740, Train: 0.8739, Val: 0.9509, Test: 0.9366\n",
      "Epoch: 1126, Loss: 0.8739, Train: 0.8738, Val: 0.9508, Test: 0.9365\n",
      "Epoch: 1127, Loss: 0.8739, Train: 0.8738, Val: 0.9508, Test: 0.9366\n",
      "Epoch: 1128, Loss: 0.8738, Train: 0.8737, Val: 0.9508, Test: 0.9366\n",
      "Epoch: 1129, Loss: 0.8738, Train: 0.8737, Val: 0.9508, Test: 0.9366\n",
      "Epoch: 1130, Loss: 0.8737, Train: 0.8737, Val: 0.9510, Test: 0.9367\n",
      "Epoch: 1131, Loss: 0.8737, Train: 0.8736, Val: 0.9507, Test: 0.9365\n",
      "Epoch: 1132, Loss: 0.8736, Train: 0.8736, Val: 0.9512, Test: 0.9370\n",
      "Epoch: 1133, Loss: 0.8736, Train: 0.8736, Val: 0.9506, Test: 0.9364\n",
      "Epoch: 1134, Loss: 0.8736, Train: 0.8735, Val: 0.9514, Test: 0.9372\n",
      "Epoch: 1135, Loss: 0.8735, Train: 0.8735, Val: 0.9505, Test: 0.9363\n",
      "Epoch: 1136, Loss: 0.8735, Train: 0.8734, Val: 0.9513, Test: 0.9371\n",
      "Epoch: 1137, Loss: 0.8734, Train: 0.8733, Val: 0.9508, Test: 0.9366\n",
      "Epoch: 1138, Loss: 0.8734, Train: 0.8733, Val: 0.9510, Test: 0.9368\n",
      "Epoch: 1139, Loss: 0.8733, Train: 0.8732, Val: 0.9511, Test: 0.9369\n",
      "Epoch: 1140, Loss: 0.8733, Train: 0.8732, Val: 0.9508, Test: 0.9366\n",
      "Epoch: 1141, Loss: 0.8732, Train: 0.8732, Val: 0.9513, Test: 0.9371\n",
      "Epoch: 1142, Loss: 0.8732, Train: 0.8731, Val: 0.9507, Test: 0.9365\n",
      "Epoch: 1143, Loss: 0.8731, Train: 0.8731, Val: 0.9513, Test: 0.9371\n",
      "Epoch: 1144, Loss: 0.8731, Train: 0.8730, Val: 0.9509, Test: 0.9367\n",
      "Epoch: 1145, Loss: 0.8730, Train: 0.8730, Val: 0.9510, Test: 0.9369\n",
      "Epoch: 1146, Loss: 0.8730, Train: 0.8729, Val: 0.9511, Test: 0.9370\n",
      "Epoch: 1147, Loss: 0.8729, Train: 0.8729, Val: 0.9508, Test: 0.9367\n",
      "Epoch: 1148, Loss: 0.8729, Train: 0.8728, Val: 0.9513, Test: 0.9372\n",
      "Epoch: 1149, Loss: 0.8729, Train: 0.8728, Val: 0.9508, Test: 0.9366\n",
      "Epoch: 1150, Loss: 0.8728, Train: 0.8728, Val: 0.9514, Test: 0.9372\n",
      "Epoch: 1151, Loss: 0.8728, Train: 0.8727, Val: 0.9509, Test: 0.9367\n",
      "Epoch: 1152, Loss: 0.8727, Train: 0.8726, Val: 0.9512, Test: 0.9371\n",
      "Epoch: 1153, Loss: 0.8727, Train: 0.8726, Val: 0.9511, Test: 0.9369\n",
      "Epoch: 1154, Loss: 0.8726, Train: 0.8726, Val: 0.9511, Test: 0.9369\n",
      "Epoch: 1155, Loss: 0.8726, Train: 0.8725, Val: 0.9514, Test: 0.9372\n",
      "Epoch: 1156, Loss: 0.8725, Train: 0.8725, Val: 0.9509, Test: 0.9367\n",
      "Epoch: 1157, Loss: 0.8725, Train: 0.8724, Val: 0.9514, Test: 0.9373\n",
      "Epoch: 1158, Loss: 0.8725, Train: 0.8724, Val: 0.9509, Test: 0.9367\n",
      "Epoch: 1159, Loss: 0.8724, Train: 0.8723, Val: 0.9515, Test: 0.9373\n",
      "Epoch: 1160, Loss: 0.8724, Train: 0.8723, Val: 0.9509, Test: 0.9368\n",
      "Epoch: 1161, Loss: 0.8723, Train: 0.8722, Val: 0.9514, Test: 0.9372\n",
      "Epoch: 1162, Loss: 0.8723, Train: 0.8722, Val: 0.9511, Test: 0.9370\n",
      "Epoch: 1163, Loss: 0.8722, Train: 0.8721, Val: 0.9512, Test: 0.9371\n",
      "Epoch: 1164, Loss: 0.8722, Train: 0.8721, Val: 0.9513, Test: 0.9372\n",
      "Epoch: 1165, Loss: 0.8721, Train: 0.8721, Val: 0.9511, Test: 0.9369\n",
      "Epoch: 1166, Loss: 0.8721, Train: 0.8720, Val: 0.9515, Test: 0.9374\n",
      "Epoch: 1167, Loss: 0.8720, Train: 0.8720, Val: 0.9510, Test: 0.9369\n",
      "Epoch: 1168, Loss: 0.8720, Train: 0.8720, Val: 0.9516, Test: 0.9375\n",
      "Epoch: 1169, Loss: 0.8720, Train: 0.8719, Val: 0.9510, Test: 0.9368\n",
      "Epoch: 1170, Loss: 0.8719, Train: 0.8719, Val: 0.9516, Test: 0.9375\n",
      "Epoch: 1171, Loss: 0.8719, Train: 0.8718, Val: 0.9511, Test: 0.9370\n",
      "Epoch: 1172, Loss: 0.8718, Train: 0.8718, Val: 0.9514, Test: 0.9373\n",
      "Epoch: 1173, Loss: 0.8718, Train: 0.8717, Val: 0.9513, Test: 0.9372\n",
      "Epoch: 1174, Loss: 0.8717, Train: 0.8717, Val: 0.9512, Test: 0.9371\n",
      "Epoch: 1175, Loss: 0.8717, Train: 0.8716, Val: 0.9516, Test: 0.9375\n",
      "Epoch: 1176, Loss: 0.8716, Train: 0.8716, Val: 0.9512, Test: 0.9371\n",
      "Epoch: 1177, Loss: 0.8716, Train: 0.8715, Val: 0.9515, Test: 0.9374\n",
      "Epoch: 1178, Loss: 0.8715, Train: 0.8715, Val: 0.9513, Test: 0.9372\n",
      "Epoch: 1179, Loss: 0.8715, Train: 0.8714, Val: 0.9515, Test: 0.9373\n",
      "Epoch: 1180, Loss: 0.8715, Train: 0.8714, Val: 0.9515, Test: 0.9374\n",
      "Epoch: 1181, Loss: 0.8714, Train: 0.8714, Val: 0.9514, Test: 0.9373\n",
      "Epoch: 1182, Loss: 0.8714, Train: 0.8713, Val: 0.9516, Test: 0.9375\n",
      "Epoch: 1183, Loss: 0.8713, Train: 0.8713, Val: 0.9514, Test: 0.9372\n",
      "Epoch: 1184, Loss: 0.8713, Train: 0.8712, Val: 0.9517, Test: 0.9376\n",
      "Epoch: 1185, Loss: 0.8712, Train: 0.8712, Val: 0.9513, Test: 0.9372\n",
      "Epoch: 1186, Loss: 0.8712, Train: 0.8712, Val: 0.9519, Test: 0.9378\n",
      "Epoch: 1187, Loss: 0.8712, Train: 0.8711, Val: 0.9512, Test: 0.9371\n",
      "Epoch: 1188, Loss: 0.8711, Train: 0.8711, Val: 0.9520, Test: 0.9379\n",
      "Epoch: 1189, Loss: 0.8711, Train: 0.8710, Val: 0.9512, Test: 0.9371\n",
      "Epoch: 1190, Loss: 0.8711, Train: 0.8710, Val: 0.9520, Test: 0.9379\n",
      "Epoch: 1191, Loss: 0.8710, Train: 0.8709, Val: 0.9513, Test: 0.9372\n",
      "Epoch: 1192, Loss: 0.8710, Train: 0.8709, Val: 0.9518, Test: 0.9377\n",
      "Epoch: 1193, Loss: 0.8709, Train: 0.8708, Val: 0.9516, Test: 0.9375\n",
      "Epoch: 1194, Loss: 0.8708, Train: 0.8708, Val: 0.9516, Test: 0.9375\n",
      "Epoch: 1195, Loss: 0.8708, Train: 0.8708, Val: 0.9518, Test: 0.9378\n",
      "Epoch: 1196, Loss: 0.8708, Train: 0.8707, Val: 0.9515, Test: 0.9375\n",
      "Epoch: 1197, Loss: 0.8707, Train: 0.8707, Val: 0.9518, Test: 0.9378\n",
      "Epoch: 1198, Loss: 0.8707, Train: 0.8706, Val: 0.9517, Test: 0.9376\n",
      "Epoch: 1199, Loss: 0.8706, Train: 0.8706, Val: 0.9517, Test: 0.9377\n",
      "Epoch: 1200, Loss: 0.8706, Train: 0.8705, Val: 0.9518, Test: 0.9378\n",
      "Epoch: 1201, Loss: 0.8705, Train: 0.8705, Val: 0.9516, Test: 0.9376\n",
      "Epoch: 1202, Loss: 0.8705, Train: 0.8705, Val: 0.9519, Test: 0.9379\n",
      "Epoch: 1203, Loss: 0.8705, Train: 0.8704, Val: 0.9515, Test: 0.9375\n",
      "Epoch: 1204, Loss: 0.8704, Train: 0.8704, Val: 0.9521, Test: 0.9382\n",
      "Epoch: 1205, Loss: 0.8704, Train: 0.8704, Val: 0.9513, Test: 0.9373\n",
      "Epoch: 1206, Loss: 0.8704, Train: 0.8703, Val: 0.9523, Test: 0.9384\n",
      "Epoch: 1207, Loss: 0.8703, Train: 0.8703, Val: 0.9514, Test: 0.9374\n",
      "Epoch: 1208, Loss: 0.8703, Train: 0.8702, Val: 0.9522, Test: 0.9383\n",
      "Epoch: 1209, Loss: 0.8702, Train: 0.8702, Val: 0.9516, Test: 0.9377\n",
      "Epoch: 1210, Loss: 0.8702, Train: 0.8701, Val: 0.9519, Test: 0.9380\n",
      "Epoch: 1211, Loss: 0.8701, Train: 0.8701, Val: 0.9519, Test: 0.9380\n",
      "Epoch: 1212, Loss: 0.8701, Train: 0.8700, Val: 0.9517, Test: 0.9378\n",
      "Epoch: 1213, Loss: 0.8700, Train: 0.8700, Val: 0.9521, Test: 0.9382\n",
      "Epoch: 1214, Loss: 0.8700, Train: 0.8700, Val: 0.9516, Test: 0.9377\n",
      "Epoch: 1215, Loss: 0.8700, Train: 0.8699, Val: 0.9522, Test: 0.9384\n",
      "Epoch: 1216, Loss: 0.8699, Train: 0.8699, Val: 0.9516, Test: 0.9377\n",
      "Epoch: 1217, Loss: 0.8699, Train: 0.8698, Val: 0.9522, Test: 0.9384\n",
      "Epoch: 1218, Loss: 0.8698, Train: 0.8698, Val: 0.9516, Test: 0.9378\n",
      "Epoch: 1219, Loss: 0.8698, Train: 0.8697, Val: 0.9522, Test: 0.9383\n",
      "Epoch: 1220, Loss: 0.8697, Train: 0.8697, Val: 0.9518, Test: 0.9379\n",
      "Epoch: 1221, Loss: 0.8697, Train: 0.8696, Val: 0.9521, Test: 0.9382\n",
      "Epoch: 1222, Loss: 0.8697, Train: 0.8696, Val: 0.9519, Test: 0.9381\n",
      "Epoch: 1223, Loss: 0.8696, Train: 0.8696, Val: 0.9520, Test: 0.9381\n",
      "Epoch: 1224, Loss: 0.8696, Train: 0.8695, Val: 0.9521, Test: 0.9383\n",
      "Epoch: 1225, Loss: 0.8695, Train: 0.8695, Val: 0.9519, Test: 0.9381\n",
      "Epoch: 1226, Loss: 0.8695, Train: 0.8694, Val: 0.9523, Test: 0.9385\n",
      "Epoch: 1227, Loss: 0.8695, Train: 0.8694, Val: 0.9517, Test: 0.9379\n",
      "Epoch: 1228, Loss: 0.8694, Train: 0.8694, Val: 0.9526, Test: 0.9388\n",
      "Epoch: 1229, Loss: 0.8694, Train: 0.8694, Val: 0.9516, Test: 0.9377\n",
      "Epoch: 1230, Loss: 0.8694, Train: 0.8693, Val: 0.9526, Test: 0.9388\n",
      "Epoch: 1231, Loss: 0.8693, Train: 0.8693, Val: 0.9517, Test: 0.9379\n",
      "Epoch: 1232, Loss: 0.8693, Train: 0.8692, Val: 0.9523, Test: 0.9385\n",
      "Epoch: 1233, Loss: 0.8692, Train: 0.8691, Val: 0.9521, Test: 0.9383\n",
      "Epoch: 1234, Loss: 0.8692, Train: 0.8691, Val: 0.9520, Test: 0.9382\n",
      "Epoch: 1235, Loss: 0.8691, Train: 0.8691, Val: 0.9524, Test: 0.9387\n",
      "Epoch: 1236, Loss: 0.8691, Train: 0.8690, Val: 0.9519, Test: 0.9381\n",
      "Epoch: 1237, Loss: 0.8691, Train: 0.8690, Val: 0.9525, Test: 0.9388\n",
      "Epoch: 1238, Loss: 0.8690, Train: 0.8690, Val: 0.9520, Test: 0.9382\n",
      "Epoch: 1239, Loss: 0.8690, Train: 0.8689, Val: 0.9524, Test: 0.9387\n",
      "Epoch: 1240, Loss: 0.8689, Train: 0.8689, Val: 0.9521, Test: 0.9383\n",
      "Epoch: 1241, Loss: 0.8689, Train: 0.8688, Val: 0.9523, Test: 0.9385\n",
      "Epoch: 1242, Loss: 0.8688, Train: 0.8688, Val: 0.9524, Test: 0.9385\n",
      "Epoch: 1243, Loss: 0.8688, Train: 0.8687, Val: 0.9522, Test: 0.9384\n",
      "Epoch: 1244, Loss: 0.8687, Train: 0.8687, Val: 0.9525, Test: 0.9387\n",
      "Epoch: 1245, Loss: 0.8687, Train: 0.8687, Val: 0.9520, Test: 0.9382\n",
      "Epoch: 1246, Loss: 0.8687, Train: 0.8687, Val: 0.9528, Test: 0.9390\n",
      "Epoch: 1247, Loss: 0.8687, Train: 0.8686, Val: 0.9519, Test: 0.9381\n",
      "Epoch: 1248, Loss: 0.8687, Train: 0.8686, Val: 0.9531, Test: 0.9393\n",
      "Epoch: 1249, Loss: 0.8686, Train: 0.8686, Val: 0.9519, Test: 0.9381\n",
      "Epoch: 1250, Loss: 0.8686, Train: 0.8685, Val: 0.9529, Test: 0.9391\n",
      "Epoch: 1251, Loss: 0.8685, Train: 0.8684, Val: 0.9522, Test: 0.9384\n",
      "Epoch: 1252, Loss: 0.8684, Train: 0.8684, Val: 0.9525, Test: 0.9387\n",
      "Epoch: 1253, Loss: 0.8684, Train: 0.8683, Val: 0.9526, Test: 0.9388\n",
      "Epoch: 1254, Loss: 0.8683, Train: 0.8683, Val: 0.9522, Test: 0.9384\n",
      "Epoch: 1255, Loss: 0.8683, Train: 0.8683, Val: 0.9529, Test: 0.9391\n",
      "Epoch: 1256, Loss: 0.8683, Train: 0.8682, Val: 0.9522, Test: 0.9384\n",
      "Epoch: 1257, Loss: 0.8683, Train: 0.8682, Val: 0.9529, Test: 0.9391\n",
      "Epoch: 1258, Loss: 0.8682, Train: 0.8681, Val: 0.9524, Test: 0.9386\n",
      "Epoch: 1259, Loss: 0.8681, Train: 0.8681, Val: 0.9527, Test: 0.9389\n",
      "Epoch: 1260, Loss: 0.8681, Train: 0.8680, Val: 0.9527, Test: 0.9389\n",
      "Epoch: 1261, Loss: 0.8681, Train: 0.8680, Val: 0.9525, Test: 0.9387\n",
      "Epoch: 1262, Loss: 0.8680, Train: 0.8680, Val: 0.9529, Test: 0.9391\n",
      "Epoch: 1263, Loss: 0.8680, Train: 0.8679, Val: 0.9524, Test: 0.9386\n",
      "Epoch: 1264, Loss: 0.8680, Train: 0.8679, Val: 0.9532, Test: 0.9394\n",
      "Epoch: 1265, Loss: 0.8679, Train: 0.8679, Val: 0.9523, Test: 0.9385\n",
      "Epoch: 1266, Loss: 0.8679, Train: 0.8679, Val: 0.9534, Test: 0.9396\n",
      "Epoch: 1267, Loss: 0.8679, Train: 0.8678, Val: 0.9524, Test: 0.9385\n",
      "Epoch: 1268, Loss: 0.8678, Train: 0.8678, Val: 0.9534, Test: 0.9396\n",
      "Epoch: 1269, Loss: 0.8678, Train: 0.8677, Val: 0.9525, Test: 0.9387\n",
      "Epoch: 1270, Loss: 0.8677, Train: 0.8677, Val: 0.9532, Test: 0.9394\n",
      "Epoch: 1271, Loss: 0.8677, Train: 0.8676, Val: 0.9528, Test: 0.9390\n",
      "Epoch: 1272, Loss: 0.8676, Train: 0.8676, Val: 0.9530, Test: 0.9392\n",
      "Epoch: 1273, Loss: 0.8676, Train: 0.8675, Val: 0.9531, Test: 0.9393\n",
      "Epoch: 1274, Loss: 0.8675, Train: 0.8675, Val: 0.9528, Test: 0.9390\n",
      "Epoch: 1275, Loss: 0.8675, Train: 0.8675, Val: 0.9533, Test: 0.9395\n",
      "Epoch: 1276, Loss: 0.8675, Train: 0.8674, Val: 0.9526, Test: 0.9388\n",
      "Epoch: 1277, Loss: 0.8674, Train: 0.8674, Val: 0.9535, Test: 0.9398\n",
      "Epoch: 1278, Loss: 0.8674, Train: 0.8674, Val: 0.9526, Test: 0.9388\n",
      "Epoch: 1279, Loss: 0.8674, Train: 0.8674, Val: 0.9537, Test: 0.9400\n",
      "Epoch: 1280, Loss: 0.8674, Train: 0.8673, Val: 0.9526, Test: 0.9388\n",
      "Epoch: 1281, Loss: 0.8673, Train: 0.8673, Val: 0.9536, Test: 0.9399\n",
      "Epoch: 1282, Loss: 0.8673, Train: 0.8672, Val: 0.9528, Test: 0.9390\n",
      "Epoch: 1283, Loss: 0.8672, Train: 0.8671, Val: 0.9532, Test: 0.9395\n",
      "Epoch: 1284, Loss: 0.8671, Train: 0.8671, Val: 0.9531, Test: 0.9394\n",
      "Epoch: 1285, Loss: 0.8671, Train: 0.8671, Val: 0.9529, Test: 0.9391\n",
      "Epoch: 1286, Loss: 0.8671, Train: 0.8670, Val: 0.9534, Test: 0.9397\n",
      "Epoch: 1287, Loss: 0.8670, Train: 0.8670, Val: 0.9528, Test: 0.9391\n",
      "Epoch: 1288, Loss: 0.8670, Train: 0.8670, Val: 0.9534, Test: 0.9397\n",
      "Epoch: 1289, Loss: 0.8670, Train: 0.8669, Val: 0.9529, Test: 0.9392\n",
      "Epoch: 1290, Loss: 0.8669, Train: 0.8669, Val: 0.9534, Test: 0.9397\n",
      "Epoch: 1291, Loss: 0.8669, Train: 0.8668, Val: 0.9531, Test: 0.9394\n",
      "Epoch: 1292, Loss: 0.8668, Train: 0.8668, Val: 0.9532, Test: 0.9396\n",
      "Epoch: 1293, Loss: 0.8668, Train: 0.8667, Val: 0.9532, Test: 0.9396\n",
      "Epoch: 1294, Loss: 0.8667, Train: 0.8667, Val: 0.9531, Test: 0.9395\n",
      "Epoch: 1295, Loss: 0.8667, Train: 0.8667, Val: 0.9534, Test: 0.9398\n",
      "Epoch: 1296, Loss: 0.8667, Train: 0.8666, Val: 0.9529, Test: 0.9393\n",
      "Epoch: 1297, Loss: 0.8667, Train: 0.8667, Val: 0.9538, Test: 0.9402\n",
      "Epoch: 1298, Loss: 0.8667, Train: 0.8667, Val: 0.9526, Test: 0.9390\n",
      "Epoch: 1299, Loss: 0.8667, Train: 0.8666, Val: 0.9540, Test: 0.9404\n",
      "Epoch: 1300, Loss: 0.8666, Train: 0.8665, Val: 0.9528, Test: 0.9391\n",
      "Epoch: 1301, Loss: 0.8665, Train: 0.8665, Val: 0.9536, Test: 0.9400\n",
      "Epoch: 1302, Loss: 0.8665, Train: 0.8664, Val: 0.9532, Test: 0.9396\n",
      "Epoch: 1303, Loss: 0.8664, Train: 0.8664, Val: 0.9533, Test: 0.9397\n",
      "Epoch: 1304, Loss: 0.8664, Train: 0.8663, Val: 0.9535, Test: 0.9399\n",
      "Epoch: 1305, Loss: 0.8663, Train: 0.8663, Val: 0.9531, Test: 0.9395\n",
      "Epoch: 1306, Loss: 0.8663, Train: 0.8663, Val: 0.9538, Test: 0.9402\n",
      "Epoch: 1307, Loss: 0.8663, Train: 0.8663, Val: 0.9529, Test: 0.9393\n",
      "Epoch: 1308, Loss: 0.8663, Train: 0.8662, Val: 0.9539, Test: 0.9403\n",
      "Epoch: 1309, Loss: 0.8662, Train: 0.8662, Val: 0.9531, Test: 0.9395\n",
      "Epoch: 1310, Loss: 0.8662, Train: 0.8661, Val: 0.9536, Test: 0.9401\n",
      "Epoch: 1311, Loss: 0.8661, Train: 0.8661, Val: 0.9533, Test: 0.9398\n",
      "Epoch: 1312, Loss: 0.8661, Train: 0.8660, Val: 0.9534, Test: 0.9398\n",
      "Epoch: 1313, Loss: 0.8660, Train: 0.8660, Val: 0.9535, Test: 0.9400\n",
      "Epoch: 1314, Loss: 0.8660, Train: 0.8659, Val: 0.9533, Test: 0.9397\n",
      "Epoch: 1315, Loss: 0.8660, Train: 0.8659, Val: 0.9537, Test: 0.9402\n",
      "Epoch: 1316, Loss: 0.8659, Train: 0.8659, Val: 0.9532, Test: 0.9396\n",
      "Epoch: 1317, Loss: 0.8659, Train: 0.8659, Val: 0.9539, Test: 0.9404\n",
      "Epoch: 1318, Loss: 0.8659, Train: 0.8658, Val: 0.9531, Test: 0.9396\n",
      "Epoch: 1319, Loss: 0.8658, Train: 0.8658, Val: 0.9540, Test: 0.9405\n",
      "Epoch: 1320, Loss: 0.8658, Train: 0.8658, Val: 0.9531, Test: 0.9396\n",
      "Epoch: 1321, Loss: 0.8658, Train: 0.8657, Val: 0.9540, Test: 0.9405\n",
      "Epoch: 1322, Loss: 0.8657, Train: 0.8657, Val: 0.9532, Test: 0.9397\n",
      "Epoch: 1323, Loss: 0.8657, Train: 0.8656, Val: 0.9539, Test: 0.9404\n",
      "Epoch: 1324, Loss: 0.8656, Train: 0.8656, Val: 0.9534, Test: 0.9399\n",
      "Epoch: 1325, Loss: 0.8656, Train: 0.8655, Val: 0.9536, Test: 0.9402\n",
      "Epoch: 1326, Loss: 0.8655, Train: 0.8655, Val: 0.9538, Test: 0.9403\n",
      "Epoch: 1327, Loss: 0.8655, Train: 0.8655, Val: 0.9535, Test: 0.9400\n",
      "Epoch: 1328, Loss: 0.8655, Train: 0.8654, Val: 0.9539, Test: 0.9405\n",
      "Epoch: 1329, Loss: 0.8655, Train: 0.8654, Val: 0.9534, Test: 0.9399\n",
      "Epoch: 1330, Loss: 0.8654, Train: 0.8654, Val: 0.9541, Test: 0.9406\n",
      "Epoch: 1331, Loss: 0.8654, Train: 0.8654, Val: 0.9533, Test: 0.9398\n",
      "Epoch: 1332, Loss: 0.8654, Train: 0.8653, Val: 0.9543, Test: 0.9408\n",
      "Epoch: 1333, Loss: 0.8653, Train: 0.8653, Val: 0.9532, Test: 0.9397\n",
      "Epoch: 1334, Loss: 0.8653, Train: 0.8653, Val: 0.9543, Test: 0.9409\n",
      "Epoch: 1335, Loss: 0.8653, Train: 0.8652, Val: 0.9534, Test: 0.9399\n",
      "Epoch: 1336, Loss: 0.8652, Train: 0.8652, Val: 0.9542, Test: 0.9407\n",
      "Epoch: 1337, Loss: 0.8652, Train: 0.8651, Val: 0.9537, Test: 0.9403\n",
      "Epoch: 1338, Loss: 0.8651, Train: 0.8651, Val: 0.9538, Test: 0.9403\n",
      "Epoch: 1339, Loss: 0.8651, Train: 0.8650, Val: 0.9542, Test: 0.9407\n",
      "Epoch: 1340, Loss: 0.8651, Train: 0.8650, Val: 0.9536, Test: 0.9401\n",
      "Epoch: 1341, Loss: 0.8650, Train: 0.8650, Val: 0.9544, Test: 0.9409\n",
      "Epoch: 1342, Loss: 0.8650, Train: 0.8650, Val: 0.9536, Test: 0.9401\n",
      "Epoch: 1343, Loss: 0.8650, Train: 0.8649, Val: 0.9543, Test: 0.9409\n",
      "Epoch: 1344, Loss: 0.8649, Train: 0.8649, Val: 0.9538, Test: 0.9403\n",
      "Epoch: 1345, Loss: 0.8649, Train: 0.8648, Val: 0.9542, Test: 0.9407\n",
      "Epoch: 1346, Loss: 0.8648, Train: 0.8648, Val: 0.9540, Test: 0.9405\n",
      "Epoch: 1347, Loss: 0.8648, Train: 0.8647, Val: 0.9541, Test: 0.9406\n",
      "Epoch: 1348, Loss: 0.8648, Train: 0.8647, Val: 0.9542, Test: 0.9407\n",
      "Epoch: 1349, Loss: 0.8647, Train: 0.8647, Val: 0.9540, Test: 0.9405\n",
      "Epoch: 1350, Loss: 0.8647, Train: 0.8647, Val: 0.9545, Test: 0.9410\n",
      "Epoch: 1351, Loss: 0.8647, Train: 0.8647, Val: 0.9537, Test: 0.9402\n",
      "Epoch: 1352, Loss: 0.8647, Train: 0.8647, Val: 0.9550, Test: 0.9416\n",
      "Epoch: 1353, Loss: 0.8647, Train: 0.8647, Val: 0.9534, Test: 0.9399\n",
      "Epoch: 1354, Loss: 0.8647, Train: 0.8646, Val: 0.9550, Test: 0.9415\n",
      "Epoch: 1355, Loss: 0.8646, Train: 0.8645, Val: 0.9537, Test: 0.9402\n",
      "Epoch: 1356, Loss: 0.8645, Train: 0.8644, Val: 0.9545, Test: 0.9410\n",
      "Epoch: 1357, Loss: 0.8644, Train: 0.8644, Val: 0.9543, Test: 0.9408\n",
      "Epoch: 1358, Loss: 0.8644, Train: 0.8644, Val: 0.9541, Test: 0.9406\n",
      "Epoch: 1359, Loss: 0.8644, Train: 0.8644, Val: 0.9548, Test: 0.9413\n",
      "Epoch: 1360, Loss: 0.8644, Train: 0.8644, Val: 0.9538, Test: 0.9403\n",
      "Epoch: 1361, Loss: 0.8644, Train: 0.8643, Val: 0.9551, Test: 0.9415\n",
      "Epoch: 1362, Loss: 0.8643, Train: 0.8643, Val: 0.9540, Test: 0.9404\n",
      "Epoch: 1363, Loss: 0.8643, Train: 0.8642, Val: 0.9549, Test: 0.9414\n",
      "Epoch: 1364, Loss: 0.8642, Train: 0.8642, Val: 0.9542, Test: 0.9406\n",
      "Epoch: 1365, Loss: 0.8642, Train: 0.8641, Val: 0.9546, Test: 0.9410\n",
      "Epoch: 1366, Loss: 0.8641, Train: 0.8641, Val: 0.9546, Test: 0.9410\n",
      "Epoch: 1367, Loss: 0.8641, Train: 0.8641, Val: 0.9543, Test: 0.9407\n",
      "Epoch: 1368, Loss: 0.8641, Train: 0.8640, Val: 0.9549, Test: 0.9413\n",
      "Epoch: 1369, Loss: 0.8640, Train: 0.8640, Val: 0.9542, Test: 0.9406\n",
      "Epoch: 1370, Loss: 0.8640, Train: 0.8640, Val: 0.9551, Test: 0.9414\n",
      "Epoch: 1371, Loss: 0.8640, Train: 0.8640, Val: 0.9542, Test: 0.9406\n",
      "Epoch: 1372, Loss: 0.8640, Train: 0.8639, Val: 0.9551, Test: 0.9414\n",
      "Epoch: 1373, Loss: 0.8639, Train: 0.8639, Val: 0.9543, Test: 0.9407\n",
      "Epoch: 1374, Loss: 0.8639, Train: 0.8638, Val: 0.9550, Test: 0.9414\n",
      "Epoch: 1375, Loss: 0.8638, Train: 0.8638, Val: 0.9545, Test: 0.9408\n",
      "Epoch: 1376, Loss: 0.8638, Train: 0.8637, Val: 0.9549, Test: 0.9412\n",
      "Epoch: 1377, Loss: 0.8638, Train: 0.8637, Val: 0.9548, Test: 0.9411\n",
      "Epoch: 1378, Loss: 0.8637, Train: 0.8637, Val: 0.9547, Test: 0.9410\n",
      "Epoch: 1379, Loss: 0.8637, Train: 0.8636, Val: 0.9550, Test: 0.9413\n",
      "Epoch: 1380, Loss: 0.8637, Train: 0.8636, Val: 0.9546, Test: 0.9409\n",
      "Epoch: 1381, Loss: 0.8636, Train: 0.8636, Val: 0.9553, Test: 0.9415\n",
      "Epoch: 1382, Loss: 0.8636, Train: 0.8636, Val: 0.9544, Test: 0.9406\n",
      "Epoch: 1383, Loss: 0.8636, Train: 0.8636, Val: 0.9557, Test: 0.9420\n",
      "Epoch: 1384, Loss: 0.8636, Train: 0.8636, Val: 0.9542, Test: 0.9404\n",
      "Epoch: 1385, Loss: 0.8636, Train: 0.8635, Val: 0.9558, Test: 0.9421\n",
      "Epoch: 1386, Loss: 0.8635, Train: 0.8635, Val: 0.9544, Test: 0.9407\n",
      "Epoch: 1387, Loss: 0.8635, Train: 0.8634, Val: 0.9554, Test: 0.9416\n",
      "Epoch: 1388, Loss: 0.8634, Train: 0.8633, Val: 0.9550, Test: 0.9412\n",
      "Epoch: 1389, Loss: 0.8633, Train: 0.8633, Val: 0.9550, Test: 0.9412\n",
      "Epoch: 1390, Loss: 0.8633, Train: 0.8633, Val: 0.9555, Test: 0.9417\n",
      "Epoch: 1391, Loss: 0.8633, Train: 0.8633, Val: 0.9547, Test: 0.9409\n",
      "Epoch: 1392, Loss: 0.8633, Train: 0.8632, Val: 0.9557, Test: 0.9419\n",
      "Epoch: 1393, Loss: 0.8632, Train: 0.8632, Val: 0.9548, Test: 0.9410\n",
      "Epoch: 1394, Loss: 0.8632, Train: 0.8631, Val: 0.9555, Test: 0.9417\n",
      "Epoch: 1395, Loss: 0.8631, Train: 0.8631, Val: 0.9552, Test: 0.9414\n",
      "Epoch: 1396, Loss: 0.8631, Train: 0.8631, Val: 0.9551, Test: 0.9414\n",
      "Epoch: 1397, Loss: 0.8631, Train: 0.8630, Val: 0.9557, Test: 0.9419\n",
      "Epoch: 1398, Loss: 0.8631, Train: 0.8630, Val: 0.9548, Test: 0.9411\n",
      "Epoch: 1399, Loss: 0.8630, Train: 0.8630, Val: 0.9560, Test: 0.9422\n",
      "Epoch: 1400, Loss: 0.8630, Train: 0.8630, Val: 0.9548, Test: 0.9410\n",
      "Epoch: 1401, Loss: 0.8630, Train: 0.8629, Val: 0.9560, Test: 0.9422\n",
      "Epoch: 1402, Loss: 0.8629, Train: 0.8629, Val: 0.9550, Test: 0.9412\n",
      "Epoch: 1403, Loss: 0.8629, Train: 0.8628, Val: 0.9558, Test: 0.9420\n",
      "Epoch: 1404, Loss: 0.8628, Train: 0.8628, Val: 0.9554, Test: 0.9416\n",
      "Epoch: 1405, Loss: 0.8628, Train: 0.8628, Val: 0.9554, Test: 0.9416\n",
      "Epoch: 1406, Loss: 0.8628, Train: 0.8627, Val: 0.9558, Test: 0.9420\n",
      "Epoch: 1407, Loss: 0.8627, Train: 0.8627, Val: 0.9553, Test: 0.9415\n",
      "Epoch: 1408, Loss: 0.8627, Train: 0.8627, Val: 0.9560, Test: 0.9422\n",
      "Epoch: 1409, Loss: 0.8627, Train: 0.8627, Val: 0.9551, Test: 0.9413\n",
      "Epoch: 1410, Loss: 0.8627, Train: 0.8626, Val: 0.9562, Test: 0.9424\n",
      "Epoch: 1411, Loss: 0.8626, Train: 0.8626, Val: 0.9551, Test: 0.9413\n",
      "Epoch: 1412, Loss: 0.8626, Train: 0.8626, Val: 0.9564, Test: 0.9426\n",
      "Epoch: 1413, Loss: 0.8626, Train: 0.8625, Val: 0.9551, Test: 0.9413\n",
      "Epoch: 1414, Loss: 0.8626, Train: 0.8625, Val: 0.9563, Test: 0.9425\n",
      "Epoch: 1415, Loss: 0.8625, Train: 0.8624, Val: 0.9554, Test: 0.9416\n",
      "Epoch: 1416, Loss: 0.8624, Train: 0.8624, Val: 0.9559, Test: 0.9421\n",
      "Epoch: 1417, Loss: 0.8624, Train: 0.8624, Val: 0.9559, Test: 0.9421\n",
      "Epoch: 1418, Loss: 0.8624, Train: 0.8623, Val: 0.9555, Test: 0.9417\n",
      "Epoch: 1419, Loss: 0.8623, Train: 0.8623, Val: 0.9562, Test: 0.9425\n",
      "Epoch: 1420, Loss: 0.8623, Train: 0.8623, Val: 0.9554, Test: 0.9416\n",
      "Epoch: 1421, Loss: 0.8623, Train: 0.8622, Val: 0.9563, Test: 0.9425\n",
      "Epoch: 1422, Loss: 0.8623, Train: 0.8622, Val: 0.9556, Test: 0.9418\n",
      "Epoch: 1423, Loss: 0.8622, Train: 0.8622, Val: 0.9562, Test: 0.9423\n",
      "Epoch: 1424, Loss: 0.8622, Train: 0.8621, Val: 0.9558, Test: 0.9420\n",
      "Epoch: 1425, Loss: 0.8621, Train: 0.8621, Val: 0.9560, Test: 0.9422\n",
      "Epoch: 1426, Loss: 0.8621, Train: 0.8620, Val: 0.9561, Test: 0.9423\n",
      "Epoch: 1427, Loss: 0.8621, Train: 0.8620, Val: 0.9559, Test: 0.9421\n",
      "Epoch: 1428, Loss: 0.8620, Train: 0.8620, Val: 0.9563, Test: 0.9425\n",
      "Epoch: 1429, Loss: 0.8620, Train: 0.8620, Val: 0.9556, Test: 0.9418\n",
      "Epoch: 1430, Loss: 0.8620, Train: 0.8620, Val: 0.9569, Test: 0.9431\n",
      "Epoch: 1431, Loss: 0.8620, Train: 0.8620, Val: 0.9553, Test: 0.9415\n",
      "Epoch: 1432, Loss: 0.8620, Train: 0.8620, Val: 0.9571, Test: 0.9433\n",
      "Epoch: 1433, Loss: 0.8620, Train: 0.8619, Val: 0.9556, Test: 0.9417\n",
      "Epoch: 1434, Loss: 0.8619, Train: 0.8618, Val: 0.9566, Test: 0.9428\n",
      "Epoch: 1435, Loss: 0.8618, Train: 0.8617, Val: 0.9561, Test: 0.9423\n",
      "Epoch: 1436, Loss: 0.8618, Train: 0.8617, Val: 0.9562, Test: 0.9424\n",
      "Epoch: 1437, Loss: 0.8617, Train: 0.8617, Val: 0.9565, Test: 0.9427\n",
      "Epoch: 1438, Loss: 0.8617, Train: 0.8617, Val: 0.9559, Test: 0.9421\n",
      "Epoch: 1439, Loss: 0.8617, Train: 0.8616, Val: 0.9568, Test: 0.9430\n",
      "Epoch: 1440, Loss: 0.8616, Train: 0.8616, Val: 0.9558, Test: 0.9420\n",
      "Epoch: 1441, Loss: 0.8616, Train: 0.8616, Val: 0.9570, Test: 0.9431\n",
      "Epoch: 1442, Loss: 0.8616, Train: 0.8616, Val: 0.9558, Test: 0.9420\n",
      "Epoch: 1443, Loss: 0.8616, Train: 0.8615, Val: 0.9569, Test: 0.9430\n",
      "Epoch: 1444, Loss: 0.8615, Train: 0.8615, Val: 0.9561, Test: 0.9422\n",
      "Epoch: 1445, Loss: 0.8615, Train: 0.8614, Val: 0.9567, Test: 0.9429\n",
      "Epoch: 1446, Loss: 0.8614, Train: 0.8614, Val: 0.9564, Test: 0.9426\n",
      "Epoch: 1447, Loss: 0.8614, Train: 0.8613, Val: 0.9565, Test: 0.9426\n",
      "Epoch: 1448, Loss: 0.8614, Train: 0.8613, Val: 0.9568, Test: 0.9429\n",
      "Epoch: 1449, Loss: 0.8613, Train: 0.8613, Val: 0.9563, Test: 0.9425\n",
      "Epoch: 1450, Loss: 0.8613, Train: 0.8613, Val: 0.9571, Test: 0.9432\n",
      "Epoch: 1451, Loss: 0.8613, Train: 0.8612, Val: 0.9562, Test: 0.9423\n",
      "Epoch: 1452, Loss: 0.8613, Train: 0.8612, Val: 0.9573, Test: 0.9434\n",
      "Epoch: 1453, Loss: 0.8612, Train: 0.8612, Val: 0.9561, Test: 0.9422\n",
      "Epoch: 1454, Loss: 0.8612, Train: 0.8612, Val: 0.9575, Test: 0.9436\n",
      "Epoch: 1455, Loss: 0.8612, Train: 0.8612, Val: 0.9561, Test: 0.9422\n",
      "Epoch: 1456, Loss: 0.8612, Train: 0.8611, Val: 0.9574, Test: 0.9435\n",
      "Epoch: 1457, Loss: 0.8611, Train: 0.8610, Val: 0.9564, Test: 0.9425\n",
      "Epoch: 1458, Loss: 0.8610, Train: 0.8610, Val: 0.9570, Test: 0.9432\n",
      "Epoch: 1459, Loss: 0.8610, Train: 0.8609, Val: 0.9570, Test: 0.9431\n",
      "Epoch: 1460, Loss: 0.8610, Train: 0.8609, Val: 0.9566, Test: 0.9427\n",
      "Epoch: 1461, Loss: 0.8609, Train: 0.8609, Val: 0.9574, Test: 0.9435\n",
      "Epoch: 1462, Loss: 0.8609, Train: 0.8609, Val: 0.9566, Test: 0.9427\n",
      "Epoch: 1463, Loss: 0.8609, Train: 0.8608, Val: 0.9574, Test: 0.9435\n",
      "Epoch: 1464, Loss: 0.8608, Train: 0.8608, Val: 0.9569, Test: 0.9430\n",
      "Epoch: 1465, Loss: 0.8608, Train: 0.8607, Val: 0.9570, Test: 0.9431\n",
      "Epoch: 1466, Loss: 0.8608, Train: 0.8607, Val: 0.9573, Test: 0.9434\n",
      "Epoch: 1467, Loss: 0.8607, Train: 0.8607, Val: 0.9568, Test: 0.9428\n",
      "Epoch: 1468, Loss: 0.8607, Train: 0.8607, Val: 0.9577, Test: 0.9437\n",
      "Epoch: 1469, Loss: 0.8607, Train: 0.8607, Val: 0.9566, Test: 0.9427\n",
      "Epoch: 1470, Loss: 0.8607, Train: 0.8607, Val: 0.9580, Test: 0.9440\n",
      "Epoch: 1471, Loss: 0.8607, Train: 0.8606, Val: 0.9565, Test: 0.9426\n",
      "Epoch: 1472, Loss: 0.8607, Train: 0.8606, Val: 0.9580, Test: 0.9441\n",
      "Epoch: 1473, Loss: 0.8606, Train: 0.8605, Val: 0.9567, Test: 0.9428\n",
      "Epoch: 1474, Loss: 0.8605, Train: 0.8605, Val: 0.9577, Test: 0.9438\n",
      "Epoch: 1475, Loss: 0.8605, Train: 0.8604, Val: 0.9572, Test: 0.9433\n",
      "Epoch: 1476, Loss: 0.8604, Train: 0.8604, Val: 0.9572, Test: 0.9433\n",
      "Epoch: 1477, Loss: 0.8604, Train: 0.8604, Val: 0.9578, Test: 0.9438\n",
      "Epoch: 1478, Loss: 0.8604, Train: 0.8604, Val: 0.9569, Test: 0.9430\n",
      "Epoch: 1479, Loss: 0.8604, Train: 0.8603, Val: 0.9579, Test: 0.9440\n",
      "Epoch: 1480, Loss: 0.8603, Train: 0.8603, Val: 0.9570, Test: 0.9431\n",
      "Epoch: 1481, Loss: 0.8603, Train: 0.8602, Val: 0.9578, Test: 0.9438\n",
      "Epoch: 1482, Loss: 0.8602, Train: 0.8602, Val: 0.9573, Test: 0.9434\n",
      "Epoch: 1483, Loss: 0.8602, Train: 0.8602, Val: 0.9574, Test: 0.9435\n",
      "Epoch: 1484, Loss: 0.8602, Train: 0.8601, Val: 0.9577, Test: 0.9438\n",
      "Epoch: 1485, Loss: 0.8601, Train: 0.8601, Val: 0.9572, Test: 0.9433\n",
      "Epoch: 1486, Loss: 0.8601, Train: 0.8601, Val: 0.9580, Test: 0.9440\n",
      "Epoch: 1487, Loss: 0.8601, Train: 0.8601, Val: 0.9571, Test: 0.9431\n",
      "Epoch: 1488, Loss: 0.8601, Train: 0.8601, Val: 0.9584, Test: 0.9444\n",
      "Epoch: 1489, Loss: 0.8601, Train: 0.8601, Val: 0.9568, Test: 0.9429\n",
      "Epoch: 1490, Loss: 0.8601, Train: 0.8600, Val: 0.9586, Test: 0.9446\n",
      "Epoch: 1491, Loss: 0.8600, Train: 0.8600, Val: 0.9570, Test: 0.9430\n",
      "Epoch: 1492, Loss: 0.8600, Train: 0.8599, Val: 0.9582, Test: 0.9442\n",
      "Epoch: 1493, Loss: 0.8599, Train: 0.8598, Val: 0.9575, Test: 0.9435\n",
      "Epoch: 1494, Loss: 0.8598, Train: 0.8598, Val: 0.9577, Test: 0.9437\n",
      "Epoch: 1495, Loss: 0.8598, Train: 0.8598, Val: 0.9580, Test: 0.9441\n",
      "Epoch: 1496, Loss: 0.8598, Train: 0.8598, Val: 0.9574, Test: 0.9435\n",
      "Epoch: 1497, Loss: 0.8598, Train: 0.8597, Val: 0.9584, Test: 0.9444\n",
      "Epoch: 1498, Loss: 0.8597, Train: 0.8597, Val: 0.9573, Test: 0.9434\n",
      "Epoch: 1499, Loss: 0.8597, Train: 0.8597, Val: 0.9585, Test: 0.9445\n",
      "Epoch: 1500, Loss: 0.8597, Train: 0.8597, Val: 0.9573, Test: 0.9434\n",
      "Epoch: 1501, Loss: 0.8597, Train: 0.8596, Val: 0.9585, Test: 0.9445\n",
      "Epoch: 1502, Loss: 0.8596, Train: 0.8596, Val: 0.9575, Test: 0.9435\n",
      "Epoch: 1503, Loss: 0.8596, Train: 0.8595, Val: 0.9583, Test: 0.9444\n",
      "Epoch: 1504, Loss: 0.8595, Train: 0.8595, Val: 0.9577, Test: 0.9438\n",
      "Epoch: 1505, Loss: 0.8595, Train: 0.8594, Val: 0.9580, Test: 0.9440\n",
      "Epoch: 1506, Loss: 0.8594, Train: 0.8594, Val: 0.9582, Test: 0.9442\n",
      "Epoch: 1507, Loss: 0.8594, Train: 0.8594, Val: 0.9577, Test: 0.9438\n",
      "Epoch: 1508, Loss: 0.8594, Train: 0.8594, Val: 0.9585, Test: 0.9446\n",
      "Epoch: 1509, Loss: 0.8594, Train: 0.8594, Val: 0.9575, Test: 0.9436\n",
      "Epoch: 1510, Loss: 0.8594, Train: 0.8594, Val: 0.9589, Test: 0.9449\n",
      "Epoch: 1511, Loss: 0.8594, Train: 0.8594, Val: 0.9573, Test: 0.9434\n",
      "Epoch: 1512, Loss: 0.8594, Train: 0.8593, Val: 0.9590, Test: 0.9450\n",
      "Epoch: 1513, Loss: 0.8593, Train: 0.8592, Val: 0.9575, Test: 0.9435\n",
      "Epoch: 1514, Loss: 0.8592, Train: 0.8592, Val: 0.9587, Test: 0.9446\n",
      "Epoch: 1515, Loss: 0.8592, Train: 0.8591, Val: 0.9581, Test: 0.9441\n",
      "Epoch: 1516, Loss: 0.8591, Train: 0.8591, Val: 0.9582, Test: 0.9441\n",
      "Epoch: 1517, Loss: 0.8591, Train: 0.8591, Val: 0.9587, Test: 0.9446\n",
      "Epoch: 1518, Loss: 0.8591, Train: 0.8590, Val: 0.9578, Test: 0.9438\n",
      "Epoch: 1519, Loss: 0.8591, Train: 0.8590, Val: 0.9590, Test: 0.9450\n",
      "Epoch: 1520, Loss: 0.8590, Train: 0.8590, Val: 0.9578, Test: 0.9438\n",
      "Epoch: 1521, Loss: 0.8590, Train: 0.8589, Val: 0.9590, Test: 0.9449\n",
      "Epoch: 1522, Loss: 0.8590, Train: 0.8589, Val: 0.9580, Test: 0.9440\n",
      "Epoch: 1523, Loss: 0.8589, Train: 0.8589, Val: 0.9588, Test: 0.9447\n",
      "Epoch: 1524, Loss: 0.8589, Train: 0.8588, Val: 0.9583, Test: 0.9443\n",
      "Epoch: 1525, Loss: 0.8588, Train: 0.8588, Val: 0.9585, Test: 0.9444\n",
      "Epoch: 1526, Loss: 0.8588, Train: 0.8587, Val: 0.9587, Test: 0.9446\n",
      "Epoch: 1527, Loss: 0.8588, Train: 0.8587, Val: 0.9583, Test: 0.9443\n",
      "Epoch: 1528, Loss: 0.8587, Train: 0.8587, Val: 0.9590, Test: 0.9449\n",
      "Epoch: 1529, Loss: 0.8587, Train: 0.8587, Val: 0.9580, Test: 0.9439\n",
      "Epoch: 1530, Loss: 0.8587, Train: 0.8587, Val: 0.9595, Test: 0.9454\n",
      "Epoch: 1531, Loss: 0.8587, Train: 0.8587, Val: 0.9577, Test: 0.9437\n",
      "Epoch: 1532, Loss: 0.8587, Train: 0.8586, Val: 0.9596, Test: 0.9454\n",
      "Epoch: 1533, Loss: 0.8587, Train: 0.8586, Val: 0.9580, Test: 0.9439\n",
      "Epoch: 1534, Loss: 0.8586, Train: 0.8585, Val: 0.9591, Test: 0.9450\n",
      "Epoch: 1535, Loss: 0.8585, Train: 0.8584, Val: 0.9587, Test: 0.9445\n",
      "Epoch: 1536, Loss: 0.8585, Train: 0.8584, Val: 0.9586, Test: 0.9444\n",
      "Epoch: 1537, Loss: 0.8584, Train: 0.8584, Val: 0.9593, Test: 0.9452\n",
      "Epoch: 1538, Loss: 0.8584, Train: 0.8584, Val: 0.9581, Test: 0.9440\n",
      "Epoch: 1539, Loss: 0.8584, Train: 0.8584, Val: 0.9597, Test: 0.9455\n",
      "Epoch: 1540, Loss: 0.8584, Train: 0.8584, Val: 0.9582, Test: 0.9441\n",
      "Epoch: 1541, Loss: 0.8584, Train: 0.8583, Val: 0.9595, Test: 0.9453\n",
      "Epoch: 1542, Loss: 0.8583, Train: 0.8582, Val: 0.9587, Test: 0.9445\n",
      "Epoch: 1543, Loss: 0.8582, Train: 0.8582, Val: 0.9590, Test: 0.9448\n",
      "Epoch: 1544, Loss: 0.8582, Train: 0.8582, Val: 0.9591, Test: 0.9450\n",
      "Epoch: 1545, Loss: 0.8582, Train: 0.8581, Val: 0.9587, Test: 0.9445\n",
      "Epoch: 1546, Loss: 0.8581, Train: 0.8581, Val: 0.9595, Test: 0.9454\n",
      "Epoch: 1547, Loss: 0.8581, Train: 0.8581, Val: 0.9585, Test: 0.9443\n",
      "Epoch: 1548, Loss: 0.8581, Train: 0.8581, Val: 0.9598, Test: 0.9456\n",
      "Epoch: 1549, Loss: 0.8581, Train: 0.8581, Val: 0.9584, Test: 0.9443\n",
      "Epoch: 1550, Loss: 0.8581, Train: 0.8580, Val: 0.9599, Test: 0.9457\n",
      "Epoch: 1551, Loss: 0.8580, Train: 0.8580, Val: 0.9584, Test: 0.9443\n",
      "Epoch: 1552, Loss: 0.8580, Train: 0.8579, Val: 0.9598, Test: 0.9457\n",
      "Epoch: 1553, Loss: 0.8580, Train: 0.8579, Val: 0.9586, Test: 0.9445\n",
      "Epoch: 1554, Loss: 0.8579, Train: 0.8578, Val: 0.9596, Test: 0.9454\n",
      "Epoch: 1555, Loss: 0.8578, Train: 0.8578, Val: 0.9591, Test: 0.9449\n",
      "Epoch: 1556, Loss: 0.8578, Train: 0.8578, Val: 0.9592, Test: 0.9450\n",
      "Epoch: 1557, Loss: 0.8578, Train: 0.8577, Val: 0.9596, Test: 0.9454\n",
      "Epoch: 1558, Loss: 0.8577, Train: 0.8577, Val: 0.9589, Test: 0.9448\n",
      "Epoch: 1559, Loss: 0.8577, Train: 0.8577, Val: 0.9599, Test: 0.9457\n",
      "Epoch: 1560, Loss: 0.8577, Train: 0.8577, Val: 0.9588, Test: 0.9447\n",
      "Epoch: 1561, Loss: 0.8577, Train: 0.8576, Val: 0.9601, Test: 0.9459\n",
      "Epoch: 1562, Loss: 0.8577, Train: 0.8576, Val: 0.9587, Test: 0.9446\n",
      "Epoch: 1563, Loss: 0.8576, Train: 0.8576, Val: 0.9602, Test: 0.9461\n",
      "Epoch: 1564, Loss: 0.8576, Train: 0.8576, Val: 0.9587, Test: 0.9445\n",
      "Epoch: 1565, Loss: 0.8576, Train: 0.8575, Val: 0.9602, Test: 0.9460\n",
      "Epoch: 1566, Loss: 0.8575, Train: 0.8575, Val: 0.9590, Test: 0.9448\n",
      "Epoch: 1567, Loss: 0.8575, Train: 0.8574, Val: 0.9598, Test: 0.9456\n",
      "Epoch: 1568, Loss: 0.8574, Train: 0.8574, Val: 0.9596, Test: 0.9455\n",
      "Epoch: 1569, Loss: 0.8574, Train: 0.8573, Val: 0.9593, Test: 0.9451\n",
      "Epoch: 1570, Loss: 0.8573, Train: 0.8573, Val: 0.9601, Test: 0.9460\n",
      "Epoch: 1571, Loss: 0.8573, Train: 0.8573, Val: 0.9591, Test: 0.9449\n",
      "Epoch: 1572, Loss: 0.8573, Train: 0.8573, Val: 0.9603, Test: 0.9462\n",
      "Epoch: 1573, Loss: 0.8573, Train: 0.8572, Val: 0.9591, Test: 0.9449\n",
      "Epoch: 1574, Loss: 0.8573, Train: 0.8572, Val: 0.9604, Test: 0.9462\n",
      "Epoch: 1575, Loss: 0.8572, Train: 0.8572, Val: 0.9591, Test: 0.9450\n",
      "Epoch: 1576, Loss: 0.8572, Train: 0.8571, Val: 0.9603, Test: 0.9461\n",
      "Epoch: 1577, Loss: 0.8571, Train: 0.8571, Val: 0.9594, Test: 0.9452\n",
      "Epoch: 1578, Loss: 0.8571, Train: 0.8570, Val: 0.9600, Test: 0.9458\n",
      "Epoch: 1579, Loss: 0.8570, Train: 0.8570, Val: 0.9598, Test: 0.9456\n",
      "Epoch: 1580, Loss: 0.8570, Train: 0.8570, Val: 0.9596, Test: 0.9454\n",
      "Epoch: 1581, Loss: 0.8570, Train: 0.8569, Val: 0.9603, Test: 0.9460\n",
      "Epoch: 1582, Loss: 0.8570, Train: 0.8569, Val: 0.9595, Test: 0.9453\n",
      "Epoch: 1583, Loss: 0.8569, Train: 0.8569, Val: 0.9605, Test: 0.9463\n",
      "Epoch: 1584, Loss: 0.8569, Train: 0.8569, Val: 0.9592, Test: 0.9450\n",
      "Epoch: 1585, Loss: 0.8569, Train: 0.8569, Val: 0.9609, Test: 0.9467\n",
      "Epoch: 1586, Loss: 0.8569, Train: 0.8569, Val: 0.9590, Test: 0.9448\n",
      "Epoch: 1587, Loss: 0.8569, Train: 0.8568, Val: 0.9609, Test: 0.9466\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "model = Model(layer_name=\"SAGE\", hidden_channels=16, data=data, encoder_num_layers=2,\n",
    "              decoder_num_layers=10, encoder_dropout=0.0, decoder_dropout=0.0, encoder_skip_connections=1)\n",
    "train_test(model, train_data=train_data, test_data=test_data,\n",
    "           val_data=val_data, logging_step=1, epochs=3000, use_weighted_loss=False, lr=0.016)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with rounding with 0.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGE\n",
      "Aggregation: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ioannisathanasiou/diploma/environ/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 3.5613, Train: 3.5825, Val: 3.5677, Test: 3.5810\n",
      "Epoch: 002, Loss: 3.5167, Train: 3.5825, Val: 3.5677, Test: 3.5810\n",
      "Epoch: 003, Loss: 3.4767, Train: 3.5825, Val: 3.5677, Test: 3.5810\n",
      "Epoch: 004, Loss: 3.4349, Train: 3.5825, Val: 3.5677, Test: 3.5810\n",
      "Epoch: 005, Loss: 3.3892, Train: 3.1110, Val: 3.0969, Test: 3.1092\n",
      "Epoch: 006, Loss: 3.3338, Train: 3.1110, Val: 3.0969, Test: 3.1092\n",
      "Epoch: 007, Loss: 3.2667, Train: 3.1110, Val: 3.0969, Test: 3.1092\n",
      "Epoch: 008, Loss: 3.1658, Train: 3.1110, Val: 3.0969, Test: 3.1092\n",
      "Epoch: 009, Loss: 3.0013, Train: 2.6501, Val: 2.6400, Test: 2.6504\n",
      "Epoch: 010, Loss: 2.7217, Train: 2.2065, Val: 2.2098, Test: 2.2154\n",
      "Epoch: 011, Loss: 2.2428, Train: 1.4531, Val: 1.4596, Test: 1.4560\n",
      "Epoch: 012, Loss: 1.4909, Train: 1.3061, Val: 1.3108, Test: 1.2858\n",
      "Epoch: 013, Loss: 1.3352, Train: 1.5499, Val: 1.5476, Test: 1.5241\n",
      "Epoch: 014, Loss: 1.5730, Train: 1.2432, Val: 1.2534, Test: 1.2287\n",
      "Epoch: 015, Loss: 1.2044, Train: 1.1804, Val: 1.1919, Test: 1.1796\n",
      "Epoch: 016, Loss: 1.1517, Train: 1.3322, Val: 1.3417, Test: 1.3354\n",
      "Epoch: 017, Loss: 1.3085, Train: 1.4339, Val: 1.4391, Test: 1.4361\n",
      "Epoch: 018, Loss: 1.3712, Train: 1.3774, Val: 1.3843, Test: 1.3805\n",
      "Epoch: 019, Loss: 1.3219, Train: 1.1962, Val: 1.2074, Test: 1.1955\n",
      "Epoch: 020, Loss: 1.2032, Train: 1.1265, Val: 1.1369, Test: 1.1193\n",
      "Epoch: 021, Loss: 1.1132, Train: 1.2006, Val: 1.2182, Test: 1.1925\n",
      "Epoch: 022, Loss: 1.1696, Train: 1.2558, Val: 1.2601, Test: 1.2369\n",
      "Epoch: 023, Loss: 1.2534, Train: 1.2467, Val: 1.2536, Test: 1.2300\n",
      "Epoch: 024, Loss: 1.2086, Train: 1.1156, Val: 1.1267, Test: 1.1048\n",
      "Epoch: 025, Loss: 1.1235, Train: 1.1201, Val: 1.1333, Test: 1.1188\n",
      "Epoch: 026, Loss: 1.1148, Train: 1.1849, Val: 1.1943, Test: 1.1828\n",
      "Epoch: 027, Loss: 1.1567, Train: 1.1891, Val: 1.2003, Test: 1.1871\n",
      "Epoch: 028, Loss: 1.1839, Train: 1.1872, Val: 1.1983, Test: 1.1855\n",
      "Epoch: 029, Loss: 1.1739, Train: 1.1714, Val: 1.1802, Test: 1.1709\n",
      "Epoch: 030, Loss: 1.1382, Train: 1.1169, Val: 1.1298, Test: 1.1118\n",
      "Epoch: 031, Loss: 1.1074, Train: 1.1169, Val: 1.1279, Test: 1.1062\n",
      "Epoch: 032, Loss: 1.1102, Train: 1.1200, Val: 1.1325, Test: 1.1098\n",
      "Epoch: 033, Loss: 1.1362, Train: 1.1391, Val: 1.1528, Test: 1.1314\n",
      "Epoch: 034, Loss: 1.1447, Train: 1.1104, Val: 1.1214, Test: 1.1011\n",
      "Epoch: 035, Loss: 1.1249, Train: 1.1174, Val: 1.1290, Test: 1.1075\n",
      "Epoch: 036, Loss: 1.1038, Train: 1.1133, Val: 1.1260, Test: 1.1072\n",
      "Epoch: 037, Loss: 1.1024, Train: 1.1108, Val: 1.1224, Test: 1.1113\n",
      "Epoch: 038, Loss: 1.1140, Train: 1.1347, Val: 1.1476, Test: 1.1340\n",
      "Epoch: 039, Loss: 1.1220, Train: 1.1244, Val: 1.1355, Test: 1.1244\n",
      "Epoch: 040, Loss: 1.1187, Train: 1.1011, Val: 1.1169, Test: 1.1021\n",
      "Epoch: 041, Loss: 1.1074, Train: 1.1124, Val: 1.1246, Test: 1.1061\n",
      "Epoch: 042, Loss: 1.0981, Train: 1.1165, Val: 1.1276, Test: 1.1069\n",
      "Epoch: 043, Loss: 1.0981, Train: 1.1134, Val: 1.1255, Test: 1.1026\n",
      "Epoch: 044, Loss: 1.1047, Train: 1.1109, Val: 1.1215, Test: 1.1004\n",
      "Epoch: 045, Loss: 1.1081, Train: 1.1129, Val: 1.1245, Test: 1.1024\n",
      "Epoch: 046, Loss: 1.1038, Train: 1.1156, Val: 1.1278, Test: 1.1063\n",
      "Epoch: 047, Loss: 1.0968, Train: 1.1130, Val: 1.1243, Test: 1.1055\n",
      "Epoch: 048, Loss: 1.0937, Train: 1.1019, Val: 1.1157, Test: 1.0970\n",
      "Epoch: 049, Loss: 1.0956, Train: 1.0938, Val: 1.1085, Test: 1.0932\n",
      "Epoch: 050, Loss: 1.0985, Train: 1.0928, Val: 1.1075, Test: 1.0922\n",
      "Epoch: 051, Loss: 1.0987, Train: 1.0951, Val: 1.1082, Test: 1.0933\n",
      "Epoch: 052, Loss: 1.0958, Train: 1.1034, Val: 1.1168, Test: 1.0991\n",
      "Epoch: 053, Loss: 1.0922, Train: 1.1109, Val: 1.1229, Test: 1.1044\n",
      "Epoch: 054, Loss: 1.0905, Train: 1.1138, Val: 1.1259, Test: 1.1052\n",
      "Epoch: 055, Loss: 1.0913, Train: 1.1133, Val: 1.1257, Test: 1.1033\n",
      "Epoch: 056, Loss: 1.0927, Train: 1.1131, Val: 1.1253, Test: 1.1032\n",
      "Epoch: 057, Loss: 1.0924, Train: 1.1129, Val: 1.1255, Test: 1.1039\n",
      "Epoch: 058, Loss: 1.0904, Train: 1.1107, Val: 1.1236, Test: 1.1032\n",
      "Epoch: 059, Loss: 1.0883, Train: 1.1056, Val: 1.1196, Test: 1.1005\n",
      "Epoch: 060, Loss: 1.0875, Train: 1.0987, Val: 1.1132, Test: 1.0935\n",
      "Epoch: 061, Loss: 1.0879, Train: 1.0943, Val: 1.1084, Test: 1.0904\n",
      "Epoch: 062, Loss: 1.0883, Train: 1.0938, Val: 1.1078, Test: 1.0896\n",
      "Epoch: 063, Loss: 1.0878, Train: 1.0966, Val: 1.1124, Test: 1.0922\n",
      "Epoch: 064, Loss: 1.0865, Train: 1.1010, Val: 1.1153, Test: 1.0971\n",
      "Epoch: 065, Loss: 1.0852, Train: 1.1057, Val: 1.1203, Test: 1.1003\n",
      "Epoch: 066, Loss: 1.0846, Train: 1.1075, Val: 1.1220, Test: 1.1013\n",
      "Epoch: 067, Loss: 1.0846, Train: 1.1080, Val: 1.1217, Test: 1.1008\n",
      "Epoch: 068, Loss: 1.0846, Train: 1.1075, Val: 1.1214, Test: 1.1004\n",
      "Epoch: 069, Loss: 1.0841, Train: 1.1061, Val: 1.1205, Test: 1.0999\n",
      "Epoch: 070, Loss: 1.0832, Train: 1.1033, Val: 1.1185, Test: 1.0985\n",
      "Epoch: 071, Loss: 1.0823, Train: 1.0995, Val: 1.1145, Test: 1.0955\n",
      "Epoch: 072, Loss: 1.0818, Train: 1.0953, Val: 1.1116, Test: 1.0913\n",
      "Epoch: 073, Loss: 1.0816, Train: 1.0934, Val: 1.1078, Test: 1.0901\n",
      "Epoch: 074, Loss: 1.0814, Train: 1.0929, Val: 1.1071, Test: 1.0896\n",
      "Epoch: 075, Loss: 1.0809, Train: 1.0936, Val: 1.1100, Test: 1.0905\n",
      "Epoch: 076, Loss: 1.0803, Train: 1.0957, Val: 1.1118, Test: 1.0911\n",
      "Epoch: 077, Loss: 1.0796, Train: 1.0976, Val: 1.1129, Test: 1.0942\n",
      "Epoch: 078, Loss: 1.0791, Train: 1.0989, Val: 1.1146, Test: 1.0943\n",
      "Epoch: 079, Loss: 1.0788, Train: 1.0994, Val: 1.1155, Test: 1.0947\n",
      "Epoch: 080, Loss: 1.0785, Train: 1.0988, Val: 1.1148, Test: 1.0943\n",
      "Epoch: 081, Loss: 1.0781, Train: 1.0977, Val: 1.1138, Test: 1.0927\n",
      "Epoch: 082, Loss: 1.0775, Train: 1.0951, Val: 1.1122, Test: 1.0918\n",
      "Epoch: 083, Loss: 1.0770, Train: 1.0933, Val: 1.1086, Test: 1.0898\n",
      "Epoch: 084, Loss: 1.0765, Train: 1.0910, Val: 1.1076, Test: 1.0880\n",
      "Epoch: 085, Loss: 1.0761, Train: 1.0897, Val: 1.1052, Test: 1.0862\n",
      "Epoch: 086, Loss: 1.0758, Train: 1.0890, Val: 1.1047, Test: 1.0856\n",
      "Epoch: 087, Loss: 1.0754, Train: 1.0888, Val: 1.1045, Test: 1.0860\n",
      "Epoch: 088, Loss: 1.0749, Train: 1.0895, Val: 1.1055, Test: 1.0866\n",
      "Epoch: 089, Loss: 1.0744, Train: 1.0900, Val: 1.1069, Test: 1.0872\n",
      "Epoch: 090, Loss: 1.0739, Train: 1.0903, Val: 1.1073, Test: 1.0876\n",
      "Epoch: 091, Loss: 1.0735, Train: 1.0904, Val: 1.1076, Test: 1.0874\n",
      "Epoch: 092, Loss: 1.0732, Train: 1.0900, Val: 1.1073, Test: 1.0872\n",
      "Epoch: 093, Loss: 1.0728, Train: 1.0891, Val: 1.1067, Test: 1.0863\n",
      "Epoch: 094, Loss: 1.0723, Train: 1.0881, Val: 1.1054, Test: 1.0854\n",
      "Epoch: 095, Loss: 1.0719, Train: 1.0866, Val: 1.1037, Test: 1.0845\n",
      "Epoch: 096, Loss: 1.0714, Train: 1.0854, Val: 1.1018, Test: 1.0831\n",
      "Epoch: 097, Loss: 1.0710, Train: 1.0845, Val: 1.1012, Test: 1.0814\n",
      "Epoch: 098, Loss: 1.0706, Train: 1.0838, Val: 1.1006, Test: 1.0803\n",
      "Epoch: 099, Loss: 1.0702, Train: 1.0834, Val: 1.1002, Test: 1.0799\n",
      "Epoch: 100, Loss: 1.0698, Train: 1.0832, Val: 1.0997, Test: 1.0798\n",
      "Epoch: 101, Loss: 1.0694, Train: 1.0829, Val: 1.0999, Test: 1.0796\n",
      "Epoch: 102, Loss: 1.0690, Train: 1.0827, Val: 1.0999, Test: 1.0801\n",
      "Epoch: 103, Loss: 1.0685, Train: 1.0825, Val: 1.0993, Test: 1.0801\n",
      "Epoch: 104, Loss: 1.0681, Train: 1.0821, Val: 1.0991, Test: 1.0797\n",
      "Epoch: 105, Loss: 1.0677, Train: 1.0815, Val: 1.0987, Test: 1.0785\n",
      "Epoch: 106, Loss: 1.0672, Train: 1.0805, Val: 1.0982, Test: 1.0774\n",
      "Epoch: 107, Loss: 1.0668, Train: 1.0799, Val: 1.0971, Test: 1.0755\n",
      "Epoch: 108, Loss: 1.0663, Train: 1.0791, Val: 1.0966, Test: 1.0753\n",
      "Epoch: 109, Loss: 1.0658, Train: 1.0785, Val: 1.0957, Test: 1.0748\n",
      "Epoch: 110, Loss: 1.0654, Train: 1.0778, Val: 1.0949, Test: 1.0744\n",
      "Epoch: 111, Loss: 1.0650, Train: 1.0771, Val: 1.0942, Test: 1.0742\n",
      "Epoch: 112, Loss: 1.0646, Train: 1.0765, Val: 1.0935, Test: 1.0739\n",
      "Epoch: 113, Loss: 1.0642, Train: 1.0762, Val: 1.0935, Test: 1.0738\n",
      "Epoch: 114, Loss: 1.0638, Train: 1.0758, Val: 1.0934, Test: 1.0734\n",
      "Epoch: 115, Loss: 1.0634, Train: 1.0756, Val: 1.0930, Test: 1.0730\n",
      "Epoch: 116, Loss: 1.0630, Train: 1.0750, Val: 1.0923, Test: 1.0726\n",
      "Epoch: 117, Loss: 1.0626, Train: 1.0745, Val: 1.0917, Test: 1.0720\n",
      "Epoch: 118, Loss: 1.0622, Train: 1.0737, Val: 1.0908, Test: 1.0715\n",
      "Epoch: 119, Loss: 1.0618, Train: 1.0731, Val: 1.0901, Test: 1.0716\n",
      "Epoch: 120, Loss: 1.0614, Train: 1.0724, Val: 1.0897, Test: 1.0705\n",
      "Epoch: 121, Loss: 1.0610, Train: 1.0719, Val: 1.0893, Test: 1.0704\n",
      "Epoch: 122, Loss: 1.0606, Train: 1.0712, Val: 1.0883, Test: 1.0701\n",
      "Epoch: 123, Loss: 1.0603, Train: 1.0708, Val: 1.0881, Test: 1.0697\n",
      "Epoch: 124, Loss: 1.0599, Train: 1.0702, Val: 1.0880, Test: 1.0691\n",
      "Epoch: 125, Loss: 1.0595, Train: 1.0698, Val: 1.0876, Test: 1.0689\n",
      "Epoch: 126, Loss: 1.0591, Train: 1.0692, Val: 1.0873, Test: 1.0683\n",
      "Epoch: 127, Loss: 1.0587, Train: 1.0687, Val: 1.0866, Test: 1.0675\n",
      "Epoch: 128, Loss: 1.0583, Train: 1.0682, Val: 1.0863, Test: 1.0668\n",
      "Epoch: 129, Loss: 1.0579, Train: 1.0677, Val: 1.0861, Test: 1.0663\n",
      "Epoch: 130, Loss: 1.0576, Train: 1.0673, Val: 1.0850, Test: 1.0659\n",
      "Epoch: 131, Loss: 1.0572, Train: 1.0667, Val: 1.0848, Test: 1.0658\n",
      "Epoch: 132, Loss: 1.0568, Train: 1.0663, Val: 1.0846, Test: 1.0651\n",
      "Epoch: 133, Loss: 1.0564, Train: 1.0659, Val: 1.0847, Test: 1.0651\n",
      "Epoch: 134, Loss: 1.0560, Train: 1.0656, Val: 1.0837, Test: 1.0648\n",
      "Epoch: 135, Loss: 1.0556, Train: 1.0652, Val: 1.0832, Test: 1.0640\n",
      "Epoch: 136, Loss: 1.0553, Train: 1.0646, Val: 1.0826, Test: 1.0632\n",
      "Epoch: 137, Loss: 1.0549, Train: 1.0642, Val: 1.0816, Test: 1.0630\n",
      "Epoch: 138, Loss: 1.0545, Train: 1.0637, Val: 1.0815, Test: 1.0627\n",
      "Epoch: 139, Loss: 1.0541, Train: 1.0633, Val: 1.0810, Test: 1.0622\n",
      "Epoch: 140, Loss: 1.0538, Train: 1.0628, Val: 1.0803, Test: 1.0620\n",
      "Epoch: 141, Loss: 1.0534, Train: 1.0624, Val: 1.0799, Test: 1.0614\n",
      "Epoch: 142, Loss: 1.0530, Train: 1.0621, Val: 1.0794, Test: 1.0611\n",
      "Epoch: 143, Loss: 1.0526, Train: 1.0618, Val: 1.0794, Test: 1.0602\n",
      "Epoch: 144, Loss: 1.0523, Train: 1.0615, Val: 1.0791, Test: 1.0595\n",
      "Epoch: 145, Loss: 1.0519, Train: 1.0612, Val: 1.0785, Test: 1.0593\n",
      "Epoch: 146, Loss: 1.0515, Train: 1.0608, Val: 1.0778, Test: 1.0588\n",
      "Epoch: 147, Loss: 1.0512, Train: 1.0604, Val: 1.0775, Test: 1.0589\n",
      "Epoch: 148, Loss: 1.0508, Train: 1.0599, Val: 1.0773, Test: 1.0586\n",
      "Epoch: 149, Loss: 1.0504, Train: 1.0594, Val: 1.0773, Test: 1.0580\n",
      "Epoch: 150, Loss: 1.0501, Train: 1.0589, Val: 1.0766, Test: 1.0573\n",
      "Epoch: 151, Loss: 1.0497, Train: 1.0585, Val: 1.0763, Test: 1.0570\n",
      "Epoch: 152, Loss: 1.0493, Train: 1.0581, Val: 1.0759, Test: 1.0567\n",
      "Epoch: 153, Loss: 1.0490, Train: 1.0576, Val: 1.0760, Test: 1.0564\n",
      "Epoch: 154, Loss: 1.0486, Train: 1.0573, Val: 1.0756, Test: 1.0563\n",
      "Epoch: 155, Loss: 1.0482, Train: 1.0568, Val: 1.0755, Test: 1.0560\n",
      "Epoch: 156, Loss: 1.0479, Train: 1.0565, Val: 1.0753, Test: 1.0556\n",
      "Epoch: 157, Loss: 1.0475, Train: 1.0564, Val: 1.0751, Test: 1.0554\n",
      "Epoch: 158, Loss: 1.0472, Train: 1.0559, Val: 1.0746, Test: 1.0554\n",
      "Epoch: 159, Loss: 1.0468, Train: 1.0557, Val: 1.0745, Test: 1.0558\n",
      "Epoch: 160, Loss: 1.0464, Train: 1.0552, Val: 1.0741, Test: 1.0556\n",
      "Epoch: 161, Loss: 1.0461, Train: 1.0548, Val: 1.0734, Test: 1.0554\n",
      "Epoch: 162, Loss: 1.0457, Train: 1.0546, Val: 1.0732, Test: 1.0553\n",
      "Epoch: 163, Loss: 1.0454, Train: 1.0541, Val: 1.0727, Test: 1.0549\n",
      "Epoch: 164, Loss: 1.0450, Train: 1.0538, Val: 1.0725, Test: 1.0544\n",
      "Epoch: 165, Loss: 1.0447, Train: 1.0535, Val: 1.0723, Test: 1.0544\n",
      "Epoch: 166, Loss: 1.0443, Train: 1.0531, Val: 1.0721, Test: 1.0540\n",
      "Epoch: 167, Loss: 1.0440, Train: 1.0528, Val: 1.0718, Test: 1.0535\n",
      "Epoch: 168, Loss: 1.0437, Train: 1.0525, Val: 1.0713, Test: 1.0534\n",
      "Epoch: 169, Loss: 1.0433, Train: 1.0521, Val: 1.0708, Test: 1.0532\n",
      "Epoch: 170, Loss: 1.0430, Train: 1.0517, Val: 1.0701, Test: 1.0529\n",
      "Epoch: 171, Loss: 1.0426, Train: 1.0514, Val: 1.0698, Test: 1.0520\n",
      "Epoch: 172, Loss: 1.0423, Train: 1.0512, Val: 1.0692, Test: 1.0520\n",
      "Epoch: 173, Loss: 1.0420, Train: 1.0511, Val: 1.0692, Test: 1.0517\n",
      "Epoch: 174, Loss: 1.0416, Train: 1.0509, Val: 1.0689, Test: 1.0513\n",
      "Epoch: 175, Loss: 1.0413, Train: 1.0506, Val: 1.0687, Test: 1.0513\n",
      "Epoch: 176, Loss: 1.0410, Train: 1.0504, Val: 1.0682, Test: 1.0510\n",
      "Epoch: 177, Loss: 1.0406, Train: 1.0500, Val: 1.0679, Test: 1.0505\n",
      "Epoch: 178, Loss: 1.0403, Train: 1.0497, Val: 1.0678, Test: 1.0503\n",
      "Epoch: 179, Loss: 1.0400, Train: 1.0493, Val: 1.0670, Test: 1.0501\n",
      "Epoch: 180, Loss: 1.0397, Train: 1.0491, Val: 1.0670, Test: 1.0499\n",
      "Epoch: 181, Loss: 1.0393, Train: 1.0487, Val: 1.0667, Test: 1.0496\n",
      "Epoch: 182, Loss: 1.0390, Train: 1.0484, Val: 1.0665, Test: 1.0493\n",
      "Epoch: 183, Loss: 1.0387, Train: 1.0480, Val: 1.0660, Test: 1.0493\n",
      "Epoch: 184, Loss: 1.0384, Train: 1.0476, Val: 1.0662, Test: 1.0492\n",
      "Epoch: 185, Loss: 1.0381, Train: 1.0472, Val: 1.0659, Test: 1.0490\n",
      "Epoch: 186, Loss: 1.0377, Train: 1.0469, Val: 1.0651, Test: 1.0484\n",
      "Epoch: 187, Loss: 1.0374, Train: 1.0465, Val: 1.0646, Test: 1.0482\n",
      "Epoch: 188, Loss: 1.0371, Train: 1.0463, Val: 1.0639, Test: 1.0479\n",
      "Epoch: 189, Loss: 1.0368, Train: 1.0459, Val: 1.0638, Test: 1.0474\n",
      "Epoch: 190, Loss: 1.0365, Train: 1.0457, Val: 1.0635, Test: 1.0471\n",
      "Epoch: 191, Loss: 1.0362, Train: 1.0454, Val: 1.0626, Test: 1.0466\n",
      "Epoch: 192, Loss: 1.0359, Train: 1.0452, Val: 1.0620, Test: 1.0466\n",
      "Epoch: 193, Loss: 1.0356, Train: 1.0448, Val: 1.0618, Test: 1.0464\n",
      "Epoch: 194, Loss: 1.0353, Train: 1.0446, Val: 1.0615, Test: 1.0465\n",
      "Epoch: 195, Loss: 1.0349, Train: 1.0443, Val: 1.0612, Test: 1.0463\n",
      "Epoch: 196, Loss: 1.0346, Train: 1.0440, Val: 1.0611, Test: 1.0460\n",
      "Epoch: 197, Loss: 1.0343, Train: 1.0438, Val: 1.0607, Test: 1.0460\n",
      "Epoch: 198, Loss: 1.0340, Train: 1.0436, Val: 1.0607, Test: 1.0458\n",
      "Epoch: 199, Loss: 1.0337, Train: 1.0434, Val: 1.0604, Test: 1.0449\n",
      "Epoch: 200, Loss: 1.0334, Train: 1.0431, Val: 1.0599, Test: 1.0444\n",
      "Epoch: 201, Loss: 1.0331, Train: 1.0428, Val: 1.0597, Test: 1.0441\n",
      "Epoch: 202, Loss: 1.0328, Train: 1.0425, Val: 1.0595, Test: 1.0438\n",
      "Epoch: 203, Loss: 1.0325, Train: 1.0422, Val: 1.0592, Test: 1.0437\n",
      "Epoch: 204, Loss: 1.0322, Train: 1.0419, Val: 1.0587, Test: 1.0432\n",
      "Epoch: 205, Loss: 1.0320, Train: 1.0415, Val: 1.0581, Test: 1.0433\n",
      "Epoch: 206, Loss: 1.0317, Train: 1.0411, Val: 1.0576, Test: 1.0435\n",
      "Epoch: 207, Loss: 1.0314, Train: 1.0408, Val: 1.0575, Test: 1.0434\n",
      "Epoch: 208, Loss: 1.0311, Train: 1.0407, Val: 1.0570, Test: 1.0428\n",
      "Epoch: 209, Loss: 1.0308, Train: 1.0405, Val: 1.0566, Test: 1.0422\n",
      "Epoch: 210, Loss: 1.0305, Train: 1.0403, Val: 1.0566, Test: 1.0422\n",
      "Epoch: 211, Loss: 1.0302, Train: 1.0401, Val: 1.0563, Test: 1.0414\n",
      "Epoch: 212, Loss: 1.0299, Train: 1.0398, Val: 1.0561, Test: 1.0413\n",
      "Epoch: 213, Loss: 1.0296, Train: 1.0395, Val: 1.0562, Test: 1.0412\n",
      "Epoch: 214, Loss: 1.0293, Train: 1.0393, Val: 1.0559, Test: 1.0411\n",
      "Epoch: 215, Loss: 1.0291, Train: 1.0390, Val: 1.0559, Test: 1.0405\n",
      "Epoch: 216, Loss: 1.0288, Train: 1.0388, Val: 1.0555, Test: 1.0404\n",
      "Epoch: 217, Loss: 1.0285, Train: 1.0386, Val: 1.0556, Test: 1.0404\n",
      "Epoch: 218, Loss: 1.0282, Train: 1.0384, Val: 1.0550, Test: 1.0401\n",
      "Epoch: 219, Loss: 1.0279, Train: 1.0380, Val: 1.0547, Test: 1.0400\n",
      "Epoch: 220, Loss: 1.0276, Train: 1.0377, Val: 1.0548, Test: 1.0396\n",
      "Epoch: 221, Loss: 1.0274, Train: 1.0373, Val: 1.0546, Test: 1.0394\n",
      "Epoch: 222, Loss: 1.0271, Train: 1.0370, Val: 1.0544, Test: 1.0391\n",
      "Epoch: 223, Loss: 1.0268, Train: 1.0367, Val: 1.0543, Test: 1.0390\n",
      "Epoch: 224, Loss: 1.0265, Train: 1.0366, Val: 1.0541, Test: 1.0388\n",
      "Epoch: 225, Loss: 1.0263, Train: 1.0361, Val: 1.0538, Test: 1.0384\n",
      "Epoch: 226, Loss: 1.0260, Train: 1.0360, Val: 1.0537, Test: 1.0383\n",
      "Epoch: 227, Loss: 1.0257, Train: 1.0357, Val: 1.0538, Test: 1.0377\n",
      "Epoch: 228, Loss: 1.0254, Train: 1.0354, Val: 1.0536, Test: 1.0376\n",
      "Epoch: 229, Loss: 1.0251, Train: 1.0351, Val: 1.0532, Test: 1.0373\n",
      "Epoch: 230, Loss: 1.0249, Train: 1.0349, Val: 1.0528, Test: 1.0375\n",
      "Epoch: 231, Loss: 1.0246, Train: 1.0346, Val: 1.0525, Test: 1.0373\n",
      "Epoch: 232, Loss: 1.0243, Train: 1.0344, Val: 1.0523, Test: 1.0372\n",
      "Epoch: 233, Loss: 1.0240, Train: 1.0341, Val: 1.0523, Test: 1.0367\n",
      "Epoch: 234, Loss: 1.0238, Train: 1.0339, Val: 1.0519, Test: 1.0367\n",
      "Epoch: 235, Loss: 1.0235, Train: 1.0337, Val: 1.0515, Test: 1.0365\n",
      "Epoch: 236, Loss: 1.0232, Train: 1.0335, Val: 1.0516, Test: 1.0361\n",
      "Epoch: 237, Loss: 1.0230, Train: 1.0332, Val: 1.0516, Test: 1.0359\n",
      "Epoch: 238, Loss: 1.0227, Train: 1.0329, Val: 1.0510, Test: 1.0358\n",
      "Epoch: 239, Loss: 1.0224, Train: 1.0326, Val: 1.0510, Test: 1.0360\n",
      "Epoch: 240, Loss: 1.0221, Train: 1.0324, Val: 1.0510, Test: 1.0357\n",
      "Epoch: 241, Loss: 1.0219, Train: 1.0322, Val: 1.0507, Test: 1.0355\n",
      "Epoch: 242, Loss: 1.0216, Train: 1.0320, Val: 1.0503, Test: 1.0352\n",
      "Epoch: 243, Loss: 1.0213, Train: 1.0317, Val: 1.0501, Test: 1.0350\n",
      "Epoch: 244, Loss: 1.0211, Train: 1.0315, Val: 1.0501, Test: 1.0344\n",
      "Epoch: 245, Loss: 1.0208, Train: 1.0311, Val: 1.0498, Test: 1.0340\n",
      "Epoch: 246, Loss: 1.0205, Train: 1.0307, Val: 1.0499, Test: 1.0334\n",
      "Epoch: 247, Loss: 1.0202, Train: 1.0305, Val: 1.0497, Test: 1.0332\n",
      "Epoch: 248, Loss: 1.0200, Train: 1.0302, Val: 1.0503, Test: 1.0329\n",
      "Epoch: 249, Loss: 1.0197, Train: 1.0299, Val: 1.0502, Test: 1.0328\n",
      "Epoch: 250, Loss: 1.0194, Train: 1.0297, Val: 1.0500, Test: 1.0327\n",
      "Epoch: 251, Loss: 1.0192, Train: 1.0294, Val: 1.0501, Test: 1.0325\n",
      "Epoch: 252, Loss: 1.0189, Train: 1.0291, Val: 1.0498, Test: 1.0324\n",
      "Epoch: 253, Loss: 1.0186, Train: 1.0289, Val: 1.0498, Test: 1.0325\n",
      "Epoch: 254, Loss: 1.0184, Train: 1.0286, Val: 1.0493, Test: 1.0323\n",
      "Epoch: 255, Loss: 1.0181, Train: 1.0284, Val: 1.0495, Test: 1.0321\n",
      "Epoch: 256, Loss: 1.0178, Train: 1.0281, Val: 1.0491, Test: 1.0321\n",
      "Epoch: 257, Loss: 1.0175, Train: 1.0280, Val: 1.0490, Test: 1.0317\n",
      "Epoch: 258, Loss: 1.0173, Train: 1.0277, Val: 1.0489, Test: 1.0316\n",
      "Epoch: 259, Loss: 1.0170, Train: 1.0274, Val: 1.0489, Test: 1.0316\n",
      "Epoch: 260, Loss: 1.0167, Train: 1.0271, Val: 1.0489, Test: 1.0315\n",
      "Epoch: 261, Loss: 1.0165, Train: 1.0269, Val: 1.0491, Test: 1.0311\n",
      "Epoch: 262, Loss: 1.0162, Train: 1.0265, Val: 1.0489, Test: 1.0307\n",
      "Epoch: 263, Loss: 1.0159, Train: 1.0263, Val: 1.0485, Test: 1.0303\n",
      "Epoch: 264, Loss: 1.0157, Train: 1.0259, Val: 1.0483, Test: 1.0303\n",
      "Epoch: 265, Loss: 1.0154, Train: 1.0257, Val: 1.0483, Test: 1.0302\n",
      "Epoch: 266, Loss: 1.0151, Train: 1.0254, Val: 1.0482, Test: 1.0297\n",
      "Epoch: 267, Loss: 1.0149, Train: 1.0252, Val: 1.0479, Test: 1.0296\n",
      "Epoch: 268, Loss: 1.0146, Train: 1.0249, Val: 1.0477, Test: 1.0293\n",
      "Epoch: 269, Loss: 1.0143, Train: 1.0246, Val: 1.0474, Test: 1.0289\n",
      "Epoch: 270, Loss: 1.0140, Train: 1.0243, Val: 1.0472, Test: 1.0287\n",
      "Epoch: 271, Loss: 1.0138, Train: 1.0240, Val: 1.0471, Test: 1.0284\n",
      "Epoch: 272, Loss: 1.0135, Train: 1.0237, Val: 1.0471, Test: 1.0283\n",
      "Epoch: 273, Loss: 1.0132, Train: 1.0235, Val: 1.0466, Test: 1.0280\n",
      "Epoch: 274, Loss: 1.0130, Train: 1.0233, Val: 1.0462, Test: 1.0278\n",
      "Epoch: 275, Loss: 1.0127, Train: 1.0230, Val: 1.0460, Test: 1.0278\n",
      "Epoch: 276, Loss: 1.0124, Train: 1.0228, Val: 1.0458, Test: 1.0276\n",
      "Epoch: 277, Loss: 1.0121, Train: 1.0225, Val: 1.0458, Test: 1.0273\n",
      "Epoch: 278, Loss: 1.0119, Train: 1.0223, Val: 1.0457, Test: 1.0270\n",
      "Epoch: 279, Loss: 1.0116, Train: 1.0220, Val: 1.0453, Test: 1.0269\n",
      "Epoch: 280, Loss: 1.0113, Train: 1.0217, Val: 1.0447, Test: 1.0270\n",
      "Epoch: 281, Loss: 1.0110, Train: 1.0216, Val: 1.0443, Test: 1.0268\n",
      "Epoch: 282, Loss: 1.0108, Train: 1.0213, Val: 1.0439, Test: 1.0267\n",
      "Epoch: 283, Loss: 1.0105, Train: 1.0211, Val: 1.0436, Test: 1.0267\n",
      "Epoch: 284, Loss: 1.0102, Train: 1.0207, Val: 1.0433, Test: 1.0265\n",
      "Epoch: 285, Loss: 1.0099, Train: 1.0204, Val: 1.0430, Test: 1.0261\n",
      "Epoch: 286, Loss: 1.0097, Train: 1.0201, Val: 1.0427, Test: 1.0260\n",
      "Epoch: 287, Loss: 1.0094, Train: 1.0199, Val: 1.0430, Test: 1.0258\n",
      "Epoch: 288, Loss: 1.0091, Train: 1.0196, Val: 1.0428, Test: 1.0256\n",
      "Epoch: 289, Loss: 1.0088, Train: 1.0193, Val: 1.0424, Test: 1.0255\n",
      "Epoch: 290, Loss: 1.0086, Train: 1.0191, Val: 1.0424, Test: 1.0252\n",
      "Epoch: 291, Loss: 1.0083, Train: 1.0187, Val: 1.0420, Test: 1.0247\n",
      "Epoch: 292, Loss: 1.0080, Train: 1.0185, Val: 1.0420, Test: 1.0245\n",
      "Epoch: 293, Loss: 1.0077, Train: 1.0181, Val: 1.0419, Test: 1.0242\n",
      "Epoch: 294, Loss: 1.0074, Train: 1.0179, Val: 1.0415, Test: 1.0243\n",
      "Epoch: 295, Loss: 1.0072, Train: 1.0176, Val: 1.0414, Test: 1.0242\n",
      "Epoch: 296, Loss: 1.0069, Train: 1.0173, Val: 1.0407, Test: 1.0235\n",
      "Epoch: 297, Loss: 1.0066, Train: 1.0170, Val: 1.0406, Test: 1.0233\n",
      "Epoch: 298, Loss: 1.0063, Train: 1.0167, Val: 1.0407, Test: 1.0231\n",
      "Epoch: 299, Loss: 1.0060, Train: 1.0163, Val: 1.0403, Test: 1.0231\n",
      "Epoch: 300, Loss: 1.0057, Train: 1.0161, Val: 1.0403, Test: 1.0229\n",
      "Epoch: 301, Loss: 1.0055, Train: 1.0160, Val: 1.0401, Test: 1.0226\n",
      "Epoch: 302, Loss: 1.0052, Train: 1.0157, Val: 1.0402, Test: 1.0225\n",
      "Epoch: 303, Loss: 1.0049, Train: 1.0153, Val: 1.0397, Test: 1.0222\n",
      "Epoch: 304, Loss: 1.0046, Train: 1.0151, Val: 1.0392, Test: 1.0219\n",
      "Epoch: 305, Loss: 1.0043, Train: 1.0149, Val: 1.0388, Test: 1.0218\n",
      "Epoch: 306, Loss: 1.0040, Train: 1.0146, Val: 1.0386, Test: 1.0216\n",
      "Epoch: 307, Loss: 1.0037, Train: 1.0143, Val: 1.0384, Test: 1.0213\n",
      "Epoch: 308, Loss: 1.0034, Train: 1.0139, Val: 1.0383, Test: 1.0213\n",
      "Epoch: 309, Loss: 1.0031, Train: 1.0136, Val: 1.0380, Test: 1.0208\n",
      "Epoch: 310, Loss: 1.0028, Train: 1.0133, Val: 1.0376, Test: 1.0208\n",
      "Epoch: 311, Loss: 1.0026, Train: 1.0130, Val: 1.0376, Test: 1.0203\n",
      "Epoch: 312, Loss: 1.0023, Train: 1.0125, Val: 1.0373, Test: 1.0200\n",
      "Epoch: 313, Loss: 1.0020, Train: 1.0122, Val: 1.0373, Test: 1.0193\n",
      "Epoch: 314, Loss: 1.0017, Train: 1.0120, Val: 1.0368, Test: 1.0193\n",
      "Epoch: 315, Loss: 1.0014, Train: 1.0117, Val: 1.0369, Test: 1.0191\n",
      "Epoch: 316, Loss: 1.0011, Train: 1.0114, Val: 1.0364, Test: 1.0187\n",
      "Epoch: 317, Loss: 1.0008, Train: 1.0111, Val: 1.0361, Test: 1.0185\n",
      "Epoch: 318, Loss: 1.0005, Train: 1.0108, Val: 1.0355, Test: 1.0182\n",
      "Epoch: 319, Loss: 1.0002, Train: 1.0105, Val: 1.0354, Test: 1.0179\n",
      "Epoch: 320, Loss: 0.9999, Train: 1.0101, Val: 1.0353, Test: 1.0176\n",
      "Epoch: 321, Loss: 0.9996, Train: 1.0099, Val: 1.0350, Test: 1.0171\n",
      "Epoch: 322, Loss: 0.9993, Train: 1.0095, Val: 1.0348, Test: 1.0167\n",
      "Epoch: 323, Loss: 0.9990, Train: 1.0094, Val: 1.0345, Test: 1.0164\n",
      "Epoch: 324, Loss: 0.9987, Train: 1.0091, Val: 1.0343, Test: 1.0165\n",
      "Epoch: 325, Loss: 0.9984, Train: 1.0086, Val: 1.0344, Test: 1.0160\n",
      "Epoch: 326, Loss: 0.9980, Train: 1.0084, Val: 1.0340, Test: 1.0155\n",
      "Epoch: 327, Loss: 0.9977, Train: 1.0080, Val: 1.0334, Test: 1.0153\n",
      "Epoch: 328, Loss: 0.9974, Train: 1.0078, Val: 1.0331, Test: 1.0153\n",
      "Epoch: 329, Loss: 0.9971, Train: 1.0076, Val: 1.0329, Test: 1.0150\n",
      "Epoch: 330, Loss: 0.9968, Train: 1.0074, Val: 1.0325, Test: 1.0149\n",
      "Epoch: 331, Loss: 0.9965, Train: 1.0071, Val: 1.0321, Test: 1.0146\n",
      "Epoch: 332, Loss: 0.9962, Train: 1.0068, Val: 1.0311, Test: 1.0141\n",
      "Epoch: 333, Loss: 0.9959, Train: 1.0064, Val: 1.0306, Test: 1.0139\n",
      "Epoch: 334, Loss: 0.9956, Train: 1.0060, Val: 1.0308, Test: 1.0134\n",
      "Epoch: 335, Loss: 0.9952, Train: 1.0057, Val: 1.0305, Test: 1.0133\n",
      "Epoch: 336, Loss: 0.9949, Train: 1.0053, Val: 1.0302, Test: 1.0129\n",
      "Epoch: 337, Loss: 0.9946, Train: 1.0050, Val: 1.0302, Test: 1.0126\n",
      "Epoch: 338, Loss: 0.9943, Train: 1.0048, Val: 1.0302, Test: 1.0124\n",
      "Epoch: 339, Loss: 0.9940, Train: 1.0044, Val: 1.0304, Test: 1.0124\n",
      "Epoch: 340, Loss: 0.9937, Train: 1.0041, Val: 1.0304, Test: 1.0118\n",
      "Epoch: 341, Loss: 0.9933, Train: 1.0038, Val: 1.0301, Test: 1.0116\n",
      "Epoch: 342, Loss: 0.9930, Train: 1.0034, Val: 1.0302, Test: 1.0114\n",
      "Epoch: 343, Loss: 0.9927, Train: 1.0030, Val: 1.0299, Test: 1.0111\n",
      "Epoch: 344, Loss: 0.9924, Train: 1.0026, Val: 1.0299, Test: 1.0108\n",
      "Epoch: 345, Loss: 0.9920, Train: 1.0021, Val: 1.0293, Test: 1.0106\n",
      "Epoch: 346, Loss: 0.9917, Train: 1.0017, Val: 1.0291, Test: 1.0105\n",
      "Epoch: 347, Loss: 0.9914, Train: 1.0015, Val: 1.0289, Test: 1.0104\n",
      "Epoch: 348, Loss: 0.9911, Train: 1.0012, Val: 1.0285, Test: 1.0099\n",
      "Epoch: 349, Loss: 0.9907, Train: 1.0009, Val: 1.0282, Test: 1.0099\n",
      "Epoch: 350, Loss: 0.9904, Train: 1.0005, Val: 1.0278, Test: 1.0097\n",
      "Epoch: 351, Loss: 0.9901, Train: 1.0001, Val: 1.0275, Test: 1.0095\n",
      "Epoch: 352, Loss: 0.9897, Train: 0.9997, Val: 1.0273, Test: 1.0092\n",
      "Epoch: 353, Loss: 0.9894, Train: 0.9994, Val: 1.0266, Test: 1.0094\n",
      "Epoch: 354, Loss: 0.9891, Train: 0.9990, Val: 1.0265, Test: 1.0095\n",
      "Epoch: 355, Loss: 0.9887, Train: 0.9987, Val: 1.0261, Test: 1.0092\n",
      "Epoch: 356, Loss: 0.9884, Train: 0.9983, Val: 1.0256, Test: 1.0087\n",
      "Epoch: 357, Loss: 0.9881, Train: 0.9979, Val: 1.0255, Test: 1.0082\n",
      "Epoch: 358, Loss: 0.9877, Train: 0.9975, Val: 1.0252, Test: 1.0081\n",
      "Epoch: 359, Loss: 0.9874, Train: 0.9972, Val: 1.0247, Test: 1.0078\n",
      "Epoch: 360, Loss: 0.9870, Train: 0.9968, Val: 1.0244, Test: 1.0075\n",
      "Epoch: 361, Loss: 0.9867, Train: 0.9964, Val: 1.0241, Test: 1.0074\n",
      "Epoch: 362, Loss: 0.9864, Train: 0.9962, Val: 1.0242, Test: 1.0071\n",
      "Epoch: 363, Loss: 0.9860, Train: 0.9959, Val: 1.0239, Test: 1.0067\n",
      "Epoch: 364, Loss: 0.9857, Train: 0.9953, Val: 1.0238, Test: 1.0066\n",
      "Epoch: 365, Loss: 0.9853, Train: 0.9950, Val: 1.0232, Test: 1.0070\n",
      "Epoch: 366, Loss: 0.9850, Train: 0.9947, Val: 1.0231, Test: 1.0068\n",
      "Epoch: 367, Loss: 0.9846, Train: 0.9943, Val: 1.0228, Test: 1.0066\n",
      "Epoch: 368, Loss: 0.9843, Train: 0.9940, Val: 1.0229, Test: 1.0060\n",
      "Epoch: 369, Loss: 0.9839, Train: 0.9937, Val: 1.0226, Test: 1.0057\n",
      "Epoch: 370, Loss: 0.9836, Train: 0.9934, Val: 1.0217, Test: 1.0056\n",
      "Epoch: 371, Loss: 0.9832, Train: 0.9929, Val: 1.0215, Test: 1.0052\n",
      "Epoch: 372, Loss: 0.9829, Train: 0.9926, Val: 1.0210, Test: 1.0045\n",
      "Epoch: 373, Loss: 0.9825, Train: 0.9922, Val: 1.0207, Test: 1.0041\n",
      "Epoch: 374, Loss: 0.9822, Train: 0.9919, Val: 1.0202, Test: 1.0043\n",
      "Epoch: 375, Loss: 0.9818, Train: 0.9916, Val: 1.0198, Test: 1.0038\n",
      "Epoch: 376, Loss: 0.9815, Train: 0.9912, Val: 1.0196, Test: 1.0034\n",
      "Epoch: 377, Loss: 0.9811, Train: 0.9910, Val: 1.0193, Test: 1.0031\n",
      "Epoch: 378, Loss: 0.9807, Train: 0.9904, Val: 1.0190, Test: 1.0022\n",
      "Epoch: 379, Loss: 0.9804, Train: 0.9900, Val: 1.0184, Test: 1.0025\n",
      "Epoch: 380, Loss: 0.9800, Train: 0.9898, Val: 1.0184, Test: 1.0020\n",
      "Epoch: 381, Loss: 0.9797, Train: 0.9895, Val: 1.0181, Test: 1.0019\n",
      "Epoch: 382, Loss: 0.9793, Train: 0.9893, Val: 1.0179, Test: 1.0022\n",
      "Epoch: 383, Loss: 0.9789, Train: 0.9890, Val: 1.0178, Test: 1.0024\n",
      "Epoch: 384, Loss: 0.9786, Train: 0.9887, Val: 1.0175, Test: 1.0021\n",
      "Epoch: 385, Loss: 0.9782, Train: 0.9884, Val: 1.0170, Test: 1.0018\n",
      "Epoch: 386, Loss: 0.9779, Train: 0.9881, Val: 1.0167, Test: 1.0014\n",
      "Epoch: 387, Loss: 0.9775, Train: 0.9877, Val: 1.0165, Test: 1.0008\n",
      "Epoch: 388, Loss: 0.9771, Train: 0.9874, Val: 1.0160, Test: 1.0006\n",
      "Epoch: 389, Loss: 0.9768, Train: 0.9871, Val: 1.0154, Test: 1.0005\n",
      "Epoch: 390, Loss: 0.9764, Train: 0.9868, Val: 1.0155, Test: 1.0000\n",
      "Epoch: 391, Loss: 0.9760, Train: 0.9863, Val: 1.0152, Test: 0.9998\n",
      "Epoch: 392, Loss: 0.9757, Train: 0.9858, Val: 1.0144, Test: 0.9997\n",
      "Epoch: 393, Loss: 0.9753, Train: 0.9854, Val: 1.0136, Test: 0.9993\n",
      "Epoch: 394, Loss: 0.9749, Train: 0.9849, Val: 1.0136, Test: 0.9983\n",
      "Epoch: 395, Loss: 0.9745, Train: 0.9844, Val: 1.0134, Test: 0.9981\n",
      "Epoch: 396, Loss: 0.9742, Train: 0.9841, Val: 1.0129, Test: 0.9975\n",
      "Epoch: 397, Loss: 0.9738, Train: 0.9837, Val: 1.0123, Test: 0.9970\n",
      "Epoch: 398, Loss: 0.9734, Train: 0.9834, Val: 1.0120, Test: 0.9970\n",
      "Epoch: 399, Loss: 0.9731, Train: 0.9830, Val: 1.0123, Test: 0.9965\n",
      "Epoch: 400, Loss: 0.9727, Train: 0.9827, Val: 1.0122, Test: 0.9962\n",
      "Epoch: 401, Loss: 0.9723, Train: 0.9824, Val: 1.0123, Test: 0.9953\n",
      "Epoch: 402, Loss: 0.9719, Train: 0.9820, Val: 1.0121, Test: 0.9946\n",
      "Epoch: 403, Loss: 0.9716, Train: 0.9818, Val: 1.0115, Test: 0.9946\n",
      "Epoch: 404, Loss: 0.9712, Train: 0.9814, Val: 1.0114, Test: 0.9946\n",
      "Epoch: 405, Loss: 0.9708, Train: 0.9810, Val: 1.0106, Test: 0.9940\n",
      "Epoch: 406, Loss: 0.9704, Train: 0.9805, Val: 1.0102, Test: 0.9936\n",
      "Epoch: 407, Loss: 0.9701, Train: 0.9801, Val: 1.0097, Test: 0.9932\n",
      "Epoch: 408, Loss: 0.9697, Train: 0.9798, Val: 1.0095, Test: 0.9925\n",
      "Epoch: 409, Loss: 0.9693, Train: 0.9794, Val: 1.0096, Test: 0.9922\n",
      "Epoch: 410, Loss: 0.9689, Train: 0.9791, Val: 1.0096, Test: 0.9917\n",
      "Epoch: 411, Loss: 0.9685, Train: 0.9787, Val: 1.0088, Test: 0.9915\n",
      "Epoch: 412, Loss: 0.9682, Train: 0.9783, Val: 1.0083, Test: 0.9913\n",
      "Epoch: 413, Loss: 0.9678, Train: 0.9778, Val: 1.0079, Test: 0.9910\n",
      "Epoch: 414, Loss: 0.9674, Train: 0.9774, Val: 1.0079, Test: 0.9903\n",
      "Epoch: 415, Loss: 0.9670, Train: 0.9770, Val: 1.0072, Test: 0.9900\n",
      "Epoch: 416, Loss: 0.9666, Train: 0.9767, Val: 1.0064, Test: 0.9897\n",
      "Epoch: 417, Loss: 0.9662, Train: 0.9764, Val: 1.0061, Test: 0.9893\n",
      "Epoch: 418, Loss: 0.9658, Train: 0.9758, Val: 1.0055, Test: 0.9888\n",
      "Epoch: 419, Loss: 0.9654, Train: 0.9753, Val: 1.0053, Test: 0.9883\n",
      "Epoch: 420, Loss: 0.9650, Train: 0.9750, Val: 1.0052, Test: 0.9882\n",
      "Epoch: 421, Loss: 0.9646, Train: 0.9746, Val: 1.0052, Test: 0.9882\n",
      "Epoch: 422, Loss: 0.9642, Train: 0.9741, Val: 1.0048, Test: 0.9882\n",
      "Epoch: 423, Loss: 0.9638, Train: 0.9735, Val: 1.0046, Test: 0.9877\n",
      "Epoch: 424, Loss: 0.9634, Train: 0.9733, Val: 1.0038, Test: 0.9875\n",
      "Epoch: 425, Loss: 0.9630, Train: 0.9728, Val: 1.0035, Test: 0.9871\n",
      "Epoch: 426, Loss: 0.9626, Train: 0.9723, Val: 1.0032, Test: 0.9867\n",
      "Epoch: 427, Loss: 0.9621, Train: 0.9719, Val: 1.0029, Test: 0.9861\n",
      "Epoch: 428, Loss: 0.9617, Train: 0.9715, Val: 1.0020, Test: 0.9859\n",
      "Epoch: 429, Loss: 0.9613, Train: 0.9709, Val: 1.0015, Test: 0.9857\n",
      "Epoch: 430, Loss: 0.9608, Train: 0.9704, Val: 1.0009, Test: 0.9852\n",
      "Epoch: 431, Loss: 0.9604, Train: 0.9700, Val: 1.0004, Test: 0.9847\n",
      "Epoch: 432, Loss: 0.9600, Train: 0.9694, Val: 0.9998, Test: 0.9840\n",
      "Epoch: 433, Loss: 0.9595, Train: 0.9690, Val: 0.9991, Test: 0.9834\n",
      "Epoch: 434, Loss: 0.9591, Train: 0.9685, Val: 0.9987, Test: 0.9832\n",
      "Epoch: 435, Loss: 0.9587, Train: 0.9682, Val: 0.9987, Test: 0.9832\n",
      "Epoch: 436, Loss: 0.9582, Train: 0.9678, Val: 0.9982, Test: 0.9830\n",
      "Epoch: 437, Loss: 0.9578, Train: 0.9674, Val: 0.9978, Test: 0.9828\n",
      "Epoch: 438, Loss: 0.9573, Train: 0.9670, Val: 0.9976, Test: 0.9824\n",
      "Epoch: 439, Loss: 0.9569, Train: 0.9664, Val: 0.9975, Test: 0.9821\n",
      "Epoch: 440, Loss: 0.9565, Train: 0.9659, Val: 0.9963, Test: 0.9814\n",
      "Epoch: 441, Loss: 0.9560, Train: 0.9655, Val: 0.9955, Test: 0.9815\n",
      "Epoch: 442, Loss: 0.9556, Train: 0.9647, Val: 0.9953, Test: 0.9808\n",
      "Epoch: 443, Loss: 0.9552, Train: 0.9644, Val: 0.9945, Test: 0.9805\n",
      "Epoch: 444, Loss: 0.9547, Train: 0.9640, Val: 0.9941, Test: 0.9805\n",
      "Epoch: 445, Loss: 0.9543, Train: 0.9635, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 446, Loss: 0.9539, Train: 0.9629, Val: 0.9934, Test: 0.9793\n",
      "Epoch: 447, Loss: 0.9534, Train: 0.9626, Val: 0.9933, Test: 0.9785\n",
      "Epoch: 448, Loss: 0.9530, Train: 0.9621, Val: 0.9934, Test: 0.9779\n",
      "Epoch: 449, Loss: 0.9526, Train: 0.9617, Val: 0.9931, Test: 0.9778\n",
      "Epoch: 450, Loss: 0.9522, Train: 0.9611, Val: 0.9923, Test: 0.9775\n",
      "Epoch: 451, Loss: 0.9518, Train: 0.9607, Val: 0.9922, Test: 0.9772\n",
      "Epoch: 452, Loss: 0.9513, Train: 0.9605, Val: 0.9926, Test: 0.9770\n",
      "Epoch: 453, Loss: 0.9509, Train: 0.9603, Val: 0.9925, Test: 0.9765\n",
      "Epoch: 454, Loss: 0.9505, Train: 0.9599, Val: 0.9922, Test: 0.9762\n",
      "Epoch: 455, Loss: 0.9501, Train: 0.9595, Val: 0.9924, Test: 0.9761\n",
      "Epoch: 456, Loss: 0.9497, Train: 0.9591, Val: 0.9925, Test: 0.9756\n",
      "Epoch: 457, Loss: 0.9493, Train: 0.9588, Val: 0.9919, Test: 0.9748\n",
      "Epoch: 458, Loss: 0.9489, Train: 0.9584, Val: 0.9915, Test: 0.9750\n",
      "Epoch: 459, Loss: 0.9485, Train: 0.9581, Val: 0.9915, Test: 0.9750\n",
      "Epoch: 460, Loss: 0.9481, Train: 0.9579, Val: 0.9908, Test: 0.9747\n",
      "Epoch: 461, Loss: 0.9478, Train: 0.9575, Val: 0.9907, Test: 0.9742\n",
      "Epoch: 462, Loss: 0.9474, Train: 0.9570, Val: 0.9907, Test: 0.9740\n",
      "Epoch: 463, Loss: 0.9470, Train: 0.9564, Val: 0.9906, Test: 0.9739\n",
      "Epoch: 464, Loss: 0.9466, Train: 0.9562, Val: 0.9903, Test: 0.9737\n",
      "Epoch: 465, Loss: 0.9463, Train: 0.9558, Val: 0.9901, Test: 0.9737\n",
      "Epoch: 466, Loss: 0.9459, Train: 0.9555, Val: 0.9895, Test: 0.9735\n",
      "Epoch: 467, Loss: 0.9456, Train: 0.9552, Val: 0.9893, Test: 0.9729\n",
      "Epoch: 468, Loss: 0.9452, Train: 0.9548, Val: 0.9889, Test: 0.9726\n",
      "Epoch: 469, Loss: 0.9449, Train: 0.9545, Val: 0.9884, Test: 0.9717\n",
      "Epoch: 470, Loss: 0.9445, Train: 0.9542, Val: 0.9883, Test: 0.9711\n",
      "Epoch: 471, Loss: 0.9442, Train: 0.9538, Val: 0.9879, Test: 0.9712\n",
      "Epoch: 472, Loss: 0.9439, Train: 0.9535, Val: 0.9878, Test: 0.9706\n",
      "Epoch: 473, Loss: 0.9435, Train: 0.9531, Val: 0.9875, Test: 0.9699\n",
      "Epoch: 474, Loss: 0.9432, Train: 0.9529, Val: 0.9875, Test: 0.9693\n",
      "Epoch: 475, Loss: 0.9429, Train: 0.9527, Val: 0.9875, Test: 0.9692\n",
      "Epoch: 476, Loss: 0.9426, Train: 0.9524, Val: 0.9874, Test: 0.9692\n",
      "Epoch: 477, Loss: 0.9423, Train: 0.9521, Val: 0.9867, Test: 0.9693\n",
      "Epoch: 478, Loss: 0.9420, Train: 0.9519, Val: 0.9864, Test: 0.9692\n",
      "Epoch: 479, Loss: 0.9417, Train: 0.9517, Val: 0.9865, Test: 0.9683\n",
      "Epoch: 480, Loss: 0.9414, Train: 0.9516, Val: 0.9860, Test: 0.9682\n",
      "Epoch: 481, Loss: 0.9411, Train: 0.9513, Val: 0.9860, Test: 0.9679\n",
      "Epoch: 482, Loss: 0.9408, Train: 0.9511, Val: 0.9860, Test: 0.9675\n",
      "Epoch: 483, Loss: 0.9405, Train: 0.9508, Val: 0.9856, Test: 0.9671\n",
      "Epoch: 484, Loss: 0.9402, Train: 0.9504, Val: 0.9857, Test: 0.9667\n",
      "Epoch: 485, Loss: 0.9400, Train: 0.9502, Val: 0.9856, Test: 0.9669\n",
      "Epoch: 486, Loss: 0.9397, Train: 0.9500, Val: 0.9853, Test: 0.9668\n",
      "Epoch: 487, Loss: 0.9394, Train: 0.9498, Val: 0.9852, Test: 0.9667\n",
      "Epoch: 488, Loss: 0.9392, Train: 0.9496, Val: 0.9852, Test: 0.9664\n",
      "Epoch: 489, Loss: 0.9389, Train: 0.9494, Val: 0.9850, Test: 0.9664\n",
      "Epoch: 490, Loss: 0.9387, Train: 0.9490, Val: 0.9849, Test: 0.9658\n",
      "Epoch: 491, Loss: 0.9384, Train: 0.9488, Val: 0.9844, Test: 0.9656\n",
      "Epoch: 492, Loss: 0.9382, Train: 0.9486, Val: 0.9842, Test: 0.9653\n",
      "Epoch: 493, Loss: 0.9380, Train: 0.9483, Val: 0.9841, Test: 0.9649\n",
      "Epoch: 494, Loss: 0.9377, Train: 0.9480, Val: 0.9839, Test: 0.9645\n",
      "Epoch: 495, Loss: 0.9375, Train: 0.9476, Val: 0.9838, Test: 0.9644\n",
      "Epoch: 496, Loss: 0.9373, Train: 0.9475, Val: 0.9834, Test: 0.9638\n",
      "Epoch: 497, Loss: 0.9370, Train: 0.9474, Val: 0.9833, Test: 0.9634\n",
      "Epoch: 498, Loss: 0.9368, Train: 0.9473, Val: 0.9832, Test: 0.9635\n",
      "Epoch: 499, Loss: 0.9366, Train: 0.9473, Val: 0.9831, Test: 0.9633\n",
      "Epoch: 500, Loss: 0.9364, Train: 0.9470, Val: 0.9827, Test: 0.9635\n",
      "Epoch: 501, Loss: 0.9362, Train: 0.9470, Val: 0.9821, Test: 0.9635\n",
      "Epoch: 502, Loss: 0.9360, Train: 0.9468, Val: 0.9820, Test: 0.9636\n",
      "Epoch: 503, Loss: 0.9358, Train: 0.9467, Val: 0.9820, Test: 0.9631\n",
      "Epoch: 504, Loss: 0.9356, Train: 0.9465, Val: 0.9818, Test: 0.9627\n",
      "Epoch: 505, Loss: 0.9354, Train: 0.9463, Val: 0.9820, Test: 0.9629\n",
      "Epoch: 506, Loss: 0.9352, Train: 0.9462, Val: 0.9813, Test: 0.9630\n",
      "Epoch: 507, Loss: 0.9350, Train: 0.9460, Val: 0.9815, Test: 0.9628\n",
      "Epoch: 508, Loss: 0.9348, Train: 0.9458, Val: 0.9813, Test: 0.9631\n",
      "Epoch: 509, Loss: 0.9346, Train: 0.9457, Val: 0.9809, Test: 0.9626\n",
      "Epoch: 510, Loss: 0.9345, Train: 0.9456, Val: 0.9808, Test: 0.9625\n",
      "Epoch: 511, Loss: 0.9343, Train: 0.9456, Val: 0.9808, Test: 0.9623\n",
      "Epoch: 512, Loss: 0.9341, Train: 0.9454, Val: 0.9808, Test: 0.9623\n",
      "Epoch: 513, Loss: 0.9339, Train: 0.9451, Val: 0.9805, Test: 0.9623\n",
      "Epoch: 514, Loss: 0.9338, Train: 0.9450, Val: 0.9804, Test: 0.9625\n",
      "Epoch: 515, Loss: 0.9336, Train: 0.9449, Val: 0.9800, Test: 0.9622\n",
      "Epoch: 516, Loss: 0.9334, Train: 0.9448, Val: 0.9802, Test: 0.9621\n",
      "Epoch: 517, Loss: 0.9333, Train: 0.9447, Val: 0.9802, Test: 0.9622\n",
      "Epoch: 518, Loss: 0.9331, Train: 0.9445, Val: 0.9799, Test: 0.9617\n",
      "Epoch: 519, Loss: 0.9330, Train: 0.9441, Val: 0.9797, Test: 0.9614\n",
      "Epoch: 520, Loss: 0.9328, Train: 0.9439, Val: 0.9795, Test: 0.9610\n",
      "Epoch: 521, Loss: 0.9326, Train: 0.9439, Val: 0.9790, Test: 0.9608\n",
      "Epoch: 522, Loss: 0.9325, Train: 0.9437, Val: 0.9788, Test: 0.9604\n",
      "Epoch: 523, Loss: 0.9323, Train: 0.9436, Val: 0.9787, Test: 0.9605\n",
      "Epoch: 524, Loss: 0.9322, Train: 0.9436, Val: 0.9787, Test: 0.9604\n",
      "Epoch: 525, Loss: 0.9320, Train: 0.9435, Val: 0.9785, Test: 0.9595\n",
      "Epoch: 526, Loss: 0.9319, Train: 0.9433, Val: 0.9784, Test: 0.9591\n",
      "Epoch: 527, Loss: 0.9318, Train: 0.9431, Val: 0.9782, Test: 0.9591\n",
      "Epoch: 528, Loss: 0.9316, Train: 0.9430, Val: 0.9779, Test: 0.9590\n",
      "Epoch: 529, Loss: 0.9315, Train: 0.9427, Val: 0.9779, Test: 0.9588\n",
      "Epoch: 530, Loss: 0.9313, Train: 0.9425, Val: 0.9779, Test: 0.9585\n",
      "Epoch: 531, Loss: 0.9312, Train: 0.9424, Val: 0.9776, Test: 0.9584\n",
      "Epoch: 532, Loss: 0.9311, Train: 0.9422, Val: 0.9770, Test: 0.9582\n",
      "Epoch: 533, Loss: 0.9309, Train: 0.9420, Val: 0.9768, Test: 0.9582\n",
      "Epoch: 534, Loss: 0.9308, Train: 0.9421, Val: 0.9771, Test: 0.9580\n",
      "Epoch: 535, Loss: 0.9307, Train: 0.9420, Val: 0.9773, Test: 0.9582\n",
      "Epoch: 536, Loss: 0.9306, Train: 0.9419, Val: 0.9770, Test: 0.9582\n",
      "Epoch: 537, Loss: 0.9304, Train: 0.9418, Val: 0.9772, Test: 0.9581\n",
      "Epoch: 538, Loss: 0.9303, Train: 0.9416, Val: 0.9771, Test: 0.9579\n",
      "Epoch: 539, Loss: 0.9302, Train: 0.9415, Val: 0.9768, Test: 0.9579\n",
      "Epoch: 540, Loss: 0.9301, Train: 0.9416, Val: 0.9769, Test: 0.9579\n",
      "Epoch: 541, Loss: 0.9299, Train: 0.9414, Val: 0.9766, Test: 0.9579\n",
      "Epoch: 542, Loss: 0.9298, Train: 0.9413, Val: 0.9764, Test: 0.9581\n",
      "Epoch: 543, Loss: 0.9297, Train: 0.9410, Val: 0.9764, Test: 0.9579\n",
      "Epoch: 544, Loss: 0.9296, Train: 0.9410, Val: 0.9762, Test: 0.9580\n",
      "Epoch: 545, Loss: 0.9295, Train: 0.9409, Val: 0.9760, Test: 0.9578\n",
      "Epoch: 546, Loss: 0.9294, Train: 0.9407, Val: 0.9760, Test: 0.9580\n",
      "Epoch: 547, Loss: 0.9292, Train: 0.9405, Val: 0.9756, Test: 0.9579\n",
      "Epoch: 548, Loss: 0.9291, Train: 0.9405, Val: 0.9753, Test: 0.9580\n",
      "Epoch: 549, Loss: 0.9290, Train: 0.9403, Val: 0.9752, Test: 0.9578\n",
      "Epoch: 550, Loss: 0.9289, Train: 0.9403, Val: 0.9750, Test: 0.9577\n",
      "Epoch: 551, Loss: 0.9288, Train: 0.9401, Val: 0.9751, Test: 0.9576\n",
      "Epoch: 552, Loss: 0.9287, Train: 0.9400, Val: 0.9750, Test: 0.9576\n",
      "Epoch: 553, Loss: 0.9286, Train: 0.9399, Val: 0.9751, Test: 0.9576\n",
      "Epoch: 554, Loss: 0.9285, Train: 0.9399, Val: 0.9751, Test: 0.9580\n",
      "Epoch: 555, Loss: 0.9284, Train: 0.9398, Val: 0.9750, Test: 0.9579\n",
      "Epoch: 556, Loss: 0.9283, Train: 0.9396, Val: 0.9751, Test: 0.9582\n",
      "Epoch: 557, Loss: 0.9282, Train: 0.9395, Val: 0.9751, Test: 0.9579\n",
      "Epoch: 558, Loss: 0.9281, Train: 0.9395, Val: 0.9750, Test: 0.9579\n",
      "Epoch: 559, Loss: 0.9280, Train: 0.9395, Val: 0.9750, Test: 0.9579\n",
      "Epoch: 560, Loss: 0.9279, Train: 0.9393, Val: 0.9753, Test: 0.9579\n",
      "Epoch: 561, Loss: 0.9278, Train: 0.9392, Val: 0.9751, Test: 0.9579\n",
      "Epoch: 562, Loss: 0.9277, Train: 0.9392, Val: 0.9754, Test: 0.9577\n",
      "Epoch: 563, Loss: 0.9276, Train: 0.9391, Val: 0.9752, Test: 0.9576\n",
      "Epoch: 564, Loss: 0.9275, Train: 0.9390, Val: 0.9747, Test: 0.9576\n",
      "Epoch: 565, Loss: 0.9274, Train: 0.9390, Val: 0.9746, Test: 0.9576\n",
      "Epoch: 566, Loss: 0.9273, Train: 0.9389, Val: 0.9746, Test: 0.9576\n",
      "Epoch: 567, Loss: 0.9272, Train: 0.9388, Val: 0.9742, Test: 0.9575\n",
      "Epoch: 568, Loss: 0.9271, Train: 0.9387, Val: 0.9741, Test: 0.9571\n",
      "Epoch: 569, Loss: 0.9270, Train: 0.9386, Val: 0.9741, Test: 0.9570\n",
      "Epoch: 570, Loss: 0.9269, Train: 0.9385, Val: 0.9742, Test: 0.9571\n",
      "Epoch: 571, Loss: 0.9268, Train: 0.9384, Val: 0.9740, Test: 0.9572\n",
      "Epoch: 572, Loss: 0.9267, Train: 0.9384, Val: 0.9738, Test: 0.9569\n",
      "Epoch: 573, Loss: 0.9266, Train: 0.9384, Val: 0.9738, Test: 0.9572\n",
      "Epoch: 574, Loss: 0.9265, Train: 0.9382, Val: 0.9739, Test: 0.9572\n",
      "Epoch: 575, Loss: 0.9264, Train: 0.9381, Val: 0.9739, Test: 0.9570\n",
      "Epoch: 576, Loss: 0.9264, Train: 0.9380, Val: 0.9738, Test: 0.9569\n",
      "Epoch: 577, Loss: 0.9263, Train: 0.9380, Val: 0.9735, Test: 0.9570\n",
      "Epoch: 578, Loss: 0.9262, Train: 0.9380, Val: 0.9735, Test: 0.9569\n",
      "Epoch: 579, Loss: 0.9261, Train: 0.9379, Val: 0.9730, Test: 0.9569\n",
      "Epoch: 580, Loss: 0.9260, Train: 0.9378, Val: 0.9732, Test: 0.9567\n",
      "Epoch: 581, Loss: 0.9259, Train: 0.9377, Val: 0.9732, Test: 0.9565\n",
      "Epoch: 582, Loss: 0.9258, Train: 0.9375, Val: 0.9732, Test: 0.9564\n",
      "Epoch: 583, Loss: 0.9257, Train: 0.9373, Val: 0.9730, Test: 0.9561\n",
      "Epoch: 584, Loss: 0.9257, Train: 0.9373, Val: 0.9728, Test: 0.9559\n",
      "Epoch: 585, Loss: 0.9256, Train: 0.9373, Val: 0.9728, Test: 0.9560\n",
      "Epoch: 586, Loss: 0.9255, Train: 0.9373, Val: 0.9728, Test: 0.9559\n",
      "Epoch: 587, Loss: 0.9254, Train: 0.9372, Val: 0.9725, Test: 0.9557\n",
      "Epoch: 588, Loss: 0.9253, Train: 0.9371, Val: 0.9726, Test: 0.9555\n",
      "Epoch: 589, Loss: 0.9252, Train: 0.9370, Val: 0.9726, Test: 0.9556\n",
      "Epoch: 590, Loss: 0.9252, Train: 0.9370, Val: 0.9727, Test: 0.9556\n",
      "Epoch: 591, Loss: 0.9251, Train: 0.9369, Val: 0.9726, Test: 0.9558\n",
      "Epoch: 592, Loss: 0.9250, Train: 0.9369, Val: 0.9722, Test: 0.9560\n",
      "Epoch: 593, Loss: 0.9249, Train: 0.9368, Val: 0.9724, Test: 0.9556\n",
      "Epoch: 594, Loss: 0.9248, Train: 0.9366, Val: 0.9726, Test: 0.9555\n",
      "Epoch: 595, Loss: 0.9248, Train: 0.9365, Val: 0.9724, Test: 0.9554\n",
      "Epoch: 596, Loss: 0.9247, Train: 0.9364, Val: 0.9722, Test: 0.9554\n",
      "Epoch: 597, Loss: 0.9246, Train: 0.9363, Val: 0.9721, Test: 0.9552\n",
      "Epoch: 598, Loss: 0.9245, Train: 0.9362, Val: 0.9719, Test: 0.9553\n",
      "Epoch: 599, Loss: 0.9244, Train: 0.9362, Val: 0.9718, Test: 0.9556\n",
      "Epoch: 600, Loss: 0.9244, Train: 0.9361, Val: 0.9715, Test: 0.9555\n",
      "Epoch: 601, Loss: 0.9243, Train: 0.9360, Val: 0.9717, Test: 0.9555\n",
      "Epoch: 602, Loss: 0.9242, Train: 0.9360, Val: 0.9717, Test: 0.9555\n",
      "Epoch: 603, Loss: 0.9241, Train: 0.9359, Val: 0.9716, Test: 0.9556\n",
      "Epoch: 604, Loss: 0.9241, Train: 0.9358, Val: 0.9717, Test: 0.9557\n",
      "Epoch: 605, Loss: 0.9240, Train: 0.9357, Val: 0.9717, Test: 0.9557\n",
      "Epoch: 606, Loss: 0.9239, Train: 0.9355, Val: 0.9717, Test: 0.9559\n",
      "Epoch: 607, Loss: 0.9238, Train: 0.9355, Val: 0.9717, Test: 0.9561\n",
      "Epoch: 608, Loss: 0.9238, Train: 0.9353, Val: 0.9719, Test: 0.9562\n",
      "Epoch: 609, Loss: 0.9237, Train: 0.9353, Val: 0.9721, Test: 0.9559\n",
      "Epoch: 610, Loss: 0.9236, Train: 0.9353, Val: 0.9722, Test: 0.9558\n",
      "Epoch: 611, Loss: 0.9235, Train: 0.9352, Val: 0.9725, Test: 0.9559\n",
      "Epoch: 612, Loss: 0.9235, Train: 0.9352, Val: 0.9722, Test: 0.9560\n",
      "Epoch: 613, Loss: 0.9234, Train: 0.9351, Val: 0.9722, Test: 0.9558\n",
      "Epoch: 614, Loss: 0.9233, Train: 0.9350, Val: 0.9720, Test: 0.9558\n",
      "Epoch: 615, Loss: 0.9232, Train: 0.9349, Val: 0.9721, Test: 0.9557\n",
      "Epoch: 616, Loss: 0.9232, Train: 0.9348, Val: 0.9721, Test: 0.9554\n",
      "Epoch: 617, Loss: 0.9231, Train: 0.9347, Val: 0.9721, Test: 0.9555\n",
      "Epoch: 618, Loss: 0.9230, Train: 0.9347, Val: 0.9721, Test: 0.9554\n",
      "Epoch: 619, Loss: 0.9230, Train: 0.9346, Val: 0.9718, Test: 0.9555\n",
      "Epoch: 620, Loss: 0.9229, Train: 0.9345, Val: 0.9714, Test: 0.9555\n",
      "Epoch: 621, Loss: 0.9228, Train: 0.9345, Val: 0.9715, Test: 0.9555\n",
      "Epoch: 622, Loss: 0.9227, Train: 0.9344, Val: 0.9717, Test: 0.9555\n",
      "Epoch: 623, Loss: 0.9227, Train: 0.9343, Val: 0.9717, Test: 0.9555\n",
      "Epoch: 624, Loss: 0.9226, Train: 0.9342, Val: 0.9714, Test: 0.9554\n",
      "Epoch: 625, Loss: 0.9225, Train: 0.9341, Val: 0.9713, Test: 0.9558\n",
      "Epoch: 626, Loss: 0.9225, Train: 0.9340, Val: 0.9713, Test: 0.9557\n",
      "Epoch: 627, Loss: 0.9224, Train: 0.9340, Val: 0.9714, Test: 0.9555\n",
      "Epoch: 628, Loss: 0.9223, Train: 0.9340, Val: 0.9713, Test: 0.9556\n",
      "Epoch: 629, Loss: 0.9223, Train: 0.9338, Val: 0.9714, Test: 0.9554\n",
      "Epoch: 630, Loss: 0.9222, Train: 0.9337, Val: 0.9714, Test: 0.9554\n",
      "Epoch: 631, Loss: 0.9221, Train: 0.9336, Val: 0.9713, Test: 0.9555\n",
      "Epoch: 632, Loss: 0.9221, Train: 0.9335, Val: 0.9713, Test: 0.9554\n",
      "Epoch: 633, Loss: 0.9220, Train: 0.9334, Val: 0.9714, Test: 0.9552\n",
      "Epoch: 634, Loss: 0.9219, Train: 0.9334, Val: 0.9713, Test: 0.9550\n",
      "Epoch: 635, Loss: 0.9219, Train: 0.9333, Val: 0.9710, Test: 0.9549\n",
      "Epoch: 636, Loss: 0.9218, Train: 0.9332, Val: 0.9711, Test: 0.9546\n",
      "Epoch: 637, Loss: 0.9217, Train: 0.9332, Val: 0.9711, Test: 0.9547\n",
      "Epoch: 638, Loss: 0.9217, Train: 0.9331, Val: 0.9711, Test: 0.9545\n",
      "Epoch: 639, Loss: 0.9216, Train: 0.9330, Val: 0.9711, Test: 0.9543\n",
      "Epoch: 640, Loss: 0.9215, Train: 0.9329, Val: 0.9710, Test: 0.9540\n",
      "Epoch: 641, Loss: 0.9215, Train: 0.9328, Val: 0.9711, Test: 0.9540\n",
      "Epoch: 642, Loss: 0.9214, Train: 0.9327, Val: 0.9710, Test: 0.9540\n",
      "Epoch: 643, Loss: 0.9213, Train: 0.9327, Val: 0.9709, Test: 0.9539\n",
      "Epoch: 644, Loss: 0.9213, Train: 0.9327, Val: 0.9710, Test: 0.9542\n",
      "Epoch: 645, Loss: 0.9212, Train: 0.9326, Val: 0.9712, Test: 0.9539\n",
      "Epoch: 646, Loss: 0.9211, Train: 0.9326, Val: 0.9712, Test: 0.9541\n",
      "Epoch: 647, Loss: 0.9211, Train: 0.9326, Val: 0.9712, Test: 0.9541\n",
      "Epoch: 648, Loss: 0.9210, Train: 0.9326, Val: 0.9711, Test: 0.9540\n",
      "Epoch: 649, Loss: 0.9209, Train: 0.9324, Val: 0.9711, Test: 0.9541\n",
      "Epoch: 650, Loss: 0.9209, Train: 0.9324, Val: 0.9710, Test: 0.9540\n",
      "Epoch: 651, Loss: 0.9208, Train: 0.9324, Val: 0.9711, Test: 0.9537\n",
      "Epoch: 652, Loss: 0.9208, Train: 0.9324, Val: 0.9708, Test: 0.9539\n",
      "Epoch: 653, Loss: 0.9207, Train: 0.9323, Val: 0.9707, Test: 0.9538\n",
      "Epoch: 654, Loss: 0.9206, Train: 0.9322, Val: 0.9708, Test: 0.9537\n",
      "Epoch: 655, Loss: 0.9206, Train: 0.9322, Val: 0.9710, Test: 0.9536\n",
      "Epoch: 656, Loss: 0.9205, Train: 0.9321, Val: 0.9712, Test: 0.9539\n",
      "Epoch: 657, Loss: 0.9205, Train: 0.9321, Val: 0.9713, Test: 0.9540\n",
      "Epoch: 658, Loss: 0.9204, Train: 0.9320, Val: 0.9713, Test: 0.9541\n",
      "Epoch: 659, Loss: 0.9203, Train: 0.9320, Val: 0.9712, Test: 0.9541\n",
      "Epoch: 660, Loss: 0.9203, Train: 0.9319, Val: 0.9713, Test: 0.9539\n",
      "Epoch: 661, Loss: 0.9202, Train: 0.9317, Val: 0.9712, Test: 0.9539\n",
      "Epoch: 662, Loss: 0.9201, Train: 0.9317, Val: 0.9713, Test: 0.9539\n",
      "Epoch: 663, Loss: 0.9201, Train: 0.9317, Val: 0.9713, Test: 0.9538\n",
      "Epoch: 664, Loss: 0.9200, Train: 0.9315, Val: 0.9710, Test: 0.9540\n",
      "Epoch: 665, Loss: 0.9200, Train: 0.9315, Val: 0.9708, Test: 0.9541\n",
      "Epoch: 666, Loss: 0.9199, Train: 0.9314, Val: 0.9707, Test: 0.9539\n",
      "Epoch: 667, Loss: 0.9198, Train: 0.9313, Val: 0.9708, Test: 0.9539\n",
      "Epoch: 668, Loss: 0.9198, Train: 0.9313, Val: 0.9709, Test: 0.9539\n",
      "Epoch: 669, Loss: 0.9197, Train: 0.9312, Val: 0.9710, Test: 0.9538\n",
      "Epoch: 670, Loss: 0.9197, Train: 0.9310, Val: 0.9709, Test: 0.9535\n",
      "Epoch: 671, Loss: 0.9196, Train: 0.9310, Val: 0.9708, Test: 0.9534\n",
      "Epoch: 672, Loss: 0.9196, Train: 0.9310, Val: 0.9706, Test: 0.9535\n",
      "Epoch: 673, Loss: 0.9195, Train: 0.9308, Val: 0.9704, Test: 0.9537\n",
      "Epoch: 674, Loss: 0.9194, Train: 0.9308, Val: 0.9704, Test: 0.9538\n",
      "Epoch: 675, Loss: 0.9194, Train: 0.9307, Val: 0.9704, Test: 0.9538\n",
      "Epoch: 676, Loss: 0.9193, Train: 0.9305, Val: 0.9703, Test: 0.9539\n",
      "Epoch: 677, Loss: 0.9193, Train: 0.9304, Val: 0.9702, Test: 0.9540\n",
      "Epoch: 678, Loss: 0.9192, Train: 0.9305, Val: 0.9702, Test: 0.9541\n",
      "Epoch: 679, Loss: 0.9191, Train: 0.9305, Val: 0.9702, Test: 0.9542\n",
      "Epoch: 680, Loss: 0.9191, Train: 0.9304, Val: 0.9702, Test: 0.9542\n",
      "Epoch: 681, Loss: 0.9190, Train: 0.9303, Val: 0.9704, Test: 0.9540\n",
      "Epoch: 682, Loss: 0.9190, Train: 0.9303, Val: 0.9700, Test: 0.9538\n",
      "Epoch: 683, Loss: 0.9189, Train: 0.9302, Val: 0.9702, Test: 0.9540\n",
      "Epoch: 684, Loss: 0.9189, Train: 0.9301, Val: 0.9701, Test: 0.9539\n",
      "Epoch: 685, Loss: 0.9188, Train: 0.9300, Val: 0.9699, Test: 0.9538\n",
      "Epoch: 686, Loss: 0.9188, Train: 0.9299, Val: 0.9698, Test: 0.9537\n",
      "Epoch: 687, Loss: 0.9187, Train: 0.9298, Val: 0.9699, Test: 0.9535\n",
      "Epoch: 688, Loss: 0.9186, Train: 0.9297, Val: 0.9698, Test: 0.9534\n",
      "Epoch: 689, Loss: 0.9186, Train: 0.9297, Val: 0.9697, Test: 0.9533\n",
      "Epoch: 690, Loss: 0.9185, Train: 0.9297, Val: 0.9698, Test: 0.9533\n",
      "Epoch: 691, Loss: 0.9185, Train: 0.9297, Val: 0.9697, Test: 0.9530\n",
      "Epoch: 692, Loss: 0.9184, Train: 0.9296, Val: 0.9698, Test: 0.9529\n",
      "Epoch: 693, Loss: 0.9184, Train: 0.9295, Val: 0.9698, Test: 0.9531\n",
      "Epoch: 694, Loss: 0.9183, Train: 0.9294, Val: 0.9700, Test: 0.9533\n",
      "Epoch: 695, Loss: 0.9183, Train: 0.9294, Val: 0.9698, Test: 0.9533\n",
      "Epoch: 696, Loss: 0.9182, Train: 0.9294, Val: 0.9698, Test: 0.9532\n",
      "Epoch: 697, Loss: 0.9181, Train: 0.9294, Val: 0.9696, Test: 0.9531\n",
      "Epoch: 698, Loss: 0.9181, Train: 0.9293, Val: 0.9695, Test: 0.9529\n",
      "Epoch: 699, Loss: 0.9180, Train: 0.9293, Val: 0.9695, Test: 0.9529\n",
      "Epoch: 700, Loss: 0.9180, Train: 0.9293, Val: 0.9692, Test: 0.9529\n",
      "Epoch: 701, Loss: 0.9179, Train: 0.9292, Val: 0.9692, Test: 0.9530\n",
      "Epoch: 702, Loss: 0.9179, Train: 0.9292, Val: 0.9692, Test: 0.9530\n",
      "Epoch: 703, Loss: 0.9178, Train: 0.9291, Val: 0.9691, Test: 0.9529\n",
      "Epoch: 704, Loss: 0.9178, Train: 0.9290, Val: 0.9689, Test: 0.9528\n",
      "Epoch: 705, Loss: 0.9177, Train: 0.9290, Val: 0.9688, Test: 0.9531\n",
      "Epoch: 706, Loss: 0.9176, Train: 0.9289, Val: 0.9689, Test: 0.9529\n",
      "Epoch: 707, Loss: 0.9176, Train: 0.9288, Val: 0.9692, Test: 0.9529\n",
      "Epoch: 708, Loss: 0.9175, Train: 0.9288, Val: 0.9690, Test: 0.9527\n",
      "Epoch: 709, Loss: 0.9175, Train: 0.9287, Val: 0.9694, Test: 0.9530\n",
      "Epoch: 710, Loss: 0.9174, Train: 0.9286, Val: 0.9693, Test: 0.9530\n",
      "Epoch: 711, Loss: 0.9174, Train: 0.9286, Val: 0.9693, Test: 0.9528\n",
      "Epoch: 712, Loss: 0.9173, Train: 0.9286, Val: 0.9693, Test: 0.9527\n",
      "Epoch: 713, Loss: 0.9173, Train: 0.9285, Val: 0.9693, Test: 0.9525\n",
      "Epoch: 714, Loss: 0.9172, Train: 0.9285, Val: 0.9693, Test: 0.9524\n",
      "Epoch: 715, Loss: 0.9172, Train: 0.9285, Val: 0.9693, Test: 0.9525\n",
      "Epoch: 716, Loss: 0.9171, Train: 0.9284, Val: 0.9693, Test: 0.9525\n",
      "Epoch: 717, Loss: 0.9170, Train: 0.9284, Val: 0.9692, Test: 0.9523\n",
      "Epoch: 718, Loss: 0.9170, Train: 0.9283, Val: 0.9689, Test: 0.9522\n",
      "Epoch: 719, Loss: 0.9169, Train: 0.9282, Val: 0.9689, Test: 0.9521\n",
      "Epoch: 720, Loss: 0.9169, Train: 0.9282, Val: 0.9686, Test: 0.9525\n",
      "Epoch: 721, Loss: 0.9168, Train: 0.9281, Val: 0.9686, Test: 0.9525\n",
      "Epoch: 722, Loss: 0.9168, Train: 0.9281, Val: 0.9685, Test: 0.9521\n",
      "Epoch: 723, Loss: 0.9167, Train: 0.9280, Val: 0.9684, Test: 0.9523\n",
      "Epoch: 724, Loss: 0.9167, Train: 0.9278, Val: 0.9682, Test: 0.9525\n",
      "Epoch: 725, Loss: 0.9166, Train: 0.9277, Val: 0.9684, Test: 0.9524\n",
      "Epoch: 726, Loss: 0.9165, Train: 0.9277, Val: 0.9683, Test: 0.9526\n",
      "Epoch: 727, Loss: 0.9165, Train: 0.9277, Val: 0.9682, Test: 0.9531\n",
      "Epoch: 728, Loss: 0.9164, Train: 0.9276, Val: 0.9679, Test: 0.9532\n",
      "Epoch: 729, Loss: 0.9164, Train: 0.9277, Val: 0.9677, Test: 0.9527\n",
      "Epoch: 730, Loss: 0.9163, Train: 0.9277, Val: 0.9677, Test: 0.9525\n",
      "Epoch: 731, Loss: 0.9162, Train: 0.9276, Val: 0.9674, Test: 0.9526\n",
      "Epoch: 732, Loss: 0.9162, Train: 0.9276, Val: 0.9676, Test: 0.9525\n",
      "Epoch: 733, Loss: 0.9161, Train: 0.9275, Val: 0.9672, Test: 0.9525\n",
      "Epoch: 734, Loss: 0.9160, Train: 0.9273, Val: 0.9671, Test: 0.9524\n",
      "Epoch: 735, Loss: 0.9160, Train: 0.9272, Val: 0.9671, Test: 0.9526\n",
      "Epoch: 736, Loss: 0.9159, Train: 0.9271, Val: 0.9670, Test: 0.9525\n",
      "Epoch: 737, Loss: 0.9158, Train: 0.9268, Val: 0.9671, Test: 0.9524\n",
      "Epoch: 738, Loss: 0.9158, Train: 0.9269, Val: 0.9671, Test: 0.9523\n",
      "Epoch: 739, Loss: 0.9157, Train: 0.9269, Val: 0.9670, Test: 0.9522\n",
      "Epoch: 740, Loss: 0.9156, Train: 0.9269, Val: 0.9672, Test: 0.9522\n",
      "Epoch: 741, Loss: 0.9156, Train: 0.9268, Val: 0.9671, Test: 0.9519\n",
      "Epoch: 742, Loss: 0.9155, Train: 0.9268, Val: 0.9672, Test: 0.9518\n",
      "Epoch: 743, Loss: 0.9154, Train: 0.9268, Val: 0.9675, Test: 0.9518\n",
      "Epoch: 744, Loss: 0.9154, Train: 0.9267, Val: 0.9673, Test: 0.9513\n",
      "Epoch: 745, Loss: 0.9153, Train: 0.9266, Val: 0.9671, Test: 0.9511\n",
      "Epoch: 746, Loss: 0.9152, Train: 0.9264, Val: 0.9672, Test: 0.9515\n",
      "Epoch: 747, Loss: 0.9152, Train: 0.9263, Val: 0.9669, Test: 0.9511\n",
      "Epoch: 748, Loss: 0.9151, Train: 0.9262, Val: 0.9668, Test: 0.9509\n",
      "Epoch: 749, Loss: 0.9151, Train: 0.9262, Val: 0.9669, Test: 0.9508\n",
      "Epoch: 750, Loss: 0.9150, Train: 0.9262, Val: 0.9668, Test: 0.9506\n",
      "Epoch: 751, Loss: 0.9150, Train: 0.9261, Val: 0.9669, Test: 0.9505\n",
      "Epoch: 752, Loss: 0.9149, Train: 0.9260, Val: 0.9668, Test: 0.9504\n",
      "Epoch: 753, Loss: 0.9148, Train: 0.9261, Val: 0.9668, Test: 0.9506\n",
      "Epoch: 754, Loss: 0.9148, Train: 0.9261, Val: 0.9670, Test: 0.9507\n",
      "Epoch: 755, Loss: 0.9147, Train: 0.9261, Val: 0.9670, Test: 0.9507\n",
      "Epoch: 756, Loss: 0.9146, Train: 0.9260, Val: 0.9672, Test: 0.9506\n",
      "Epoch: 757, Loss: 0.9146, Train: 0.9260, Val: 0.9672, Test: 0.9506\n",
      "Epoch: 758, Loss: 0.9145, Train: 0.9260, Val: 0.9672, Test: 0.9503\n",
      "Epoch: 759, Loss: 0.9145, Train: 0.9259, Val: 0.9674, Test: 0.9503\n",
      "Epoch: 760, Loss: 0.9144, Train: 0.9258, Val: 0.9671, Test: 0.9501\n",
      "Epoch: 761, Loss: 0.9143, Train: 0.9257, Val: 0.9673, Test: 0.9501\n",
      "Epoch: 762, Loss: 0.9143, Train: 0.9257, Val: 0.9674, Test: 0.9500\n",
      "Epoch: 763, Loss: 0.9142, Train: 0.9256, Val: 0.9672, Test: 0.9496\n",
      "Epoch: 764, Loss: 0.9142, Train: 0.9255, Val: 0.9671, Test: 0.9497\n",
      "Epoch: 765, Loss: 0.9141, Train: 0.9255, Val: 0.9669, Test: 0.9493\n",
      "Epoch: 766, Loss: 0.9141, Train: 0.9255, Val: 0.9666, Test: 0.9496\n",
      "Epoch: 767, Loss: 0.9140, Train: 0.9255, Val: 0.9665, Test: 0.9496\n",
      "Epoch: 768, Loss: 0.9140, Train: 0.9254, Val: 0.9663, Test: 0.9496\n",
      "Epoch: 769, Loss: 0.9139, Train: 0.9253, Val: 0.9662, Test: 0.9495\n",
      "Epoch: 770, Loss: 0.9138, Train: 0.9252, Val: 0.9661, Test: 0.9495\n",
      "Epoch: 771, Loss: 0.9138, Train: 0.9252, Val: 0.9659, Test: 0.9495\n",
      "Epoch: 772, Loss: 0.9137, Train: 0.9251, Val: 0.9661, Test: 0.9497\n",
      "Epoch: 773, Loss: 0.9137, Train: 0.9250, Val: 0.9662, Test: 0.9497\n",
      "Epoch: 774, Loss: 0.9136, Train: 0.9249, Val: 0.9663, Test: 0.9496\n",
      "Epoch: 775, Loss: 0.9136, Train: 0.9249, Val: 0.9662, Test: 0.9495\n",
      "Epoch: 776, Loss: 0.9135, Train: 0.9249, Val: 0.9661, Test: 0.9495\n",
      "Epoch: 777, Loss: 0.9135, Train: 0.9248, Val: 0.9663, Test: 0.9497\n",
      "Epoch: 778, Loss: 0.9134, Train: 0.9248, Val: 0.9661, Test: 0.9499\n",
      "Epoch: 779, Loss: 0.9134, Train: 0.9247, Val: 0.9662, Test: 0.9500\n",
      "Epoch: 780, Loss: 0.9133, Train: 0.9247, Val: 0.9662, Test: 0.9500\n",
      "Epoch: 781, Loss: 0.9132, Train: 0.9246, Val: 0.9660, Test: 0.9498\n",
      "Epoch: 782, Loss: 0.9132, Train: 0.9246, Val: 0.9661, Test: 0.9497\n",
      "Epoch: 783, Loss: 0.9131, Train: 0.9246, Val: 0.9662, Test: 0.9495\n",
      "Epoch: 784, Loss: 0.9131, Train: 0.9245, Val: 0.9662, Test: 0.9495\n",
      "Epoch: 785, Loss: 0.9130, Train: 0.9245, Val: 0.9663, Test: 0.9493\n",
      "Epoch: 786, Loss: 0.9130, Train: 0.9244, Val: 0.9660, Test: 0.9492\n",
      "Epoch: 787, Loss: 0.9129, Train: 0.9244, Val: 0.9662, Test: 0.9492\n",
      "Epoch: 788, Loss: 0.9129, Train: 0.9244, Val: 0.9663, Test: 0.9492\n",
      "Epoch: 789, Loss: 0.9128, Train: 0.9243, Val: 0.9660, Test: 0.9491\n",
      "Epoch: 790, Loss: 0.9128, Train: 0.9242, Val: 0.9661, Test: 0.9492\n",
      "Epoch: 791, Loss: 0.9127, Train: 0.9241, Val: 0.9660, Test: 0.9492\n",
      "Epoch: 792, Loss: 0.9126, Train: 0.9240, Val: 0.9660, Test: 0.9494\n",
      "Epoch: 793, Loss: 0.9126, Train: 0.9241, Val: 0.9659, Test: 0.9493\n",
      "Epoch: 794, Loss: 0.9125, Train: 0.9240, Val: 0.9662, Test: 0.9494\n",
      "Epoch: 795, Loss: 0.9125, Train: 0.9239, Val: 0.9664, Test: 0.9492\n",
      "Epoch: 796, Loss: 0.9124, Train: 0.9237, Val: 0.9661, Test: 0.9491\n",
      "Epoch: 797, Loss: 0.9123, Train: 0.9237, Val: 0.9662, Test: 0.9490\n",
      "Epoch: 798, Loss: 0.9123, Train: 0.9237, Val: 0.9661, Test: 0.9491\n",
      "Epoch: 799, Loss: 0.9122, Train: 0.9236, Val: 0.9663, Test: 0.9495\n",
      "Epoch: 800, Loss: 0.9122, Train: 0.9235, Val: 0.9663, Test: 0.9495\n",
      "Epoch: 801, Loss: 0.9121, Train: 0.9235, Val: 0.9662, Test: 0.9496\n",
      "Epoch: 802, Loss: 0.9121, Train: 0.9235, Val: 0.9664, Test: 0.9489\n",
      "Epoch: 803, Loss: 0.9120, Train: 0.9235, Val: 0.9667, Test: 0.9487\n",
      "Epoch: 804, Loss: 0.9120, Train: 0.9234, Val: 0.9663, Test: 0.9490\n",
      "Epoch: 805, Loss: 0.9119, Train: 0.9234, Val: 0.9662, Test: 0.9487\n",
      "Epoch: 806, Loss: 0.9119, Train: 0.9233, Val: 0.9662, Test: 0.9488\n",
      "Epoch: 807, Loss: 0.9118, Train: 0.9233, Val: 0.9660, Test: 0.9489\n",
      "Epoch: 808, Loss: 0.9118, Train: 0.9232, Val: 0.9662, Test: 0.9487\n",
      "Epoch: 809, Loss: 0.9117, Train: 0.9231, Val: 0.9663, Test: 0.9488\n",
      "Epoch: 810, Loss: 0.9116, Train: 0.9231, Val: 0.9661, Test: 0.9489\n",
      "Epoch: 811, Loss: 0.9116, Train: 0.9231, Val: 0.9660, Test: 0.9489\n",
      "Epoch: 812, Loss: 0.9115, Train: 0.9231, Val: 0.9657, Test: 0.9487\n",
      "Epoch: 813, Loss: 0.9115, Train: 0.9231, Val: 0.9655, Test: 0.9485\n",
      "Epoch: 814, Loss: 0.9114, Train: 0.9231, Val: 0.9655, Test: 0.9485\n",
      "Epoch: 815, Loss: 0.9114, Train: 0.9230, Val: 0.9655, Test: 0.9482\n",
      "Epoch: 816, Loss: 0.9113, Train: 0.9230, Val: 0.9655, Test: 0.9484\n",
      "Epoch: 817, Loss: 0.9113, Train: 0.9229, Val: 0.9656, Test: 0.9484\n",
      "Epoch: 818, Loss: 0.9112, Train: 0.9229, Val: 0.9656, Test: 0.9485\n",
      "Epoch: 819, Loss: 0.9112, Train: 0.9229, Val: 0.9654, Test: 0.9486\n",
      "Epoch: 820, Loss: 0.9111, Train: 0.9227, Val: 0.9651, Test: 0.9486\n",
      "Epoch: 821, Loss: 0.9111, Train: 0.9226, Val: 0.9652, Test: 0.9485\n",
      "Epoch: 822, Loss: 0.9110, Train: 0.9225, Val: 0.9653, Test: 0.9486\n",
      "Epoch: 823, Loss: 0.9110, Train: 0.9225, Val: 0.9654, Test: 0.9486\n",
      "Epoch: 824, Loss: 0.9109, Train: 0.9225, Val: 0.9652, Test: 0.9485\n",
      "Epoch: 825, Loss: 0.9109, Train: 0.9225, Val: 0.9651, Test: 0.9487\n",
      "Epoch: 826, Loss: 0.9108, Train: 0.9225, Val: 0.9651, Test: 0.9486\n",
      "Epoch: 827, Loss: 0.9108, Train: 0.9224, Val: 0.9652, Test: 0.9484\n",
      "Epoch: 828, Loss: 0.9107, Train: 0.9224, Val: 0.9653, Test: 0.9483\n",
      "Epoch: 829, Loss: 0.9107, Train: 0.9224, Val: 0.9652, Test: 0.9484\n",
      "Epoch: 830, Loss: 0.9106, Train: 0.9224, Val: 0.9651, Test: 0.9484\n",
      "Epoch: 831, Loss: 0.9106, Train: 0.9223, Val: 0.9651, Test: 0.9485\n",
      "Epoch: 832, Loss: 0.9105, Train: 0.9223, Val: 0.9654, Test: 0.9484\n",
      "Epoch: 833, Loss: 0.9105, Train: 0.9223, Val: 0.9655, Test: 0.9486\n",
      "Epoch: 834, Loss: 0.9104, Train: 0.9222, Val: 0.9656, Test: 0.9486\n",
      "Epoch: 835, Loss: 0.9104, Train: 0.9222, Val: 0.9655, Test: 0.9488\n",
      "Epoch: 836, Loss: 0.9103, Train: 0.9222, Val: 0.9655, Test: 0.9488\n",
      "Epoch: 837, Loss: 0.9102, Train: 0.9221, Val: 0.9658, Test: 0.9488\n",
      "Epoch: 838, Loss: 0.9102, Train: 0.9221, Val: 0.9658, Test: 0.9486\n",
      "Epoch: 839, Loss: 0.9101, Train: 0.9219, Val: 0.9657, Test: 0.9486\n",
      "Epoch: 840, Loss: 0.9101, Train: 0.9219, Val: 0.9658, Test: 0.9488\n",
      "Epoch: 841, Loss: 0.9100, Train: 0.9218, Val: 0.9657, Test: 0.9487\n",
      "Epoch: 842, Loss: 0.9100, Train: 0.9217, Val: 0.9656, Test: 0.9486\n",
      "Epoch: 843, Loss: 0.9099, Train: 0.9216, Val: 0.9657, Test: 0.9487\n",
      "Epoch: 844, Loss: 0.9099, Train: 0.9216, Val: 0.9659, Test: 0.9484\n",
      "Epoch: 845, Loss: 0.9098, Train: 0.9216, Val: 0.9659, Test: 0.9485\n",
      "Epoch: 846, Loss: 0.9098, Train: 0.9215, Val: 0.9659, Test: 0.9484\n",
      "Epoch: 847, Loss: 0.9097, Train: 0.9214, Val: 0.9658, Test: 0.9487\n",
      "Epoch: 848, Loss: 0.9097, Train: 0.9214, Val: 0.9660, Test: 0.9487\n",
      "Epoch: 849, Loss: 0.9096, Train: 0.9214, Val: 0.9658, Test: 0.9487\n",
      "Epoch: 850, Loss: 0.9096, Train: 0.9213, Val: 0.9659, Test: 0.9487\n",
      "Epoch: 851, Loss: 0.9095, Train: 0.9214, Val: 0.9660, Test: 0.9487\n",
      "Epoch: 852, Loss: 0.9095, Train: 0.9213, Val: 0.9659, Test: 0.9488\n",
      "Epoch: 853, Loss: 0.9094, Train: 0.9213, Val: 0.9657, Test: 0.9487\n",
      "Epoch: 854, Loss: 0.9094, Train: 0.9212, Val: 0.9656, Test: 0.9488\n",
      "Epoch: 855, Loss: 0.9093, Train: 0.9212, Val: 0.9654, Test: 0.9489\n",
      "Epoch: 856, Loss: 0.9093, Train: 0.9212, Val: 0.9655, Test: 0.9491\n",
      "Epoch: 857, Loss: 0.9092, Train: 0.9211, Val: 0.9653, Test: 0.9492\n",
      "Epoch: 858, Loss: 0.9092, Train: 0.9211, Val: 0.9654, Test: 0.9492\n",
      "Epoch: 859, Loss: 0.9091, Train: 0.9212, Val: 0.9653, Test: 0.9491\n",
      "Epoch: 860, Loss: 0.9091, Train: 0.9211, Val: 0.9651, Test: 0.9492\n",
      "Epoch: 861, Loss: 0.9090, Train: 0.9211, Val: 0.9650, Test: 0.9490\n",
      "Epoch: 862, Loss: 0.9090, Train: 0.9211, Val: 0.9650, Test: 0.9491\n",
      "Epoch: 863, Loss: 0.9089, Train: 0.9210, Val: 0.9651, Test: 0.9490\n",
      "Epoch: 864, Loss: 0.9089, Train: 0.9209, Val: 0.9651, Test: 0.9489\n",
      "Epoch: 865, Loss: 0.9088, Train: 0.9209, Val: 0.9651, Test: 0.9488\n",
      "Epoch: 866, Loss: 0.9088, Train: 0.9209, Val: 0.9650, Test: 0.9488\n",
      "Epoch: 867, Loss: 0.9087, Train: 0.9209, Val: 0.9651, Test: 0.9487\n",
      "Epoch: 868, Loss: 0.9087, Train: 0.9208, Val: 0.9653, Test: 0.9488\n",
      "Epoch: 869, Loss: 0.9087, Train: 0.9207, Val: 0.9654, Test: 0.9490\n",
      "Epoch: 870, Loss: 0.9086, Train: 0.9207, Val: 0.9653, Test: 0.9488\n",
      "Epoch: 871, Loss: 0.9086, Train: 0.9206, Val: 0.9653, Test: 0.9490\n",
      "Epoch: 872, Loss: 0.9085, Train: 0.9205, Val: 0.9654, Test: 0.9491\n",
      "Epoch: 873, Loss: 0.9085, Train: 0.9205, Val: 0.9652, Test: 0.9490\n",
      "Epoch: 874, Loss: 0.9084, Train: 0.9204, Val: 0.9653, Test: 0.9491\n",
      "Epoch: 875, Loss: 0.9084, Train: 0.9203, Val: 0.9653, Test: 0.9491\n",
      "Epoch: 876, Loss: 0.9083, Train: 0.9202, Val: 0.9652, Test: 0.9491\n",
      "Epoch: 877, Loss: 0.9083, Train: 0.9202, Val: 0.9653, Test: 0.9490\n",
      "Epoch: 878, Loss: 0.9082, Train: 0.9202, Val: 0.9655, Test: 0.9491\n",
      "Epoch: 879, Loss: 0.9082, Train: 0.9201, Val: 0.9655, Test: 0.9490\n",
      "Epoch: 880, Loss: 0.9081, Train: 0.9200, Val: 0.9655, Test: 0.9489\n",
      "Epoch: 881, Loss: 0.9081, Train: 0.9199, Val: 0.9656, Test: 0.9489\n",
      "Epoch: 882, Loss: 0.9080, Train: 0.9199, Val: 0.9658, Test: 0.9490\n",
      "Epoch: 883, Loss: 0.9080, Train: 0.9199, Val: 0.9659, Test: 0.9490\n",
      "Epoch: 884, Loss: 0.9079, Train: 0.9198, Val: 0.9658, Test: 0.9489\n",
      "Epoch: 885, Loss: 0.9079, Train: 0.9199, Val: 0.9657, Test: 0.9488\n",
      "Epoch: 886, Loss: 0.9078, Train: 0.9198, Val: 0.9660, Test: 0.9488\n",
      "Epoch: 887, Loss: 0.9078, Train: 0.9198, Val: 0.9656, Test: 0.9488\n",
      "Epoch: 888, Loss: 0.9077, Train: 0.9197, Val: 0.9656, Test: 0.9488\n",
      "Epoch: 889, Loss: 0.9077, Train: 0.9197, Val: 0.9656, Test: 0.9488\n",
      "Epoch: 890, Loss: 0.9076, Train: 0.9197, Val: 0.9655, Test: 0.9488\n",
      "Epoch: 891, Loss: 0.9076, Train: 0.9196, Val: 0.9654, Test: 0.9486\n",
      "Epoch: 892, Loss: 0.9075, Train: 0.9196, Val: 0.9654, Test: 0.9484\n",
      "Epoch: 893, Loss: 0.9075, Train: 0.9196, Val: 0.9655, Test: 0.9485\n",
      "Epoch: 894, Loss: 0.9074, Train: 0.9196, Val: 0.9653, Test: 0.9484\n",
      "Epoch: 895, Loss: 0.9074, Train: 0.9196, Val: 0.9652, Test: 0.9482\n",
      "Epoch: 896, Loss: 0.9073, Train: 0.9195, Val: 0.9652, Test: 0.9481\n",
      "Epoch: 897, Loss: 0.9073, Train: 0.9195, Val: 0.9653, Test: 0.9481\n",
      "Epoch: 898, Loss: 0.9072, Train: 0.9194, Val: 0.9655, Test: 0.9479\n",
      "Epoch: 899, Loss: 0.9072, Train: 0.9195, Val: 0.9655, Test: 0.9480\n",
      "Epoch: 900, Loss: 0.9072, Train: 0.9194, Val: 0.9656, Test: 0.9480\n",
      "Epoch: 901, Loss: 0.9071, Train: 0.9194, Val: 0.9657, Test: 0.9480\n",
      "Epoch: 902, Loss: 0.9071, Train: 0.9193, Val: 0.9657, Test: 0.9479\n",
      "Epoch: 903, Loss: 0.9070, Train: 0.9193, Val: 0.9656, Test: 0.9479\n",
      "Epoch: 904, Loss: 0.9070, Train: 0.9192, Val: 0.9659, Test: 0.9479\n",
      "Epoch: 905, Loss: 0.9069, Train: 0.9192, Val: 0.9657, Test: 0.9478\n",
      "Epoch: 906, Loss: 0.9069, Train: 0.9192, Val: 0.9656, Test: 0.9477\n",
      "Epoch: 907, Loss: 0.9068, Train: 0.9192, Val: 0.9657, Test: 0.9477\n",
      "Epoch: 908, Loss: 0.9068, Train: 0.9191, Val: 0.9655, Test: 0.9477\n",
      "Epoch: 909, Loss: 0.9067, Train: 0.9191, Val: 0.9654, Test: 0.9476\n",
      "Epoch: 910, Loss: 0.9067, Train: 0.9191, Val: 0.9651, Test: 0.9478\n",
      "Epoch: 911, Loss: 0.9066, Train: 0.9190, Val: 0.9651, Test: 0.9481\n",
      "Epoch: 912, Loss: 0.9066, Train: 0.9190, Val: 0.9651, Test: 0.9481\n",
      "Epoch: 913, Loss: 0.9065, Train: 0.9190, Val: 0.9651, Test: 0.9481\n",
      "Epoch: 914, Loss: 0.9065, Train: 0.9190, Val: 0.9651, Test: 0.9483\n",
      "Epoch: 915, Loss: 0.9064, Train: 0.9190, Val: 0.9650, Test: 0.9484\n",
      "Epoch: 916, Loss: 0.9064, Train: 0.9189, Val: 0.9650, Test: 0.9484\n",
      "Epoch: 917, Loss: 0.9063, Train: 0.9188, Val: 0.9652, Test: 0.9483\n",
      "Epoch: 918, Loss: 0.9063, Train: 0.9188, Val: 0.9652, Test: 0.9483\n",
      "Epoch: 919, Loss: 0.9063, Train: 0.9187, Val: 0.9652, Test: 0.9483\n",
      "Epoch: 920, Loss: 0.9062, Train: 0.9187, Val: 0.9652, Test: 0.9483\n",
      "Epoch: 921, Loss: 0.9062, Train: 0.9187, Val: 0.9652, Test: 0.9483\n",
      "Epoch: 922, Loss: 0.9061, Train: 0.9187, Val: 0.9650, Test: 0.9482\n",
      "Epoch: 923, Loss: 0.9061, Train: 0.9188, Val: 0.9650, Test: 0.9483\n",
      "Epoch: 924, Loss: 0.9060, Train: 0.9186, Val: 0.9649, Test: 0.9484\n",
      "Epoch: 925, Loss: 0.9060, Train: 0.9186, Val: 0.9650, Test: 0.9484\n",
      "Epoch: 926, Loss: 0.9059, Train: 0.9185, Val: 0.9650, Test: 0.9486\n",
      "Epoch: 927, Loss: 0.9059, Train: 0.9185, Val: 0.9649, Test: 0.9485\n",
      "Epoch: 928, Loss: 0.9058, Train: 0.9185, Val: 0.9650, Test: 0.9485\n",
      "Epoch: 929, Loss: 0.9058, Train: 0.9184, Val: 0.9650, Test: 0.9486\n",
      "Epoch: 930, Loss: 0.9057, Train: 0.9184, Val: 0.9652, Test: 0.9486\n",
      "Epoch: 931, Loss: 0.9057, Train: 0.9184, Val: 0.9653, Test: 0.9485\n",
      "Epoch: 932, Loss: 0.9056, Train: 0.9183, Val: 0.9653, Test: 0.9486\n",
      "Epoch: 933, Loss: 0.9056, Train: 0.9184, Val: 0.9653, Test: 0.9486\n",
      "Epoch: 934, Loss: 0.9056, Train: 0.9183, Val: 0.9655, Test: 0.9485\n",
      "Epoch: 935, Loss: 0.9055, Train: 0.9182, Val: 0.9654, Test: 0.9486\n",
      "Epoch: 936, Loss: 0.9055, Train: 0.9181, Val: 0.9655, Test: 0.9484\n",
      "Epoch: 937, Loss: 0.9054, Train: 0.9180, Val: 0.9656, Test: 0.9484\n",
      "Epoch: 938, Loss: 0.9054, Train: 0.9180, Val: 0.9654, Test: 0.9483\n",
      "Epoch: 939, Loss: 0.9053, Train: 0.9179, Val: 0.9654, Test: 0.9486\n",
      "Epoch: 940, Loss: 0.9053, Train: 0.9179, Val: 0.9654, Test: 0.9485\n",
      "Epoch: 941, Loss: 0.9052, Train: 0.9178, Val: 0.9655, Test: 0.9484\n",
      "Epoch: 942, Loss: 0.9052, Train: 0.9178, Val: 0.9655, Test: 0.9484\n",
      "Epoch: 943, Loss: 0.9051, Train: 0.9177, Val: 0.9655, Test: 0.9484\n",
      "Epoch: 944, Loss: 0.9051, Train: 0.9176, Val: 0.9654, Test: 0.9484\n",
      "Epoch: 945, Loss: 0.9050, Train: 0.9176, Val: 0.9654, Test: 0.9485\n",
      "Epoch: 946, Loss: 0.9050, Train: 0.9176, Val: 0.9654, Test: 0.9484\n",
      "Epoch: 947, Loss: 0.9050, Train: 0.9174, Val: 0.9655, Test: 0.9483\n",
      "Epoch: 948, Loss: 0.9049, Train: 0.9173, Val: 0.9654, Test: 0.9480\n",
      "Epoch: 949, Loss: 0.9049, Train: 0.9172, Val: 0.9654, Test: 0.9479\n",
      "Epoch: 950, Loss: 0.9048, Train: 0.9172, Val: 0.9654, Test: 0.9480\n",
      "Epoch: 951, Loss: 0.9048, Train: 0.9172, Val: 0.9654, Test: 0.9483\n",
      "Epoch: 952, Loss: 0.9047, Train: 0.9171, Val: 0.9654, Test: 0.9483\n",
      "Epoch: 953, Loss: 0.9047, Train: 0.9171, Val: 0.9655, Test: 0.9482\n",
      "Epoch: 954, Loss: 0.9046, Train: 0.9170, Val: 0.9653, Test: 0.9484\n",
      "Epoch: 955, Loss: 0.9046, Train: 0.9170, Val: 0.9655, Test: 0.9482\n",
      "Epoch: 956, Loss: 0.9045, Train: 0.9170, Val: 0.9655, Test: 0.9484\n",
      "Epoch: 957, Loss: 0.9045, Train: 0.9170, Val: 0.9655, Test: 0.9486\n",
      "Epoch: 958, Loss: 0.9045, Train: 0.9170, Val: 0.9655, Test: 0.9486\n",
      "Epoch: 959, Loss: 0.9044, Train: 0.9169, Val: 0.9655, Test: 0.9487\n",
      "Epoch: 960, Loss: 0.9044, Train: 0.9168, Val: 0.9654, Test: 0.9488\n",
      "Epoch: 961, Loss: 0.9043, Train: 0.9167, Val: 0.9653, Test: 0.9486\n",
      "Epoch: 962, Loss: 0.9043, Train: 0.9167, Val: 0.9654, Test: 0.9486\n",
      "Epoch: 963, Loss: 0.9042, Train: 0.9166, Val: 0.9654, Test: 0.9486\n",
      "Epoch: 964, Loss: 0.9042, Train: 0.9166, Val: 0.9654, Test: 0.9486\n",
      "Epoch: 965, Loss: 0.9041, Train: 0.9165, Val: 0.9653, Test: 0.9487\n",
      "Epoch: 966, Loss: 0.9041, Train: 0.9164, Val: 0.9653, Test: 0.9487\n",
      "Epoch: 967, Loss: 0.9040, Train: 0.9164, Val: 0.9653, Test: 0.9486\n",
      "Epoch: 968, Loss: 0.9040, Train: 0.9163, Val: 0.9655, Test: 0.9489\n",
      "Epoch: 969, Loss: 0.9040, Train: 0.9163, Val: 0.9655, Test: 0.9489\n",
      "Epoch: 970, Loss: 0.9039, Train: 0.9163, Val: 0.9655, Test: 0.9489\n",
      "Epoch: 971, Loss: 0.9039, Train: 0.9162, Val: 0.9657, Test: 0.9485\n",
      "Epoch: 972, Loss: 0.9038, Train: 0.9161, Val: 0.9657, Test: 0.9488\n",
      "Epoch: 973, Loss: 0.9038, Train: 0.9160, Val: 0.9658, Test: 0.9488\n",
      "Epoch: 974, Loss: 0.9037, Train: 0.9159, Val: 0.9656, Test: 0.9488\n",
      "Epoch: 975, Loss: 0.9037, Train: 0.9159, Val: 0.9657, Test: 0.9487\n",
      "Epoch: 976, Loss: 0.9036, Train: 0.9158, Val: 0.9655, Test: 0.9491\n",
      "Epoch: 977, Loss: 0.9036, Train: 0.9158, Val: 0.9656, Test: 0.9491\n",
      "Epoch: 978, Loss: 0.9035, Train: 0.9157, Val: 0.9657, Test: 0.9491\n",
      "Epoch: 979, Loss: 0.9035, Train: 0.9157, Val: 0.9657, Test: 0.9490\n",
      "Epoch: 980, Loss: 0.9035, Train: 0.9157, Val: 0.9658, Test: 0.9491\n",
      "Epoch: 981, Loss: 0.9034, Train: 0.9157, Val: 0.9656, Test: 0.9493\n",
      "Epoch: 982, Loss: 0.9034, Train: 0.9156, Val: 0.9655, Test: 0.9493\n",
      "Epoch: 983, Loss: 0.9033, Train: 0.9156, Val: 0.9654, Test: 0.9492\n",
      "Epoch: 984, Loss: 0.9033, Train: 0.9155, Val: 0.9655, Test: 0.9492\n",
      "Epoch: 985, Loss: 0.9032, Train: 0.9153, Val: 0.9656, Test: 0.9492\n",
      "Epoch: 986, Loss: 0.9032, Train: 0.9153, Val: 0.9655, Test: 0.9492\n",
      "Epoch: 987, Loss: 0.9031, Train: 0.9153, Val: 0.9654, Test: 0.9491\n",
      "Epoch: 988, Loss: 0.9031, Train: 0.9153, Val: 0.9650, Test: 0.9493\n",
      "Epoch: 989, Loss: 0.9030, Train: 0.9152, Val: 0.9649, Test: 0.9490\n",
      "Epoch: 990, Loss: 0.9030, Train: 0.9152, Val: 0.9651, Test: 0.9489\n",
      "Epoch: 991, Loss: 0.9030, Train: 0.9151, Val: 0.9650, Test: 0.9492\n",
      "Epoch: 992, Loss: 0.9029, Train: 0.9150, Val: 0.9649, Test: 0.9492\n",
      "Epoch: 993, Loss: 0.9029, Train: 0.9149, Val: 0.9649, Test: 0.9493\n",
      "Epoch: 994, Loss: 0.9028, Train: 0.9148, Val: 0.9646, Test: 0.9489\n",
      "Epoch: 995, Loss: 0.9028, Train: 0.9149, Val: 0.9646, Test: 0.9488\n",
      "Epoch: 996, Loss: 0.9027, Train: 0.9148, Val: 0.9645, Test: 0.9488\n",
      "Epoch: 997, Loss: 0.9027, Train: 0.9147, Val: 0.9645, Test: 0.9490\n",
      "Epoch: 998, Loss: 0.9026, Train: 0.9146, Val: 0.9644, Test: 0.9489\n",
      "Epoch: 999, Loss: 0.9026, Train: 0.9145, Val: 0.9646, Test: 0.9491\n",
      "Epoch: 1000, Loss: 0.9026, Train: 0.9145, Val: 0.9647, Test: 0.9491\n"
     ]
    }
   ],
   "source": [
    "model = Model(layer_name=\"SAGE\", hidden_channels=16, data=data, encoder_num_layers=2,\n",
    "              decoder_num_layers=10, encoder_dropout=0.0, decoder_dropout=0.0, encoder_skip_connections=1)\n",
    "train_test(model, train_data=train_data, test_data=test_data,\n",
    "           val_data=val_data, logging_step=1, epochs=1000, use_weighted_loss=False, lr=0.012, use_rounding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(losses_, ylim=1.3, ylim_bottom=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_val(losses_, ylim=1.2, ylim_bottom=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acbc58aadc5672afc04cc91f2a1726d8eb7b999e15e50d024070fdc74729208f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
