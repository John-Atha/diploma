{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ioannisathanasiou/diploma/environ/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import argparse\n",
    "parent_path = pathlib.Path(os.getcwd()).parent.absolute()\n",
    "sys.path.append(str(parent_path))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import MovieLens\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.loader import NeighborLoader, LinkNeighborLoader\n",
    "\n",
    "from utils.Neo4jMovieLensMetaData import Neo4jMovieLensMetaData\n",
    "# from utils.gnn_simple import Model\n",
    "from utils.train_test import train_test\n",
    "from utils.visualize import plot_loss, plot_train, plot_val, plot_test, plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the corresponsing csv, store the dataset to the DB, preprocess it, and get it as a pytorch graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies have features...\n",
      "Encoding title...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 565/565 [00:15<00:00, 35.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([18062, 64])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "path = osp.join(osp.dirname(osp.abspath('')), '../../data/MovieLensNeo4j')\n",
    "dataset = Neo4jMovieLensMetaData(\n",
    "    path,\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    database_url=\"bolt://localhost:7687\",\n",
    "    database_username=\"neo4j\",\n",
    "    database_password=\"admin\",\n",
    "    force_pre_process=True,\n",
    "    force_db_restore=False,\n",
    "    text_features=[\"title\"],\n",
    "    list_features=[],\n",
    "    fastRP_features=[],\n",
    "    numeric_features=[],\n",
    ")\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user node features for message passing:\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes\n",
    "\n",
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label.\n",
    "\n",
    "# Perform a link-level split into training, validation, and test edges:\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,  ..., 16235, 16235, 16235],\n",
       "        [ 1045,  5556,   813,  ...,  3238,  1187,  1056]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"user\", \"rates\", \"movie\"].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    # Sample ALL neighbors for each node and each edge type for 2 iterations:\n",
    "    num_neighbors=[-1],\n",
    "    # Use a batch size of 128 for sampling training nodes of type \"paper\":\n",
    "    batch_size=256,\n",
    "    edge_label_index = ((\"user\", \"movie\"), None),\n",
    "    edge_label = data['user', 'movie'].edge_label,\n",
    ")\n",
    "train_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mmovie\u001b[0m={ x=[5609, 64] },\n",
       "  \u001b[1muser\u001b[0m={ x=[16141, 16236] },\n",
       "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
       "    edge_index=[2, 384729],\n",
       "    edge_label=[256],\n",
       "    edge_label_index=[2, 256]\n",
       "  },\n",
       "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 40685] }\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2660790])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['user', 'rates', 'movie'].edge_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2660790])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0,data[\"user\", \"rates\", \"movie\"].edge_index.shape[1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and train-test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dict_test_lala = 0\n",
    "edge_label_index_test_lala = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, LazyLinear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GATv2Conv, GCNConv, TransformerConv, GraphConv, GINConv, GINEConv, to_hetero, HeteroLinear, HeteroConv\n",
    "from torch_geometric.nn.models import GIN, GraphSAGE\n",
    "from torch_geometric.nn.aggr import MultiAggregation\n",
    "from typing import Union\n",
    "from torch_geometric.typing import Adj, OptPairTensor, Size\n",
    "\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        global z_dict_test_lala, edge_label_index_test_lala\n",
    "        row, col = edge_label_index\n",
    "        z_dict_test_lala = z_dict\n",
    "        edge_label_index_test_lala = edge_label_index\n",
    "        movie = z_dict['movie'][col]\n",
    "        user = z_dict['user'][row]\n",
    "        z = torch.cat([user, movie], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_channels=16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.012)\n",
    "\n",
    "weight = torch.bincount(train_data['user', 'movie'].edge_label)\n",
    "# weight = torch.bincount(train_data['user', 'rates', 'movie'].edge_label)\n",
    "weight = weight.max() / weight\n",
    "\n",
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    # weight = 1. if weight is None else weight[target].to(pred.dtype)\n",
    "    weight = 1. # if weight is None else weight[target].to(pred.dtype)\n",
    "    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch, log=False):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # pred = model(batch.x_dict, batch.edge_index_dict, batch['user', 'rates', 'movie'].edge_label_index)\n",
    "    pred = model(batch.x_dict, batch.edge_index_dict, batch['user', 'movie'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = batch['user', 'movie'].edge_label.float()\n",
    "    # target = batch['user', 'rates', 'movie'].edge_label\n",
    "    # print(\"pred:\", pred)\n",
    "    # print(\"target:\", target)\n",
    "    loss = weighted_mse_loss(pred, target, weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data, log=False):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict, data['user', 'movie'].edge_label_index)\n",
    "    # pred = model(data.x_dict, data.edge_index_dict, data['user', 'rates', 'movie'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = data['user', 'movie'].edge_label.float()\n",
    "    # target = data['user', 'rates', 'movie'].edge_label.float()\n",
    "\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch index: 0\n",
      "Epoch: 001, Loss: 13.7034, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 1\n",
      "Epoch: 001, Loss: 14.5806, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 2\n",
      "Epoch: 001, Loss: 12.2936, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 3\n",
      "Epoch: 001, Loss: 12.1377, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 4\n",
      "Epoch: 001, Loss: 13.5119, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 5\n",
      "Epoch: 001, Loss: 13.0411, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 6\n",
      "Epoch: 001, Loss: 12.1706, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 7\n",
      "Epoch: 001, Loss: 11.8627, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 8\n",
      "Epoch: 001, Loss: 10.2295, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 9\n",
      "Epoch: 001, Loss: 8.8391, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 10\n",
      "Epoch: 001, Loss: 8.9862, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 11\n",
      "Epoch: 001, Loss: 6.8138, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 12\n",
      "Epoch: 001, Loss: 5.0843, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 13\n",
      "Epoch: 001, Loss: 3.0161, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 14\n",
      "Epoch: 001, Loss: 1.7100, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 15\n",
      "Epoch: 001, Loss: 1.2290, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 16\n",
      "Epoch: 001, Loss: 2.1454, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 17\n",
      "Epoch: 001, Loss: 3.2118, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 18\n",
      "Epoch: 001, Loss: 4.0845, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 19\n",
      "Epoch: 001, Loss: 2.9706, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 20\n",
      "Epoch: 001, Loss: 3.3750, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 21\n",
      "Epoch: 001, Loss: 3.4440, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 22\n",
      "Epoch: 001, Loss: 1.9453, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 23\n",
      "Epoch: 001, Loss: 3.4805, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 24\n",
      "Epoch: 001, Loss: 3.6836, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 25\n",
      "Epoch: 001, Loss: 2.9062, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 26\n",
      "Epoch: 001, Loss: 2.8750, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 27\n",
      "Epoch: 001, Loss: 2.2070, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 28\n",
      "Epoch: 001, Loss: 1.5039, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 29\n",
      "Epoch: 001, Loss: 3.7539, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 30\n",
      "Epoch: 001, Loss: 2.9531, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 31\n",
      "Epoch: 001, Loss: 4.0625, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 32\n",
      "Epoch: 001, Loss: 2.2344, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 33\n",
      "Epoch: 001, Loss: 3.0977, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 34\n",
      "Epoch: 001, Loss: 2.3359, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 35\n",
      "Epoch: 001, Loss: 3.5664, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 36\n",
      "Epoch: 001, Loss: 3.3047, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 37\n",
      "Epoch: 001, Loss: 3.9883, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 38\n",
      "Epoch: 001, Loss: 2.4688, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 39\n",
      "Epoch: 001, Loss: 2.5273, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 40\n",
      "Epoch: 001, Loss: 4.6797, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 41\n",
      "Epoch: 001, Loss: 2.4805, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 42\n",
      "Epoch: 001, Loss: 2.8242, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 43\n",
      "Epoch: 001, Loss: 2.8672, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 44\n",
      "Epoch: 001, Loss: 4.5039, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 45\n",
      "Epoch: 001, Loss: 2.5391, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 46\n",
      "Epoch: 001, Loss: 2.7852, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 47\n",
      "Epoch: 001, Loss: 2.9297, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 48\n",
      "Epoch: 001, Loss: 3.9023, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 49\n",
      "Epoch: 001, Loss: 3.5664, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 50\n",
      "Epoch: 001, Loss: 3.6211, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 51\n",
      "Epoch: 001, Loss: 3.0781, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 52\n",
      "Epoch: 001, Loss: 3.9609, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 53\n",
      "Epoch: 001, Loss: 2.6367, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 54\n",
      "Epoch: 001, Loss: 2.3281, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 55\n",
      "Epoch: 001, Loss: 2.4297, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 56\n",
      "Epoch: 001, Loss: 2.3516, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 57\n",
      "Epoch: 001, Loss: 3.4805, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 58\n",
      "Epoch: 001, Loss: 2.4609, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 59\n",
      "Epoch: 001, Loss: 2.4492, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 60\n",
      "Epoch: 001, Loss: 2.9609, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 61\n",
      "Epoch: 001, Loss: 1.8008, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 62\n",
      "Epoch: 001, Loss: 2.8828, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 63\n",
      "Epoch: 001, Loss: 2.6484, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 64\n",
      "Epoch: 001, Loss: 3.3281, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 65\n",
      "Epoch: 001, Loss: 2.8164, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 66\n",
      "Epoch: 001, Loss: 2.6992, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 67\n",
      "Epoch: 001, Loss: 2.4961, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 68\n",
      "Epoch: 001, Loss: 2.1367, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 69\n",
      "Epoch: 001, Loss: 2.6289, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 70\n",
      "Epoch: 001, Loss: 1.5742, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 71\n",
      "Epoch: 001, Loss: 2.2344, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 72\n",
      "Epoch: 001, Loss: 2.8750, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 73\n",
      "Epoch: 001, Loss: 2.6797, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 74\n",
      "Epoch: 001, Loss: 3.7891, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 75\n",
      "Epoch: 001, Loss: 2.7539, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 76\n",
      "Epoch: 001, Loss: 2.8047, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 77\n",
      "Epoch: 001, Loss: 3.9023, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 78\n",
      "Epoch: 001, Loss: 4.0625, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 79\n",
      "Epoch: 001, Loss: 3.0703, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 80\n",
      "Epoch: 001, Loss: 3.3398, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 81\n",
      "Epoch: 001, Loss: 3.6523, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 82\n",
      "Epoch: 001, Loss: 2.2188, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 83\n",
      "Epoch: 001, Loss: 3.4688, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 84\n",
      "Epoch: 001, Loss: 4.1055, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 85\n",
      "Epoch: 001, Loss: 4.0352, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 86\n",
      "Epoch: 001, Loss: 4.6250, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 87\n",
      "Epoch: 001, Loss: 3.8203, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 88\n",
      "Epoch: 001, Loss: 3.1523, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 89\n",
      "Epoch: 001, Loss: 2.7266, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 90\n",
      "Epoch: 001, Loss: 2.9648, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 91\n",
      "Epoch: 001, Loss: 1.8594, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 92\n",
      "Epoch: 001, Loss: 2.9648, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 93\n",
      "Epoch: 001, Loss: 3.5234, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 94\n",
      "Epoch: 001, Loss: 3.5156, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 95\n",
      "Epoch: 001, Loss: 2.3789, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 96\n",
      "Epoch: 001, Loss: 3.2188, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 97\n",
      "Epoch: 001, Loss: 3.1914, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 98\n",
      "Epoch: 001, Loss: 4.3984, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 99\n",
      "Epoch: 001, Loss: 3.6172, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 100\n",
      "Epoch: 001, Loss: 2.1445, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 101\n",
      "Epoch: 001, Loss: 3.1953, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 102\n",
      "Epoch: 001, Loss: 3.7344, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 103\n",
      "Epoch: 001, Loss: 3.9258, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 104\n",
      "Epoch: 001, Loss: 3.8867, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 105\n",
      "Epoch: 001, Loss: 3.6055, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 106\n",
      "Epoch: 001, Loss: 2.0430, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 107\n",
      "Epoch: 001, Loss: 3.2930, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 108\n",
      "Epoch: 001, Loss: 2.4883, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 109\n",
      "Epoch: 001, Loss: 2.2383, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 110\n",
      "Epoch: 001, Loss: 3.3750, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 111\n",
      "Epoch: 001, Loss: 2.9102, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 112\n",
      "Epoch: 001, Loss: 2.0000, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 113\n",
      "Epoch: 001, Loss: 2.5117, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 114\n",
      "Epoch: 001, Loss: 2.7773, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 115\n",
      "Epoch: 001, Loss: 3.0156, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 116\n",
      "Epoch: 001, Loss: 4.0469, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 117\n",
      "Epoch: 001, Loss: 3.3867, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 118\n",
      "Epoch: 001, Loss: 3.3203, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 119\n",
      "Epoch: 001, Loss: 2.2461, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 120\n",
      "Epoch: 001, Loss: 2.8008, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 121\n",
      "Epoch: 001, Loss: 4.1484, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 122\n",
      "Epoch: 001, Loss: 2.1445, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 123\n",
      "Epoch: 001, Loss: 2.3086, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 124\n",
      "Epoch: 001, Loss: 3.5469, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 125\n",
      "Epoch: 001, Loss: 2.3945, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 126\n",
      "Epoch: 001, Loss: 2.7109, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 127\n",
      "Epoch: 001, Loss: 3.2148, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 128\n",
      "Epoch: 001, Loss: 6.6484, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 129\n",
      "Epoch: 001, Loss: 2.7070, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 130\n",
      "Epoch: 001, Loss: 1.3828, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 131\n",
      "Epoch: 001, Loss: 2.9961, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 132\n",
      "Epoch: 001, Loss: 3.0117, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 133\n",
      "Epoch: 001, Loss: 3.2148, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 134\n",
      "Epoch: 001, Loss: 1.9922, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 135\n",
      "Epoch: 001, Loss: 3.1562, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 136\n",
      "Epoch: 001, Loss: 1.6211, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 137\n",
      "Epoch: 001, Loss: 4.1094, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 138\n",
      "Epoch: 001, Loss: 2.4531, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 139\n",
      "Epoch: 001, Loss: 1.6523, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 140\n",
      "Epoch: 001, Loss: 1.9336, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 141\n",
      "Epoch: 001, Loss: 3.6211, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 142\n",
      "Epoch: 001, Loss: 4.6758, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 143\n",
      "Epoch: 001, Loss: 3.5508, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 144\n",
      "Epoch: 001, Loss: 2.8750, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 145\n",
      "Epoch: 001, Loss: 3.9375, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 146\n",
      "Epoch: 001, Loss: 4.6562, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 147\n",
      "Epoch: 001, Loss: 4.3516, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 148\n",
      "Epoch: 001, Loss: 0.9961, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 149\n",
      "Epoch: 001, Loss: 4.4570, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 150\n",
      "Epoch: 001, Loss: 3.3594, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 151\n",
      "Epoch: 001, Loss: 2.5977, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 152\n",
      "Epoch: 001, Loss: 2.3477, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 153\n",
      "Epoch: 001, Loss: 2.3906, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 154\n",
      "Epoch: 001, Loss: 2.7031, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 155\n",
      "Epoch: 001, Loss: 4.9570, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 156\n",
      "Epoch: 001, Loss: 7.1875, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 157\n",
      "Epoch: 001, Loss: 3.1445, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 158\n",
      "Epoch: 001, Loss: 2.7500, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 159\n",
      "Epoch: 001, Loss: 2.8398, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 160\n",
      "Epoch: 001, Loss: 1.9219, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 161\n",
      "Epoch: 001, Loss: 2.1445, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 162\n",
      "Epoch: 001, Loss: 4.1602, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 163\n",
      "Epoch: 001, Loss: 2.4180, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 164\n",
      "Epoch: 001, Loss: 1.1992, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 165\n",
      "Epoch: 001, Loss: 2.5156, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 166\n",
      "Epoch: 001, Loss: 3.4648, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 167\n",
      "Epoch: 001, Loss: 2.1797, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 168\n",
      "Epoch: 001, Loss: 3.5586, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 169\n",
      "Epoch: 001, Loss: 5.3789, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 170\n",
      "Epoch: 001, Loss: 3.8672, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 171\n",
      "Epoch: 001, Loss: 3.0625, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 172\n",
      "Epoch: 001, Loss: 3.0195, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 173\n",
      "Epoch: 001, Loss: 2.4727, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 174\n",
      "Epoch: 001, Loss: 3.0078, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 175\n",
      "Epoch: 001, Loss: 3.4375, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 176\n",
      "Epoch: 001, Loss: 3.7500, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 177\n",
      "Epoch: 001, Loss: 5.0391, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 178\n",
      "Epoch: 001, Loss: 3.9570, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 179\n",
      "Epoch: 001, Loss: 2.3906, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 180\n",
      "Epoch: 001, Loss: 4.4219, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 181\n",
      "Epoch: 001, Loss: 4.4531, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 182\n",
      "Epoch: 001, Loss: 2.9883, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 183\n",
      "Epoch: 001, Loss: 2.5508, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 184\n",
      "Epoch: 001, Loss: 2.4883, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 185\n",
      "Epoch: 001, Loss: 2.6680, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 186\n",
      "Epoch: 001, Loss: 4.4023, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 187\n",
      "Epoch: 001, Loss: 3.0742, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 188\n",
      "Epoch: 001, Loss: 3.4102, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 189\n",
      "Epoch: 001, Loss: 3.7930, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 190\n",
      "Epoch: 001, Loss: 3.9609, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 191\n",
      "Epoch: 001, Loss: 4.8594, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 192\n",
      "Epoch: 001, Loss: 5.6367, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 193\n",
      "Epoch: 001, Loss: 3.7305, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 194\n",
      "Epoch: 001, Loss: 4.2344, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 195\n",
      "Epoch: 001, Loss: 3.6562, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 196\n",
      "Epoch: 001, Loss: 4.5156, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 197\n",
      "Epoch: 001, Loss: 1.8594, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 198\n",
      "Epoch: 001, Loss: 1.5742, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 199\n",
      "Epoch: 001, Loss: 1.8281, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 200\n",
      "Epoch: 001, Loss: 2.2266, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 201\n",
      "Epoch: 001, Loss: 2.9141, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 202\n",
      "Epoch: 001, Loss: 2.3320, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 203\n",
      "Epoch: 001, Loss: 2.1133, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 204\n",
      "Epoch: 001, Loss: 2.1719, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 205\n",
      "Epoch: 001, Loss: 4.4336, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 206\n",
      "Epoch: 001, Loss: 3.1523, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 207\n",
      "Epoch: 001, Loss: 4.1094, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 208\n",
      "Epoch: 001, Loss: 2.1680, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 209\n",
      "Epoch: 001, Loss: 1.6328, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 210\n",
      "Epoch: 001, Loss: 1.7773, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 211\n",
      "Epoch: 001, Loss: 4.1875, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 212\n",
      "Epoch: 001, Loss: 2.9336, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 213\n",
      "Epoch: 001, Loss: 2.7500, Train: 0.0000, Val: 0.0000, Test: 0.0000\n",
      "Batch index: 214\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch index:\u001b[39m\u001b[39m\"\u001b[39m, batch_index)\n\u001b[1;32m      7\u001b[0m \u001b[39m# loss = 0.0\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m loss \u001b[39m=\u001b[39m train(batch, log\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m(epoch\u001b[39m%\u001b[39m\u001b[39m20\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[39m# train_rmse = test(batch)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# val_rmse = test(val_data)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# test_rmse = test(test_data, log=not(epoch%20))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# losses.append((loss, train_rmse, val_rmse, test_rmse))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m losses\u001b[39m.\u001b[39mappend((loss, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n",
      "Cell \u001b[0;32mIn [14], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(batch, log)\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      4\u001b[0m \u001b[39m# pred = model(batch.x_dict, batch.edge_index_dict, batch['user', 'rates', 'movie'].edge_label_index)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m pred \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39;49mx_dict, batch\u001b[39m.\u001b[39;49medge_index_dict, batch[\u001b[39m'\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmovie\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49medge_label_index)\n\u001b[1;32m      6\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m target \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmovie\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39medge_label\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [11], line 52\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_dict, edge_index_dict, edge_label_index)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_dict, edge_index_dict, edge_label_index):\n\u001b[0;32m---> 52\u001b[0m     z_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x_dict, edge_index_dict)\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z_dict, edge_label_index)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/fx/graph_module.py:652\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_wrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/fx/graph_module.py:267\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_call(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    266\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls, obj)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m     \u001b[39massert\u001b[39;00m e\u001b[39m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.1:11\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m      9\u001b[0m edge_index__user__rates__movie \u001b[39m=\u001b[39m edge_index_dict\u001b[39m.\u001b[39mget((\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrates\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmovie\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m edge_index__movie__rev_rates__user \u001b[39m=\u001b[39m edge_index_dict\u001b[39m.\u001b[39mget((\u001b[39m'\u001b[39m\u001b[39mmovie\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrev_rates\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m);  edge_index_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m conv1__movie \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1\u001b[39m.\u001b[39;49muser__rates__movie((x__user, x__movie), edge_index__user__rates__movie)\n\u001b[1;32m     12\u001b[0m conv1__user \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mmovie__rev_rates__user((x__movie, x__user), edge_index__movie__rev_rates__user);  x__movie \u001b[39m=\u001b[39m x__user \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m relu__movie \u001b[39m=\u001b[39m conv1__movie\u001b[39m.\u001b[39mrelu();  conv1__movie \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/conv/sage_conv.py:132\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size, conv_index)\u001b[0m\n\u001b[1;32m    129\u001b[0m     x \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mrelu(), x[\u001b[39m1\u001b[39m])\n\u001b[1;32m    131\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    133\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_l(out)\n\u001b[1;32m    135\u001b[0m x_r \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:392\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m         aggr_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 392\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate(out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maggr_kwargs)\n\u001b[1;32m    394\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aggregate_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    395\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:515\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maggregate\u001b[39m(\u001b[39mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[1;32m    503\u001b[0m               ptr: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    504\u001b[0m               dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    505\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m    :math:`\\square_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggr_module(inputs, index, ptr\u001b[39m=\u001b[39;49mptr, dim_size\u001b[39m=\u001b[39;49mdim_size,\n\u001b[1;32m    516\u001b[0m                             dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:114\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39melif\u001b[39;00m index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m dim_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()):\n\u001b[1;32m    110\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEncountered invalid \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdim_size\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdim_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m but expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>= \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(x, index, ptr, dim_size, dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/aggr/basic.py:34\u001b[0m, in \u001b[0;36mMeanAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m             ptr: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m             dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduce(x, index, ptr, dim_size, dim, reduce\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:153\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m segment_csr(x, ptr, reduce\u001b[39m=\u001b[39mreduce)\n\u001b[1;32m    152\u001b[0m \u001b[39massert\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m scatter(x, index, dim\u001b[39m=\u001b[39;49mdim, dim_size\u001b[39m=\u001b[39;49mdim_size, reduce\u001b[39m=\u001b[39;49mreduce)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:64\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatter\u001b[39m(src: Tensor, index: Tensor, dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     62\u001b[0m             out: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m             reduce: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_scatter\u001b[39m.\u001b[39;49mscatter(src, index, dim, out, dim_size, reduce)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_scatter/scatter.py:156\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter_mul(src, index, dim, out, dim_size)\n\u001b[1;32m    155\u001b[0m \u001b[39melif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter_mean(src, index, dim, out, dim_size)\n\u001b[1;32m    157\u001b[0m \u001b[39melif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter_min(src, index, dim, out, dim_size)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_scatter/scatter.py:41\u001b[0m, in \u001b[0;36mscatter_mean\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatter_mean\u001b[39m(src: torch\u001b[39m.\u001b[39mTensor, index: torch\u001b[39m.\u001b[39mTensor, dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     39\u001b[0m                  out: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m                  dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 41\u001b[0m     out \u001b[39m=\u001b[39m scatter_sum(src, index, dim, out, dim_size)\n\u001b[1;32m     42\u001b[0m     dim_size \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39msize(dim)\n\u001b[1;32m     44\u001b[0m     index_dim \u001b[39m=\u001b[39m dim\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_scatter/scatter.py:21\u001b[0m, in \u001b[0;36mscatter_sum\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m         size[dim] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(size, dtype\u001b[39m=\u001b[39msrc\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39msrc\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39;49mscatter_add_(dim, index, src)\n\u001b[1;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mscatter_add_(dim, index, src)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "losses = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    batch_index = 0\n",
    "    for batch in train_loader:\n",
    "        print(\"Batch index:\", batch_index)\n",
    "        # loss = 0.0\n",
    "        loss = train(batch, log=not(epoch%20))\n",
    "        # train_rmse = test(batch)\n",
    "        # val_rmse = test(val_data)\n",
    "        # test_rmse = test(test_data, log=not(epoch%20))\n",
    "        # losses.append((loss, train_rmse, val_rmse, test_rmse))\n",
    "        losses.append((loss, 0, 0, 0))\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {0.0:.4f}, '\n",
    "                f'Val: {0.0:.4f}, Test: {0.0:.4f}')\n",
    "        batch_index += 1\n",
    "last_losses = losses[-1]\n",
    "losses = losses + [last_losses] * (epochs - len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie': tensor([[ 0.0761,  0.1135,  0.0690,  ..., -0.2256,  0.2134, -0.1368],\n",
       "         [ 0.0435,  0.0875,  0.0618,  ..., -0.2305,  0.2250, -0.1418],\n",
       "         [ 0.0662,  0.0826,  0.0691,  ..., -0.2216,  0.2238, -0.1587],\n",
       "         ...,\n",
       "         [ 0.1111,  0.1490,  0.0663,  ..., -0.2371,  0.1986, -0.0976],\n",
       "         [ 0.0922,  0.1169,  0.0603,  ..., -0.2199,  0.1890, -0.1057],\n",
       "         [ 0.0715,  0.1584,  0.0413,  ..., -0.2457,  0.2041, -0.1142]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'user': tensor([[ 0.0488, -0.0851,  0.0284,  ...,  0.1463, -0.0836,  0.0671],\n",
       "         [ 0.0509, -0.0810,  0.0300,  ...,  0.1481, -0.0808,  0.0725],\n",
       "         [ 0.0518, -0.0774,  0.0278,  ...,  0.1422, -0.0780,  0.0548],\n",
       "         ...,\n",
       "         [ 0.0553, -0.0726,  0.0294,  ...,  0.1361, -0.0786,  0.0700],\n",
       "         [ 0.0517, -0.0849,  0.0192,  ...,  0.1429, -0.0850,  0.0679],\n",
       "         [ 0.0475, -0.0816,  0.0279,  ...,  0.1458, -0.0839,  0.0664]],\n",
       "        grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dict_test_lala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0761,  0.1135,  0.0690,  ..., -0.2256,  0.2134, -0.1368],\n",
       "        [ 0.0435,  0.0875,  0.0618,  ..., -0.2305,  0.2250, -0.1418],\n",
       "        [ 0.0662,  0.0826,  0.0691,  ..., -0.2216,  0.2238, -0.1587],\n",
       "        ...,\n",
       "        [ 0.1111,  0.1490,  0.0663,  ..., -0.2371,  0.1986, -0.0976],\n",
       "        [ 0.0922,  0.1169,  0.0603,  ..., -0.2199,  0.1890, -0.1057],\n",
       "        [ 0.0715,  0.1584,  0.0413,  ..., -0.2457,  0.2041, -0.1142]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dict_test_lala[\"movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0488, -0.0851,  0.0284,  ...,  0.1463, -0.0836,  0.0671],\n",
       "        [ 0.0509, -0.0810,  0.0300,  ...,  0.1481, -0.0808,  0.0725],\n",
       "        [ 0.0518, -0.0774,  0.0278,  ...,  0.1422, -0.0780,  0.0548],\n",
       "        ...,\n",
       "        [ 0.0553, -0.0726,  0.0294,  ...,  0.1361, -0.0786,  0.0700],\n",
       "        [ 0.0517, -0.0849,  0.0192,  ...,  0.1429, -0.0850,  0.0679],\n",
       "        [ 0.0475, -0.0816,  0.0279,  ...,  0.1458, -0.0839,  0.0664]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dict_test_lala[\"user\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col = edge_label_index_test_lala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = z_dict_test_lala[\"movie\"]\n",
    "user = z_dict_test_lala[\"user\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97347])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0761,  0.1135,  0.0690,  ..., -0.2256,  0.2134, -0.1368],\n",
       "        [ 0.0761,  0.1135,  0.0690,  ..., -0.2256,  0.2134, -0.1368],\n",
       "        [ 0.0761,  0.1135,  0.0690,  ..., -0.2256,  0.2134, -0.1368],\n",
       "        ...,\n",
       "        [ 0.0567,  0.0945,  0.0843,  ..., -0.2181,  0.2255, -0.1466],\n",
       "        [ 0.0567,  0.0945,  0.0843,  ..., -0.2181,  0.2255, -0.1466],\n",
       "        [ 0.0567,  0.0945,  0.0843,  ..., -0.2181,  0.2255, -0.1466]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97347])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 14980 is out of bounds for dimension 0 with size 14979",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m user[row]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 14980 is out of bounds for dimension 0 with size 14979"
     ]
    }
   ],
   "source": [
    "user[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('environ': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Aug  5 2022, 15:21:02) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acbc58aadc5672afc04cc91f2a1726d8eb7b999e15e50d024070fdc74729208f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
