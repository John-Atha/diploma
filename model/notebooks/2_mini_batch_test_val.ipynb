{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ioannisathanasiou/diploma/environ/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import argparse\n",
    "parent_path = pathlib.Path(os.getcwd()).parent.absolute()\n",
    "sys.path.append(str(parent_path))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import MovieLens\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.loader import NeighborLoader, LinkNeighborLoader\n",
    "\n",
    "from utils.Neo4jMovieLensMetaData import Neo4jMovieLensMetaData\n",
    "# from utils.gnn_simple import Model\n",
    "from utils.train_test import train_test\n",
    "from utils.visualize import plot_loss, plot_train, plot_val, plot_test, plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the corresponsing csv, store the dataset to the DB, preprocess it, and get it as a pytorch graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies have features...\n",
      "Encoding title...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 565/565 [00:16<00:00, 34.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([18062, 64])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "path = osp.join(osp.dirname(osp.abspath('')), '../../data/MovieLensNeo4j')\n",
    "dataset = Neo4jMovieLensMetaData(\n",
    "    path,\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    database_url=\"bolt://localhost:7687\",\n",
    "    database_username=\"neo4j\",\n",
    "    database_password=\"admin\",\n",
    "    force_pre_process=True,\n",
    "    force_db_restore=False,\n",
    "    text_features=[\"title\"],\n",
    "    list_features=[],\n",
    "    fastRP_features=[],\n",
    "    numeric_features=[],\n",
    ")\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user node features for message passing:\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes\n",
    "\n",
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label.\n",
    "\n",
    "# Perform a link-level split into training, validation, and test edges:\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    # Sample ALL neighbors for each node and each edge type for 2 iterations:\n",
    "    num_neighbors=[-1],\n",
    "    # Use a batch size of 128 for sampling training nodes of type \"paper\":\n",
    "    batch_size=64,\n",
    "    edge_label_index = ((\"user\", \"movie\"), None),\n",
    "    edge_label = data['user', 'movie'].edge_label,\n",
    ")\n",
    "train_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mmovie\u001b[0m={ x=[3023, 64] },\n",
       "  \u001b[1muser\u001b[0m={ x=[15769, 16236] },\n",
       "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
       "    edge_index=[2, 109532],\n",
       "    edge_label=[64],\n",
       "    edge_label_index=[2, 64]\n",
       "  },\n",
       "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 10395] }\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and train-test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dict_test_lala = 0\n",
    "edge_label_index_test_lala = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, LazyLinear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GATv2Conv, GCNConv, TransformerConv, GraphConv, GINConv, GINEConv, to_hetero, HeteroLinear, HeteroConv\n",
    "from torch_geometric.nn.models import GIN, GraphSAGE\n",
    "from torch_geometric.nn.aggr import MultiAggregation\n",
    "from typing import Union\n",
    "from torch_geometric.typing import Adj, OptPairTensor, Size\n",
    "\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        global z_dict_test_lala, edge_label_index_test_lala\n",
    "        row, col = edge_label_index\n",
    "        z_dict_test_lala = z_dict\n",
    "        edge_label_index_test_lala = edge_label_index\n",
    "        movie = z_dict['movie'][col]\n",
    "        user = z_dict['user'][row]\n",
    "        z = torch.cat([user, movie], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_channels=16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.encoder(train_batch.x_dict, train_batch.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.012)\n",
    "\n",
    "weight = torch.bincount(train_data['user', 'movie'].edge_label)\n",
    "# weight = torch.bincount(train_data['user', 'rates', 'movie'].edge_label)\n",
    "weight = weight.max() / weight\n",
    "\n",
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    # weight = 1. if weight is None else weight[target].to(pred.dtype)\n",
    "    weight = 1. # if weight is None else weight[target].to(pred.dtype)\n",
    "    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch, log=False):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred = model(batch.x_dict, batch.edge_index_dict, batch['user', 'movie'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = batch['user', 'movie'].edge_label.float()\n",
    "\n",
    "    loss = weighted_mse_loss(pred, target, weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(batch, log=False):\n",
    "    model.eval()\n",
    "    pred = model(batch.x_dict, batch.edge_index_dict, batch['user', 'movie'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = batch['user', 'movie'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Batch 000, Loss: 10.9410, Train: 3.1711, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 010, Loss: 4.4850, Train: 2.1251, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 020, Loss: 2.1264, Train: 1.4500, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 030, Loss: 1.3298, Train: 1.1170, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 040, Loss: 1.7115, Train: 1.2723, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 050, Loss: 1.1343, Train: 1.0498, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 060, Loss: 1.6298, Train: 1.2272, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 070, Loss: 1.9519, Train: 1.3550, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 080, Loss: 0.8947, Train: 0.8901, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 090, Loss: 1.3780, Train: 1.0288, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 100, Loss: 1.2808, Train: 1.2014, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 110, Loss: 0.5822, Train: 0.7226, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 120, Loss: 1.3662, Train: 1.1831, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 130, Loss: 0.6961, Train: 0.9104, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 140, Loss: 1.9094, Train: 1.3751, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 150, Loss: 2.9940, Train: 1.7080, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 160, Loss: 1.9457, Train: 1.2863, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 170, Loss: 1.1866, Train: 1.1080, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 180, Loss: 0.7090, Train: 0.8359, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 190, Loss: 1.3066, Train: 1.1273, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 200, Loss: 1.9237, Train: 1.3838, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 210, Loss: 2.7783, Train: 1.5912, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 220, Loss: 1.2190, Train: 1.1010, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 230, Loss: 1.2635, Train: 1.0945, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 240, Loss: 0.6044, Train: 0.7660, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 250, Loss: 1.1352, Train: 1.0556, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 260, Loss: 1.2533, Train: 1.1008, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 270, Loss: 1.2753, Train: 1.1296, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 280, Loss: 0.8293, Train: 0.9192, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 290, Loss: 1.1247, Train: 0.9975, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 300, Loss: 1.1365, Train: 1.0458, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 310, Loss: 0.9705, Train: 0.9556, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 320, Loss: 1.2097, Train: 1.0535, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 330, Loss: 1.0849, Train: 1.0436, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 340, Loss: 1.1853, Train: 1.1087, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 350, Loss: 0.9498, Train: 0.9359, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 360, Loss: 1.3662, Train: 1.1083, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 370, Loss: 1.3043, Train: 1.1487, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 380, Loss: 0.7994, Train: 0.9006, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 390, Loss: 0.8826, Train: 0.9411, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 400, Loss: 1.1782, Train: 1.0443, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 410, Loss: 2.3526, Train: 1.4823, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 420, Loss: 1.6604, Train: 1.2849, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 430, Loss: 2.9752, Train: 1.6772, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 440, Loss: 1.6771, Train: 1.2628, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 450, Loss: 0.9803, Train: 0.9712, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 460, Loss: 0.5834, Train: 0.7685, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 470, Loss: 1.1315, Train: 0.9821, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 480, Loss: 1.0607, Train: 1.0198, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 490, Loss: 0.8316, Train: 0.9284, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 500, Loss: 1.4187, Train: 1.1780, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 510, Loss: 0.5537, Train: 0.7333, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 520, Loss: 0.9964, Train: 0.9527, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 530, Loss: 1.4433, Train: 1.1452, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 540, Loss: 0.5754, Train: 0.7865, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 550, Loss: 2.0620, Train: 1.3227, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 560, Loss: 0.7488, Train: 0.8585, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 570, Loss: 0.9150, Train: 0.8808, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 580, Loss: 1.8044, Train: 1.2840, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 590, Loss: 1.1353, Train: 1.0525, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 600, Loss: 2.6063, Train: 1.6118, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 610, Loss: 1.1279, Train: 1.0623, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 620, Loss: 0.5731, Train: 0.7348, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 630, Loss: 2.0516, Train: 1.4351, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 640, Loss: 0.5805, Train: 0.7389, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 650, Loss: 1.4549, Train: 1.0236, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 660, Loss: 0.7863, Train: 0.7625, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 670, Loss: 0.6366, Train: 0.7895, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 680, Loss: 1.6519, Train: 1.2690, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 690, Loss: 1.0431, Train: 1.0291, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 700, Loss: 0.4668, Train: 0.6877, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 710, Loss: 1.7143, Train: 1.2839, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 720, Loss: 1.0593, Train: 1.0456, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 730, Loss: 0.6704, Train: 0.8294, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 740, Loss: 0.8159, Train: 0.8956, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 750, Loss: 2.2068, Train: 1.5025, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 760, Loss: 1.0532, Train: 1.0165, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 770, Loss: 1.0564, Train: 0.9975, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 780, Loss: 1.0534, Train: 1.0246, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 790, Loss: 0.5727, Train: 0.7456, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 800, Loss: 0.3907, Train: 0.6249, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 810, Loss: 1.2242, Train: 1.1165, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 820, Loss: 1.4706, Train: 1.2495, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 830, Loss: 1.4883, Train: 1.2332, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 840, Loss: 0.5915, Train: 0.7616, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 850, Loss: 1.0917, Train: 1.0870, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 860, Loss: 1.2048, Train: 1.1082, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 870, Loss: 0.6191, Train: 0.7872, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 880, Loss: 1.2250, Train: 1.1014, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 890, Loss: 1.0004, Train: 0.9635, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 900, Loss: 0.5960, Train: 0.7607, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 910, Loss: 1.3894, Train: 1.1367, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 920, Loss: 1.2859, Train: 1.0684, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 930, Loss: 0.7986, Train: 0.8577, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 940, Loss: 1.4700, Train: 1.2075, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 950, Loss: 0.4043, Train: 0.6373, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 960, Loss: 0.7296, Train: 0.7813, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 970, Loss: 0.7028, Train: 0.8324, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 980, Loss: 2.2646, Train: 1.4911, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 990, Loss: 1.6263, Train: 1.2503, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 1000, Loss: 0.8732, Train: 0.8995, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 1010, Loss: 2.0303, Train: 1.2879, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 1020, Loss: 0.4631, Train: 0.6643, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 1030, Loss: 1.5408, Train: 1.3123, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 1040, Loss: 1.2196, Train: 1.1229, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 1050, Loss: 0.8948, Train: 0.9022, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 1060, Loss: 0.7258, Train: 0.8572, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 1070, Loss: 0.3604, Train: 0.5997, Val: 0.0000, Test: 0.0000\n",
      "Epoch: 001, Batch 1080, Loss: 3.5521, Train: 1.7679, Val: 0.0000, Test: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m----> 6\u001b[0m     loss \u001b[39m=\u001b[39m train(batch, log\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m(epoch\u001b[39m%\u001b[39m\u001b[39m20\u001b[39m))\n\u001b[1;32m      7\u001b[0m     train_rmse \u001b[39m=\u001b[39m test(batch)\n\u001b[1;32m      8\u001b[0m     \u001b[39m# val_rmse = test(val_data)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39m# test_rmse = test(test_data, log=not(epoch%20))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39m# losses.append((loss, train_rmse, val_rmse, test_rmse))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [11], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(batch, log)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m pred \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39;49mx_dict, batch\u001b[39m.\u001b[39;49medge_index_dict, batch[\u001b[39m'\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmovie\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49medge_label_index)\n\u001b[1;32m      6\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m target \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmovie\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39medge_label\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [8], line 52\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_dict, edge_index_dict, edge_label_index)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_dict, edge_index_dict, edge_label_index):\n\u001b[0;32m---> 52\u001b[0m     z_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x_dict, edge_index_dict)\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z_dict, edge_label_index)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/fx/graph_module.py:652\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_wrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/fx/graph_module.py:267\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_call(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    266\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls, obj)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m     \u001b[39massert\u001b[39;00m e\u001b[39m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.3:11\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m      9\u001b[0m edge_index__user__rates__movie \u001b[39m=\u001b[39m edge_index_dict\u001b[39m.\u001b[39mget((\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrates\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmovie\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m edge_index__movie__rev_rates__user \u001b[39m=\u001b[39m edge_index_dict\u001b[39m.\u001b[39mget((\u001b[39m'\u001b[39m\u001b[39mmovie\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrev_rates\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m);  edge_index_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m conv1__movie \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1\u001b[39m.\u001b[39;49muser__rates__movie((x__user, x__movie), edge_index__user__rates__movie)\n\u001b[1;32m     12\u001b[0m conv1__user \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mmovie__rev_rates__user((x__movie, x__user), edge_index__movie__rev_rates__user);  x__movie \u001b[39m=\u001b[39m x__user \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m relu__movie \u001b[39m=\u001b[39m conv1__movie\u001b[39m.\u001b[39mrelu();  conv1__movie \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/conv/sage_conv.py:132\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size, conv_index)\u001b[0m\n\u001b[1;32m    129\u001b[0m     x \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mrelu(), x[\u001b[39m1\u001b[39m])\n\u001b[1;32m    131\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    133\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_l(out)\n\u001b[1;32m    135\u001b[0m x_r \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:392\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m         aggr_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 392\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate(out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maggr_kwargs)\n\u001b[1;32m    394\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aggregate_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    395\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:515\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maggregate\u001b[39m(\u001b[39mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[1;32m    503\u001b[0m               ptr: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    504\u001b[0m               dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    505\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m    :math:`\\square_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggr_module(inputs, index, ptr\u001b[39m=\u001b[39;49mptr, dim_size\u001b[39m=\u001b[39;49mdim_size,\n\u001b[1;32m    516\u001b[0m                             dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:114\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39melif\u001b[39;00m index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m dim_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()):\n\u001b[1;32m    110\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEncountered invalid \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdim_size\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdim_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m but expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>= \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(x, index, ptr, dim_size, dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/aggr/basic.py:34\u001b[0m, in \u001b[0;36mMeanAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m             ptr: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m             dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduce(x, index, ptr, dim_size, dim, reduce\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:153\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m segment_csr(x, ptr, reduce\u001b[39m=\u001b[39mreduce)\n\u001b[1;32m    152\u001b[0m \u001b[39massert\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m scatter(x, index, dim\u001b[39m=\u001b[39;49mdim, dim_size\u001b[39m=\u001b[39;49mdim_size, reduce\u001b[39m=\u001b[39;49mreduce)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:64\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatter\u001b[39m(src: Tensor, index: Tensor, dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     62\u001b[0m             out: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m             reduce: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_scatter\u001b[39m.\u001b[39;49mscatter(src, index, dim, out, dim_size, reduce)\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_scatter/scatter.py:156\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter_mul(src, index, dim, out, dim_size)\n\u001b[1;32m    155\u001b[0m \u001b[39melif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter_mean(src, index, dim, out, dim_size)\n\u001b[1;32m    157\u001b[0m \u001b[39melif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter_min(src, index, dim, out, dim_size)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_scatter/scatter.py:41\u001b[0m, in \u001b[0;36mscatter_mean\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatter_mean\u001b[39m(src: torch\u001b[39m.\u001b[39mTensor, index: torch\u001b[39m.\u001b[39mTensor, dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     39\u001b[0m                  out: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m                  dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 41\u001b[0m     out \u001b[39m=\u001b[39m scatter_sum(src, index, dim, out, dim_size)\n\u001b[1;32m     42\u001b[0m     dim_size \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39msize(dim)\n\u001b[1;32m     44\u001b[0m     index_dim \u001b[39m=\u001b[39m dim\n",
      "File \u001b[0;32m~/diploma/environ/lib/python3.9/site-packages/torch_scatter/scatter.py:21\u001b[0m, in \u001b[0;36mscatter_sum\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m         size[dim] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(size, dtype\u001b[39m=\u001b[39msrc\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39msrc\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39;49mscatter_add_(dim, index, src)\n\u001b[1;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mscatter_add_(dim, index, src)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "losses = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    batch_index = 0\n",
    "    for batch in train_loader:\n",
    "        loss = train(batch, log=not(epoch%20))\n",
    "        train_rmse = test(batch)\n",
    "        # val_rmse = test(val_data)\n",
    "        # test_rmse = test(test_data, log=not(epoch%20))\n",
    "        # losses.append((loss, train_rmse, val_rmse, test_rmse))\n",
    "        losses.append((loss, train_rmse, 0, 0))\n",
    "        if not batch_index % 10:\n",
    "                print(f'Epoch: {epoch:03d}, Batch {batch_index:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "                        f'Val: {0.0:.4f}, Test: {0.0:.4f}')\n",
    "        batch_index += 1\n",
    "last_losses = losses[-1]\n",
    "losses = losses + [last_losses] * (epochs - len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('environ': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acbc58aadc5672afc04cc91f2a1726d8eb7b999e15e50d024070fdc74729208f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
